{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview ðŸ”Ž\nThis script demonstrates the use of a multi-agent system for collaborative research and blog post creation using OpenAI's Swarm package. The system leverages multiple agents to interact and solve tasks collaboratively, focusing on efficient research execution and content generation.\n\n## Motivation\nBy utilizing a multi-agent system, we can enhance collaborative research and content creation by distributing tasks among specialized agents. This approach demonstrates how agents with distinct roles can work together to produce a comprehensive blog post.\n\n### Why use a multi-agent system?\nMulti-agent systems offer several advantages in complex tasks like content creation:\n1. Specialization: Each agent can focus on its specific role, leading to higher quality output.\n2. Parallelization: Multiple agents can work simultaneously on different aspects of the task.\n3. Scalability: The system can be easily expanded by adding new agents with specialized roles.\n4. Robustness: If one agent fails, others can compensate, ensuring task completion.\n\n## Key Components\n- OpenAI's Swarm Package: Facilitates the creation and management of multi-agent interactions.\n- Agents: Include a human admin, AI researcher, content planner, writer, and editor, each with specific responsibilities.\n- Interaction Management: Manages the conversation flow and context among agents.\n\n## Method\nThe system follows a structured approach:\n\n1. Agent Configuration: Each agent is set up with a specific role and behavior.\n   \n   In this step, we define the characteristics and capabilities of each agent. This includes:\n   - Setting the agent's name and role\n   - Defining the agent's instructions (what it should do)\n   - Specifying the functions the agent can call (to interact with other agents or perform specific tasks)\n\n2. Role Assignment:\n   - Admin: Oversees the project and provides guidance.\n   - Researcher: Gathers information on the given topic.\n   - Planner: Organizes the research into an outline.\n   - Writer: Drafts the blog post based on the outline.\n   - Editor: Reviews and edits the draft for quality assurance.\n   \n   Each role is crucial for the successful creation of a high-quality blog post. This division of labor allows for specialization and ensures that each aspect of the content creation process receives focused attention.\n\n3. Interaction Management: Defines permissible interactions between agents to maintain orderly communication.\n   \n   This step involves:\n   - Determining which agents can communicate with each other\n   - Defining the order of operations (e.g., research before writing)\n   - Ensuring that context and information are properly passed between agents\n\n4. Task Execution: The admin initiates a task, and agents collaboratively work through researching, planning, writing, and editing.\n   \n   The task execution follows a logical flow:\n   1. Admin sets the topic and initiates the process\n   2. Planner creates an outline based on the topic\n   3. Researcher gathers information on each section of the outline\n   4. Writer uses the research to draft the blog post\n   5. Editor reviews and refines the final product\n   \n   This structured approach ensures a comprehensive and well-researched blog post as the final output.","metadata":{"id":"YKiU4gKJifTZ"}},{"cell_type":"code","source":"from dotenv import load_dotenv\n\nload_dotenv()","metadata":{"id":"NB08AM9HifTh","outputId":"7a2d2a82-171d-45cc-8cf7-2ce219ac4581","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:06:17.094088Z","iopub.execute_input":"2024-12-18T08:06:17.094561Z","iopub.status.idle":"2024-12-18T08:06:17.145034Z","shell.execute_reply.started":"2024-12-18T08:06:17.094522Z","shell.execute_reply":"2024-12-18T08:06:17.143953Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"## OpenAI Swarm Package\nThe Swarm package provides a framework for creating and managing multi-agent systems. It allows for:\n- Easy agent creation with customizable roles and behaviors\n- Seamless communication between agents\n- Task distribution and management\n- Context preservation across agent interactions","metadata":{"id":"OrHwKGxDifTk"}},{"cell_type":"markdown","source":"### Requirements\n\nSwarm requires `Python>=3.10`","metadata":{"id":"r-MDOhPfifTo"}},{"cell_type":"code","source":"%pip install git+https://github.com/openai/swarm.git","metadata":{"id":"EVyyrLh9ifTp","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:06:17.147038Z","iopub.execute_input":"2024-12-18T08:06:17.147549Z","iopub.status.idle":"2024-12-18T08:06:46.534758Z","shell.execute_reply.started":"2024-12-18T08:06:17.147513Z","shell.execute_reply":"2024-12-18T08:06:46.533449Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/swarm.git\n  Cloning https://github.com/openai/swarm.git to /tmp/pip-req-build-sscpn_x_\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/swarm.git /tmp/pip-req-build-sscpn_x_\n  Resolved https://github.com/openai/swarm.git to commit 9db581cecaacea0d46a933d6453c312b034dbf47\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from swarm==0.1.0) (1.26.4)\nCollecting openai>=1.33.0 (from swarm==0.1.0)\n  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: pytest in /opt/conda/lib/python3.10/site-packages (from swarm==0.1.0) (8.3.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from swarm==0.1.0) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from swarm==0.1.0) (4.66.4)\nCollecting pre-commit (from swarm==0.1.0)\n  Downloading pre_commit-4.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting instructor (from swarm==0.1.0)\n  Downloading instructor-1.7.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.33.0->swarm==0.1.0) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.33.0->swarm==0.1.0) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.33.0->swarm==0.1.0) (0.27.0)\nCollecting jiter<1,>=0.4.0 (from openai>=1.33.0->swarm==0.1.0)\n  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.33.0->swarm==0.1.0) (2.10.2)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>=1.33.0->swarm==0.1.0) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai>=1.33.0->swarm==0.1.0) (4.12.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /opt/conda/lib/python3.10/site-packages (from instructor->swarm==0.1.0) (3.9.5)\nRequirement already satisfied: docstring-parser<0.17,>=0.16 in /opt/conda/lib/python3.10/site-packages (from instructor->swarm==0.1.0) (0.16)\nRequirement already satisfied: jinja2<4.0.0,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from instructor->swarm==0.1.0) (3.1.4)\n  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from instructor->swarm==0.1.0) (2.27.1)\nRequirement already satisfied: rich<14.0.0,>=13.7.0 in /opt/conda/lib/python3.10/site-packages (from instructor->swarm==0.1.0) (13.7.1)\nCollecting tenacity<10.0.0,>=9.0.0 (from instructor->swarm==0.1.0)\n  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: typer<1.0.0,>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from instructor->swarm==0.1.0) (0.12.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->swarm==0.1.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->swarm==0.1.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->swarm==0.1.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->swarm==0.1.0) (2024.6.2)\nCollecting cfgv>=2.0.0 (from pre-commit->swarm==0.1.0)\n  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\nCollecting identify>=1.0.0 (from pre-commit->swarm==0.1.0)\n  Downloading identify-2.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\nCollecting nodeenv>=0.11.1 (from pre-commit->swarm==0.1.0)\n  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from pre-commit->swarm==0.1.0) (6.0.2)\nRequirement already satisfied: virtualenv>=20.10.0 in /opt/conda/lib/python3.10/site-packages (from pre-commit->swarm==0.1.0) (20.21.0)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest->swarm==0.1.0) (2.0.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pytest->swarm==0.1.0) (21.3)\nRequirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.10/site-packages (from pytest->swarm==0.1.0) (1.5.0)\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest->swarm==0.1.0) (1.2.0)\nRequirement already satisfied: tomli>=1 in /opt/conda/lib/python3.10/site-packages (from pytest->swarm==0.1.0) (2.0.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (4.0.3)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.4->instructor->swarm==0.1.0) (2.1.5)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.33.0->swarm==0.1.0) (0.7.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (2.18.0)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (1.5.4)\nRequirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (0.3.8)\nRequirement already satisfied: filelock<4,>=3.4.1 in /opt/conda/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (3.15.1)\nRequirement already satisfied: platformdirs<4,>=2.4 in /opt/conda/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (3.11.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pytest->swarm==0.1.0) (3.1.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (0.1.2)\nDownloading openai-1.58.1-py3-none-any.whl (454 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading instructor-1.7.0-py3-none-any.whl (70 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pre_commit-4.0.1-py2.py3-none-any.whl (218 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m218.7/218.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\nDownloading identify-2.6.3-py2.py3-none-any.whl (99 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\nDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\nBuilding wheels for collected packages: swarm\n  Building wheel for swarm (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for swarm: filename=swarm-0.1.0-py3-none-any.whl size=25999 sha256=253201f80c2f5e0ef1f7bd6ec930e7153ac2ca649f8e2904fe21567de552ea95\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ib1z3zgu/wheels/46/9a/f7/7b8bbb674ae80ef0f62a632706c2c4cdfcf708e4da32e4e256\nSuccessfully built swarm\nInstalling collected packages: tenacity, nodeenv, jiter, identify, cfgv, pre-commit, openai, instructor, swarm\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 8.3.0\n    Uninstalling tenacity-8.3.0:\n      Successfully uninstalled tenacity-8.3.0\nSuccessfully installed cfgv-3.4.0 identify-2.6.3 instructor-1.7.0 jiter-0.6.1 nodeenv-1.9.1 openai-1.58.1 pre-commit-4.0.1 swarm-0.1.0 tenacity-9.0.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Creating Functions for the Agents\n\nFunctions enable agents to perform specific actions and interact with each other in our multi-agent system. Here are the key points to understand:\n\n1. **Function Definition**: Functions are defined using standard Python syntax.\n\n2. **JSON Formatting**: When passed to an agent, functions are automatically formatted into JSON.\n\n3. **Flexible Argument Passing**: While function parameters aren't strictly declared elsewhere, agents will attempt to pass arguments based on the function's definition.\n\n4. **Agent Usage**: Agents interpret the JSON representation to understand available functions, their purposes, and required parameters. They then decide which function to call based on their current task.\n\n5. **Function Assignment**: Functions are assigned to agents during initialization:","metadata":{"id":"hgywWAYzifTr"}},{"cell_type":"code","source":"def complete_blog_post(title, content):\n    # Create a valid filename from the title\n    filename = title.lower().replace(\" \", \"-\") + \".md\"\n\n    with open(filename, \"w\", encoding=\"utf-8\") as file:\n        file.write(content)\n\n    print(f\"Blog post '{title}' has been written to {filename}\")\n    return \"Task completed\"","metadata":{"id":"yqucAB6OifTs","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:06:46.536474Z","iopub.execute_input":"2024-12-18T08:06:46.536868Z","iopub.status.idle":"2024-12-18T08:06:46.543394Z","shell.execute_reply.started":"2024-12-18T08:06:46.536829Z","shell.execute_reply":"2024-12-18T08:06:46.542103Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Creating the Agents\n\n1. **Define System Prompts**: Each agent has its own set of instructions. These functions return a string of instructions that will be used as the system prompt.\n\n2. **Define transfer functions**: These functions allow agents to hand off control to the next agent in the workflow.\n\n3. **Create Agent instances**: Use the Agent class to create each agent, specifying its name, instructions, and available functions.\n\n","metadata":{"id":"bNNeOL0PifTt"}},{"cell_type":"code","source":"from swarm import Agent\n\ndef admin_instructions(context_variables):\n    topic = context_variables.get(\"topic\", \"No topic provided\")\n    return f\"\"\"You are the Admin Agent overseeing the blog post project on the topic: '{topic}'.\nYour responsibilities include initiating the project, providing guidance, and reviewing the final content.\nOnce you've set the topic, call the function to transfer to the planner agent.\"\"\"\n\n\ndef planner_instructions(context_variables):\n    topic = context_variables.get(\"topic\", \"No topic provided\")\n    return f\"\"\"You are the Planner Agent. Based on the following topic: '{topic}'\nOrganize the content into topics and sections with clear headings that will each be individually researched as points in the greater blog post.\nOnce the outline is ready, call the researcher agent. \"\"\"\n\n\ndef researcher_instructions(context_variables):\n    return \"\"\"You are the Researcher Agent. your task is to provide dense context and information on the topics outlined by the previous planner agent.\nThis research will serve as the information that will be formatted into a body of a blog post. Provide comprehensive research like notes for each of the sections outlined by the planner agent.\nOnce your research is complete, transfer to the writer agent\"\"\"\n\n\ndef writer_instructions(context_variables):\n    return \"\"\"You are the Writer Agent. using the prior information write a clear blog post following the outline from the planner agent.\n    Summarise and include as much information relevant from the research into the blog post.\n    The blog post should be quite large as the context the context provided should be quite dense.\nWrite clear, engaging content for each section.\nOnce the draft is complete, call the function to transfer to the Editor Agent.\"\"\"\n\n\ndef editor_instructions(context_variables):\n    return \"\"\"You are the Editor Agent. Review and edit th prior blog post completed by the writer agent.\nMake necessary corrections and improvements.\nOnce editing is complete, call the function to complete the blog post\"\"\"\n\ndef transfer_to_researcher():\n    return researcher_agent\n\n\ndef transfer_to_planner():\n    return planner_agent\n\n\ndef transfer_to_writer():\n    return writer_agent\n\n\ndef transfer_to_editor():\n    return editor_agent\n\n\ndef transfer_to_admin():\n    return admin_agent\n\n\ndef complete_blog():\n    return \"Task completed\"\n\n\nadmin_agent = Agent(\n    name=\"Admin Agent\",\n    instructions=admin_instructions,\n    functions=[transfer_to_planner],\n)\n\nplanner_agent = Agent(\n    name=\"Planner Agent\",\n    instructions=planner_instructions,\n    functions=[transfer_to_researcher],\n)\n\nresearcher_agent = Agent(\n    name=\"Researcher Agent\",\n    instructions=researcher_instructions,\n    functions=[transfer_to_writer],\n)\n\nwriter_agent = Agent(\n    name=\"Writer Agent\",\n    instructions=writer_instructions,\n    functions=[transfer_to_editor],\n)\n\neditor_agent = Agent(\n    name=\"Editor Agent\",\n    instructions=editor_instructions,\n    functions=[complete_blog_post],\n)\n","metadata":{"id":"Q75vMNVeifTt","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:06:46.546401Z","iopub.execute_input":"2024-12-18T08:06:46.547097Z","iopub.status.idle":"2024-12-18T08:06:47.489083Z","shell.execute_reply.started":"2024-12-18T08:06:46.547029Z","shell.execute_reply":"2024-12-18T08:06:47.487966Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Run the demo","metadata":{"id":"jCMPZf0CifTu"}},{"cell_type":"code","source":"from swarm.repl import run_demo_loop\n\ndef run():\n    run_demo_loop(admin_agent, debug=True)","metadata":{"id":"QUuYfSVpifTw","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:06:47.490408Z","iopub.execute_input":"2024-12-18T08:06:47.490844Z","iopub.status.idle":"2024-12-18T08:06:47.497740Z","shell.execute_reply.started":"2024-12-18T08:06:47.490808Z","shell.execute_reply":"2024-12-18T08:06:47.496547Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"You will be prompted by the notebook to provide and input topic for the blog post","metadata":{"id":"5DkD0COaifTw"}},{"cell_type":"code","source":"run()","metadata":{"id":"TS6nXoYMifTx","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:06:47.499148Z","iopub.execute_input":"2024-12-18T08:06:47.499510Z","iopub.status.idle":"2024-12-18T08:06:47.928309Z","shell.execute_reply.started":"2024-12-18T08:06:47.499477Z","shell.execute_reply":"2024-12-18T08:06:47.926640Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m():\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mrun_demo_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43madmin_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/swarm/repl/repl.py:63\u001b[0m, in \u001b[0;36mrun_demo_loop\u001b[0;34m(starting_agent, context_variables, stream, debug)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_demo_loop\u001b[39m(\n\u001b[1;32m     61\u001b[0m     starting_agent, context_variables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     62\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mSwarm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Swarm CLI ðŸ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m     messages \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/swarm/core.py:29\u001b[0m, in \u001b[0;36mSwarm.__init__\u001b[0;34m(self, client)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m client:\n\u001b[0;32m---> 29\u001b[0m         client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m client\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/openai/_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    108\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"],"ename":"OpenAIError","evalue":"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable","output_type":"error"}],"execution_count":6},{"cell_type":"markdown","source":"Outputs will be saved to a local .md file titled as the chosen topic","metadata":{"id":"xhm0GRdeifTx"}},{"cell_type":"markdown","source":"# Results for \"Impact of LLMs on healthcare\"","metadata":{"id":"fjCfH4NLifTy"}},{"cell_type":"markdown","source":"### Introduction\n\nIn the realm of artificial intelligence, Large Language Models (LLMs) like OpenAIâ€™s GPT-3 and Google's BERT have emerged as powerful tools capable of understanding and generating human-language text with resounding proficiency. These models draw on extensive datasets to execute complex natural language processing tasks, thus opening up expansive possibilities in various fields, especially healthcare. As healthcare continues to pivot towards technology-driven solutions, the integration of LLMs offers promising pathways to enhance efficiency, elevate patient outcomes, and personalize medical care.\n\n### Enhancement of Diagnostics\n\nLLMs represent a significant leap forward in medical diagnostics. By scrutinizing clinical data, images, and test results, these models can assist pathologists and radiologists by offering diagnostic insights and recognizing subtle patterns that may elude human practitioners. This potential is particularly potent in the early detection of diseases, such as in the field of oncology. For instance, LLMs can analyze structured and unstructured patient records, medical histories, and real-time data to predict the onset of conditions like diabetes or cardiovascular diseases, allowing earlier intervention and improved patient management.\n\n### Patient Education and Engagement\n\nThe role of LLMs in patient education is transformative. They facilitate the distribution of personalized health information, enabling patients to better understand medical conditions and treatments. By simplifying complex medical jargon, LLMs improve health literacy, allowing patients to engage more actively in their care. Moreover, by providing 24/7 virtual assistance, LLMs enhance patient communication through conversational interactions, leading to increased engagement and a sense of ownership over one's health management.\n\n### Streamlining Administrative Tasks\n\nThe healthcare ecosystem is often encumbered by demanding administrative tasks, which can divert focus from patient-centered care. LLMs offer a solution by automating routine clerical tasks such as transcribing doctor's notes, managing schedules, and handling billing queries. This automation allows healthcare providers to concentrate more on direct patient care duties. Additionally, LLMs' prowess in natural language processing makes organizing and retrieving patient records efficient, significantly reducing manual errors and saving valuable time in healthcare settings.\n\n### Research and Drug Discovery\n\nIn the field of medical research and drug development, LLMs are invaluable. They expedite literature reviews and enable the formulation of hypotheses based on immense datasetsâ€”a boon for genomics and personalized medicine. Moreover, LLMs are instrumental in simulating drug interaction pathways, potentially accelerating the identification of novel drug candidates or the repurposing of existing drugs faster than conventional methods. This accelerates the translation of research findings into clinical applications, ultimately improving patient care and treatment outcomes.\n\n### Ethical Considerations and Challenges\n\nNotwithstanding their benefits, the deployment of LLMs in healthcare generates significant ethical concerns. Chief among these is data privacy, given LLMs' reliance on extensive datasets that include sensitive patient information. Ensuring compliance with regulations like HIPAA is essential. Furthermore, biases within AI algorithms, stemming from the data they are trained on, pose a risk of skewed diagnostics or treatment recommendations, disproportionately impacting marginalized communities. Addressing these biases and ensuring equitable AI practices is paramount as healthcare increasingly integrates LLMs.\n\n### Future Prospects and Predictions\n\nLooking to the future, LLMs promise broader and more refined integration into healthcare technologies, offering heightened accuracy and minimized biases. As a synergistic part of telemedicine and remote diagnostics, these models are expected to spearhead advancements in predictive analytics and bespoke healthcare solutions. Such evolution may drastically enhance resource management within healthcare systems, promising a future where patient care is more efficient, personalized, and holistic.\n\n### Conclusion\n\nLLMs stand on the cusp of redefining healthcare by enhancing diagnostic capabilities, optimizing administrative efficiency, and bolstering patient connectivity and education. However, as these technologies embed deeper into clinical applications, a careful approach is necessary to navigate ethical dilemmas, particularly concerning data security and bias. Ultimately, with a balanced fusion of innovation and regulation, LLMs hold the potential to render healthcare more effective, accessible, and patient-focused.","metadata":{"id":"-hXVaV3EifTy"}}]}