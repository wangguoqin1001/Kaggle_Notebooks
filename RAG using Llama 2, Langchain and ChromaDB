{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4966565,"sourceType":"datasetVersion","datasetId":2880535},{"sourceId":4298,"sourceType":"modelInstanceVersion","modelInstanceId":3093,"modelId":735}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F769452%2Fb18d0513200d426e556b2b7b7c825981%2FRAG.png?generation=1695504022336680&alt=media\"></img>\n\n## Objective\n\nUse Llama 2.0, Langchain and ChromaDB to create a Retrieval Augmented Generation (RAG) system. This will allow us to ask questions about our documents (that were not included in the training data), without fine-tunning the Large Language Model (LLM).\nWhen using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed. \n\n## Definitions\n\n* LLM - Large Language Model  \n* Llama 2.0 - LLM from Meta \n* Langchain - a framework designed to simplify the creation of applications using LLMs\n* Vector database - a database that organizes data through high-dimmensional vectors  \n* ChromaDB - vector database  \n* RAG - Retrieval Augmented Generation (see below more details about RAGs)\n\n## Model details\n\n* **Model**: Llama 2  \n* **Variation**: 7b-chat-hf  (7b: 7B dimm. hf: HuggingFace build)\n* **Version**: V1  \n* **Framework**: PyTorch  \n\nLlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model.\n\n\n## What is a Retrieval Augmented Generation (RAG) system?\n\nLarge Language Models (LLMs) has proven their ability to understand context and provide accurate answers to various NLP tasks, including summarization, Q&A, when prompted. While being able to provide very good answers to questions about information that they were trained with, they tend to hallucinate when the topic is about information that they do \"not know\", i.e. was not included in their training data. Retrieval Augmented Generation combines external resources with LLMs. The main two components of a RAG are therefore a retriever and a generator.  \n \nThe retriever part can be described as a system that is able to encode our data so that can be easily retrieved the relevant parts of it upon queriying it. The encoding is done using text embeddings, i.e. a model trained to create a vector representation of the information. The best option for implementing a retriever is a vector database. As vector database, there are multiple options, both open source or commercial products. Few examples are ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Our option in this Notebook will be a local instance of ChromaDB (persistent).\n\nFor the generator part, the obvious option is a LLM. In this Notebook we will use a quantized LLaMA v2 model, from the Kaggle Models collection.  \n\nThe orchestration of the retriever and generator will be done using Langchain. A specialized function from Langchain allows us to create the receiver-generator in one line of code.\n\n## More about this  \n\nDo you want to learn more? Look into the `References` section for blog posts and in `More work on the same topic` for Notebooks about the technologies used here.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Installations, imports, utils","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\nbitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-19T18:10:08.547287Z","iopub.execute_input":"2024-12-19T18:10:08.547871Z","iopub.status.idle":"2024-12-19T18:10:18.216617Z","shell.execute_reply.started":"2024-12-19T18:10:08.547843Z","shell.execute_reply":"2024-12-19T18:10:18.215399Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.33.0 in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: accelerate==0.22.0 in /opt/conda/lib/python3.10/site-packages (0.22.0)\nRequirement already satisfied: einops==0.6.1 in /opt/conda/lib/python3.10/site-packages (0.6.1)\nRequirement already satisfied: langchain==0.0.300 in /opt/conda/lib/python3.10/site-packages (0.0.300)\nRequirement already satisfied: xformers==0.0.21 in /opt/conda/lib/python3.10/site-packages (0.0.21)\nRequirement already satisfied: bitsandbytes==0.41.1 in /opt/conda/lib/python3.10/site-packages (0.41.1)\nRequirement already satisfied: sentence_transformers==2.2.2 in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: chromadb==0.4.12 in /opt/conda/lib/python3.10/site-packages (0.4.12)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (2.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (2.0.17)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (3.8.4)\nRequirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (3.7.0)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (0.6.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (1.33)\nRequirement already satisfied: langsmith<0.1.0,>=0.0.38 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (0.0.92)\nRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (2.8.5)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (1.10.9)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (8.2.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.15.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.11.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.1.99)\nRequirement already satisfied: chroma-hnswlib==0.7.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.7.3)\nRequirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.98.0)\nRequirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.22.0)\nRequirement already satisfied: posthog>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (3.7.4)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (4.6.3)\nRequirement already satisfied: pulsar-client>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (3.5.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (1.20.1)\nRequirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.48.9)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (7.3.1)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (5.12.0)\nRequirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (4.2.1)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.99)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.99)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.101)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.10.3.66)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (10.2.10.91)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.4.0.1)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.4.91)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (2.14.3)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.91)\nRequirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (2.0.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (68.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (0.40.0)\nRequirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0) (3.31.2)\nRequirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0) (18.1.8)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.1.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (3.20.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (0.9.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.12) (0.27.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2023.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.300) (2.0)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (3.20.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.33.0) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.16.0)\nRequirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.6)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.2.1)\nRequirement already satisfied: python-dateutil>2.1 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.8.2)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.12) (2023.7.22)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0) (1.26.15)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.300) (2.0.2)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.6.0)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.17.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.20.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (11.0.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers==2.2.2) (9.5.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (1.0.0)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.12) (10.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from torch import cuda, bfloat16\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer\nfrom time import time\n#import chromadb\n#from chromadb.config import Settings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:10:18.218504Z","iopub.execute_input":"2024-12-19T18:10:18.218794Z","iopub.status.idle":"2024-12-19T18:10:18.224168Z","shell.execute_reply.started":"2024-12-19T18:10:18.218769Z","shell.execute_reply":"2024-12-19T18:10:18.223354Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Initialize model, tokenizer, query pipeline","metadata":{}},{"cell_type":"markdown","source":"Define the model, the device, and the `bitsandbytes` configuration.","metadata":{}},{"cell_type":"code","source":"model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:10:18.225270Z","iopub.execute_input":"2024-12-19T18:10:18.225628Z","iopub.status.idle":"2024-12-19T18:10:18.242019Z","shell.execute_reply.started":"2024-12-19T18:10:18.225596Z","shell.execute_reply":"2024-12-19T18:10:18.241185Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Prepare the model and the tokenizer.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntime_2 = time()\nprint(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:10:18.244273Z","iopub.execute_input":"2024-12-19T18:10:18.244757Z","iopub.status.idle":"2024-12-19T18:12:45.664154Z","shell.execute_reply.started":"2024-12-19T18:10:18.244734Z","shell.execute_reply":"2024-12-19T18:12:45.663248Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3e50b9ba3844127bac846fd029057cd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Prepare model, tokenizer: 147.411 sec.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"Define the query pipeline.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\nquery_pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\ntime_2 = time()\nprint(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:12:45.665419Z","iopub.execute_input":"2024-12-19T18:12:45.665767Z","iopub.status.idle":"2024-12-19T18:12:47.336085Z","shell.execute_reply.started":"2024-12-19T18:12:45.665729Z","shell.execute_reply":"2024-12-19T18:12:47.335248Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Prepare pipeline: 1.665 sec.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"We define a function for testing the pipeline.","metadata":{}},{"cell_type":"code","source":"def test_model(tokenizer, pipeline, prompt_to_test):\n    \"\"\"\n    Perform a query\n    print the result\n    Args:\n        tokenizer: the tokenizer\n        pipeline: the pipeline\n        prompt_to_test: the prompt\n    Returns\n        None\n    \"\"\"\n    # adapted from https://huggingface.co/blog/llama2#using-transformers\n    time_1 = time()\n    sequences = pipeline(\n        prompt_to_test,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,)\n    time_2 = time()\n    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:12:47.337266Z","iopub.execute_input":"2024-12-19T18:12:47.337596Z","iopub.status.idle":"2024-12-19T18:12:47.343344Z","shell.execute_reply.started":"2024-12-19T18:12:47.337564Z","shell.execute_reply":"2024-12-19T18:12:47.342375Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Test the query pipeline\n\nWe test the pipeline with a query about the meaning of State of the Union (SOTU).","metadata":{}},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:12:47.344443Z","iopub.execute_input":"2024-12-19T18:12:47.344840Z","iopub.status.idle":"2024-12-19T18:12:53.122753Z","shell.execute_reply.started":"2024-12-19T18:12:47.344801Z","shell.execute_reply":"2024-12-19T18:12:53.121896Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Test inference: 5.761 sec.\nResult: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\nThe State of the Union address is an annual speech given by the President of the United States to a joint session of Congress, in which they provide a report on the state of the union and outline their legislative agenda for the upcoming year.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Retrieval Augmented Generation","metadata":{}},{"cell_type":"markdown","source":"## Check the model with a HuggingFace pipeline\n\n\nWe check the model with a HF pipeline, using a query about the meaning of State of the Union (SOTU).","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:22:16.433666Z","iopub.execute_input":"2023-09-23T19:22:16.434937Z","iopub.status.idle":"2023-09-23T19:22:16.440864Z","shell.execute_reply.started":"2023-09-23T19:22:16.434891Z","shell.execute_reply":"2023-09-23T19:22:16.439217Z"}}},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=query_pipeline)\n# checking again that everything is working fine\nllm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:12:53.124065Z","iopub.execute_input":"2024-12-19T18:12:53.124484Z","iopub.status.idle":"2024-12-19T18:12:57.430941Z","shell.execute_reply.started":"2024-12-19T18:12:53.124448Z","shell.execute_reply":"2024-12-19T18:12:57.429940Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'\\nThe State of the Union address is an annual speech given by the President of the United States to a joint session of Congress, in which the President reports on the current state of the union and outlines their legislative agenda for the upcoming year.'"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Ingestion of data using Text loder\n\nWe will ingest the newest presidential address, from Jan 2023.","metadata":{}},{"cell_type":"code","source":"loader = TextLoader(\"/kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\",\n                    encoding=\"utf8\")\ndocuments = loader.load()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:12:57.431921Z","iopub.execute_input":"2024-12-19T18:12:57.432142Z","iopub.status.idle":"2024-12-19T18:12:57.448727Z","shell.execute_reply.started":"2024-12-19T18:12:57.432122Z","shell.execute_reply":"2024-12-19T18:12:57.448075Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Split data in chunks\n\nWe split data in chunks using a recursive character text splitter.","metadata":{}},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:12:57.452228Z","iopub.execute_input":"2024-12-19T18:12:57.452491Z","iopub.status.idle":"2024-12-19T18:12:57.470211Z","shell.execute_reply.started":"2024-12-19T18:12:57.452468Z","shell.execute_reply":"2024-12-19T18:12:57.469253Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Creating Embeddings and Storing in Vector Store","metadata":{}},{"cell_type":"markdown","source":"Create the embeddings using Sentence Transformer and HuggingFace embeddings.","metadata":{}},{"cell_type":"code","source":"model_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:12:57.471340Z","iopub.execute_input":"2024-12-19T18:12:57.471640Z","iopub.status.idle":"2024-12-19T18:13:37.510976Z","shell.execute_reply.started":"2024-12-19T18:12:57.471616Z","shell.execute_reply":"2024-12-19T18:13:37.510062Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading .gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4afe52a617cd4706948fce0c62bbc66f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b45dfcab2cc14c14b6fdfa5fbc4dbe29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bdac83bd81642669223150657f4e34c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b909e88455f47fead1a02ad46c064ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"441ec0fc3d094fe39e84667af507c1aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0a3ba445e0046da917162125a2463b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daf38565b10241da94d7dd2c51c7734f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"697fcb844d794e5eb00602ef7d3d597b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model_O1.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ad70b5327f645b3bfe91374adb621b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model_O2.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb398cec7be3408fac0312024222299e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model_O3.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8f522b4232f4d9ab2ee156e86d5c488"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model_O4.onnx:   0%|          | 0.00/218M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f809bbd541834e509e919c5a57a5e29a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model_qint8_arm64.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9f3fc76825a48688a24944afbe906d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)el_qint8_avx512.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbeffb21e6a2468a8af46989afb09ca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nt8_avx512_vnni.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dd9a641a0394d6282b9604cf7525283"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model_quint8_avx2.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e30a159116a4d30989e9b67959d4d02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading openvino_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f368f610525a4498a7913d3940025f6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)o/openvino_model.xml:   0%|          | 0.00/433k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86b30113a8784874b66a1b2dc1d4d4ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)_qint8_quantized.bin:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e1a6ee53ea14dbb842aa848aefdea13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)_qint8_quantized.xml:   0%|          | 0.00/742k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27055737a0eb45a5bf755483b121fc25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91068af03435441e997fcf4e34bbda43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f993d9e262e4883ac51c02de0793959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f84353f1599425faa1dae7b07f1e602"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e100371e00c4e19b9ef402103a678fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcb58f007905489a96d2d2e4556d2137"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"413f2872acfa48df943b50efd734f635"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f581c93525ee4e82b24996ccf8ad487f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04335342505542ec8c2c00a0cbeb1f6f"}},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally.","metadata":{}},{"cell_type":"code","source":"vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:13:37.512104Z","iopub.execute_input":"2024-12-19T18:13:37.512427Z","iopub.status.idle":"2024-12-19T18:13:39.223071Z","shell.execute_reply.started":"2024-12-19T18:13:37.512401Z","shell.execute_reply":"2024-12-19T18:13:39.222022Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b15d15895947f8b4ff6aabb42ce63e"}},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"## Initialize chain","metadata":{}},{"cell_type":"code","source":"retriever = vectordb.as_retriever()\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:13:39.224403Z","iopub.execute_input":"2024-12-19T18:13:39.224688Z","iopub.status.idle":"2024-12-19T18:13:39.230252Z","shell.execute_reply.started":"2024-12-19T18:13:39.224664Z","shell.execute_reply":"2024-12-19T18:13:39.229056Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Test the Retrieval-Augmented Generation \n\n\nWe define a test function, that will run the query and time it.","metadata":{}},{"cell_type":"code","source":"def test_rag(qa, query):\n    print(f\"Query: {query}\\n\")\n    time_1 = time()\n    result = qa.run(query)\n    time_2 = time()\n    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n    print(\"\\nResult: \", result)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:13:39.231763Z","iopub.execute_input":"2024-12-19T18:13:39.232033Z","iopub.status.idle":"2024-12-19T18:13:39.244174Z","shell.execute_reply.started":"2024-12-19T18:13:39.232009Z","shell.execute_reply":"2024-12-19T18:13:39.243365Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"Let's check few queries.","metadata":{}},{"cell_type":"code","source":"query = \"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:13:39.245173Z","iopub.execute_input":"2024-12-19T18:13:39.245498Z","iopub.status.idle":"2024-12-19T18:13:50.412386Z","shell.execute_reply.started":"2024-12-19T18:13:39.245475Z","shell.execute_reply":"2024-12-19T18:13:50.411654Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Query: What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef1d19b8328486bbfb630d8f4f9e0da"}},"metadata":{}},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 11.154 sec.\n\nResult:   The State of the Union in 2023 focused on several key topics, including the nation's economic strength, the competition with China, and the need to come together as a nation to face the challenges ahead. The President emphasized the importance of American innovation, industries, and military modernization to ensure the country's safety and stability. The President also highlighted the nation's resilience and optimism, urging Americans to see each other as fellow citizens and to work together to overcome the challenges facing the country.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"query = \"What is the nation economic status? Summarize. Keep it under 200 words.\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:13:50.413849Z","iopub.execute_input":"2024-12-19T18:13:50.414276Z","iopub.status.idle":"2024-12-19T18:14:00.723359Z","shell.execute_reply.started":"2024-12-19T18:13:50.414240Z","shell.execute_reply":"2024-12-19T18:14:00.722327Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Query: What is the nation economic status? Summarize. Keep it under 200 words.\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"574c8d73a6a34144a615aca4540d6c80"}},"metadata":{}},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\nInference time: 10.305 sec.\n\nResult:   The nation's economic status is strong, with a low unemployment rate of 3.4%, near record lows for Black and Hispanic workers, and fastest growth in 40 years in manufacturing jobs. The president highlights the progress made in creating good-paying jobs, exporting American products, and reducing inflation. However, the president acknowledges there is still more work to be done to fully recover from the pandemic and Putin's war.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Document sources\n\nLet's check the documents sources, for the last query run.","metadata":{}},{"cell_type":"code","source":"docs = vectordb.similarity_search(query)\nprint(f\"Query: {query}\")\nprint(f\"Retrieved documents: {len(docs)}\")\nfor doc in docs:\n    doc_details = doc.to_json()['kwargs']\n    print(\"Source: \", doc_details['metadata']['source'])\n    print(\"Text: \", doc_details['page_content'], \"\\n\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-19T18:14:00.724316Z","iopub.execute_input":"2024-12-19T18:14:00.724546Z","iopub.status.idle":"2024-12-19T18:14:00.759996Z","shell.execute_reply.started":"2024-12-19T18:14:00.724527Z","shell.execute_reply":"2024-12-19T18:14:00.759111Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dba025321d2472eba8dce0fb02b8c98"}},"metadata":{}},{"name":"stdout","text":"Query: What is the nation economic status? Summarize. Keep it under 200 words.\nRetrieved documents: 4\nSource:  /kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\nText:  forward. Of never giving up. A story that is unique among all nations. We are the only country that has emerged from every crisis stronger than when we entered it. That is what we are doing again. Two years ago, our economy was reeling. As I stand here tonight, we have created a record 12 million new jobs, more jobs created in two years than any president has ever created in four years. Two years ago, COVID had shut down our businesses, closed our schools, and robbed us of so much. Today, COVID no longer controls our lives. And two years ago, our democracy faced its greatest threat since the Civil War. Today, though bruised, our democracy remains unbowed and unbroken. As we gather here tonight, we are writing the next chapter in the great American story, a story of progress and resilience. When world leaders ask me to define America, I define our country in one word: Possibilities. You know, we’re often told that Democrats and Republicans can’t work together. But over these past two \n\nSource:  /kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\nText:  on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops. \n\nSource:  /kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\nText:  Americans, we meet tonight at an inflection point. One of those moments that only a few generations ever face, where the decisions we make now will decide the course of this nation and of the world for decades to come. We are not bystanders to history. We are not powerless before the forces that confront us. It is within our power, of We the People. We are facing the test of our time and the time for choosing is at hand. We must be the nation we have always been at our best. Optimistic. Hopeful. Forward-looking. A nation that embraces, light over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the \n\nSource:  /kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\nText:  about being able to look your kid in the eye and say, “Honey –it’s going to be OK,” and mean it. So, let’s look at the results. Unemployment rate at 3.4%, a 50-year low. Near record low unemployment for Black and Hispanic workers. We’ve already created 800,000 good-paying manufacturing jobs, the fastest growth in 40 years. Where is it written that America can’t lead the world in manufacturing again? For too many decades, we imported products and exported jobs. Now, thanks to all we’ve done, we’re exporting American products and creating American jobs. Inflation has been a global problem because of the pandemic that disrupted supply chains and Putin’s war that disrupted energy and food supplies. But we’re better positioned than any country on Earth. We have more to do, but here at home, inflation is coming down. Here at home, gas prices are down $1.50 a gallon since their peak. Food inflation is coming down. Inflation has fallen every month for the last six months while take home pay \n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Conclusions\n\n\nWe used Langchain, ChromaDB and Llama 2 as a LLM to build a Retrieval Augmented Generation solution. For testing, we were using the latest State of the Union address from Jan 2023.\n\n\n# More work on the same topic\n\nYou can find more details about how to use a LLM with Kaggle. Few interesting topics are treated in:  \n\n* https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp (quantizing LLama 2 model using llama.cpp)\n* https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp  (quantized Llamam 2 model using llama.cpp)  \n* https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu (quantized model using llama.cpp - running on CPU)  \n* https://www.kaggle.com/code/gpreda/explore-enron-emails-with-langchain-and-llama-v2 (Explore Enron Emails with Langchain and Llama v2)\n","metadata":{}},{"cell_type":"markdown","source":"# References  \n\n[1] Murtuza Kazmi, Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data, https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476  \n\n[2] Patrick Lewis, Ethan Perez, et. al., Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, https://browse.arxiv.org/pdf/2005.11401.pdf \n\n[3] Minhajul Hoque, Retrieval Augmented Generation: Grounding AI Responses in Factual Data, https://medium.com/@minh.hoque/retrieval-augmented-generation-grounding-ai-responses-in-factual-data-b7855c059322  \n\n[4] Fangrui Liu\t, Discover the Performance Gain with Retrieval Augmented Generation, https://thenewstack.io/discover-the-performance-gain-with-retrieval-augmented-generation/\n\n[5] Andrew, How to use Retrieval-Augmented Generation (RAG) with Llama 2, https://agi-sphere.com/retrieval-augmented-generation-llama2/   \n\n[6] Yogendra Sisodia, Retrieval Augmented Generation Using Llama2 And Falcon, https://medium.com/@scholarly360/retrieval-augmented-generation-using-llama2-and-falcon-ed26c7b14670   \n\n","metadata":{}}]}