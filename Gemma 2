{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":116995,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":98328,"modelId":121954}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Setup the environment\n!pip install -q -U immutabledict sentencepiece \n!git clone https://github.com/google/gemma_pytorch.git\n!mkdir /kaggle/working/gemma/\n!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/\n\nimport sys \nsys.path.append(\"/kaggle/working/gemma_pytorch/\") \nfrom gemma.config import GemmaConfig, get_model_config\nfrom gemma.model import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer\nimport contextlib\nimport os\nimport torch\n\n# Load the model\nVARIANT = \"2b-v2\"\nMACHINE_TYPE = \"cuda\" \nmodel_path = '/kaggle/input/gemma-2-2b-jpn-it/pytorch/gemma-2-2b-jpn-it/1/' \nweights_file = os.path.join(model_path, \"model.ckpt\")\n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n  torch.set_default_dtype(dtype)\n  yield\n  torch.set_default_dtype(torch.float)\n\nmodel_config = get_model_config(VARIANT)\nmodel_config.tokenizer = os.path.join(model_path, \"tokenizer.model\")\n\ndevice = torch.device(MACHINE_TYPE)\nwith _set_default_tensor_type(model_config.get_dtype()):\n  model = GemmaForCausalLM(model_config)\n  model.load_weights(weights_file)\n  model = model.to(device).eval()\n\n# Use the model\nUSER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n\"\n\nprompt = (\n  USER_CHAT_TEMPLATE.format(\n    prompt=\"hello\"\n  )\n  + \"<start_of_turn>model\\n\"\n)\nprint(prompt)\n\nresult = model.generate(\n  prompt,\n  device=device,\n  output_len=2560,\n)\nprint(result)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}