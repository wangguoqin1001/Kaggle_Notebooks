{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"building_your_own_federated_learning_algorithm.ipynb","toc_visible":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2020 The TensorFlow Authors.","metadata":{"id":"vkdnLiKk71g-"}},{"cell_type":"code","source":"#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"cellView":"form","id":"0asMuNro71hA","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:18:01.571804Z","iopub.execute_input":"2024-12-18T17:18:01.572081Z","iopub.status.idle":"2024-12-18T17:18:01.588563Z","shell.execute_reply.started":"2024-12-18T17:18:01.572053Z","shell.execute_reply":"2024-12-18T17:18:01.587618Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"<table class=\"tfo-notebook-buttons\" align=\"left\">\n  <td>\n    <a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/building_your_own_federated_learning_algorithm\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.87.0/docs/tutorials/building_your_own_federated_learning_algorithm.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.87.0/docs/tutorials/building_your_own_federated_learning_algorithm.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n  </td>\n  <td>\n    <a href=\"https://storage.googleapis.com/tensorflow_docs/federated/docs/tutorials/building_your_own_federated_learning_algorithm.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n  </td>\n</table>","metadata":{"id":"iPFgLeZIsZ3Q"}},{"cell_type":"markdown","source":"## Before you start\n\nBefore you start, please run the following to make sure that your environment is\ncorrectly setup. If you don't see a greeting, please refer to the\n[Installation](../install.md) guide for instructions.","metadata":{"id":"MnUwFbCAKB2r"}},{"cell_type":"code","source":"#@test {\"skip\": true}\n!pip install --quiet --upgrade tensorflow-federated","metadata":{"id":"ZrGitA_KnRO0","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:18:01.611210Z","iopub.execute_input":"2024-12-18T17:18:01.611461Z","iopub.status.idle":"2024-12-18T17:20:04.139371Z","shell.execute_reply.started":"2024-12-18T17:18:01.611436Z","shell.execute_reply":"2024-12-18T17:20:04.138239Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.10.1 requires cubinlinker, which is not installed.\ncudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.10.1 requires libcudf==24.10.*, which is not installed.\ncudf 24.10.1 requires ptxcompiler, which is not installed.\ncuml 24.10.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 24.10.0 requires cuvs==24.10.*, which is not installed.\ncuml 24.10.0 requires nvidia-cublas, which is not installed.\ncuml 24.10.0 requires nvidia-cufft, which is not installed.\ncuml 24.10.0 requires nvidia-curand, which is not installed.\ncuml 24.10.0 requires nvidia-cusolver, which is not installed.\ncuml 24.10.0 requires nvidia-cusparse, which is not installed.\ndask-cudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\npylibcudf 24.10.1 requires libcudf==24.10.*, which is not installed.\npylibraft 24.10.0 requires nvidia-cublas, which is not installed.\npylibraft 24.10.0 requires nvidia-curand, which is not installed.\npylibraft 24.10.0 requires nvidia-cusolver, which is not installed.\npylibraft 24.10.0 requires nvidia-cusparse, which is not installed.\nucxx 0.40.0 requires libucxx==0.40.*, which is not installed.\nalbumentations 1.4.21 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\naltair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.5.0 which is incompatible.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.1.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.25.2 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.5 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.3 which is incompatible.\nblis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.2 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.25.2 which is incompatible.\nchex 0.1.86 requires jax>=0.4.16, but you have jax 0.4.14 which is incompatible.\nconda 24.5.0 requires packaging>=23.0, but you have packaging 22.0 which is incompatible.\ncudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\ncudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ndask-cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.10.1 which is incompatible.\ndistributed 2024.9.0 requires dask==2024.9.0, but you have dask 2024.11.2 which is incompatible.\nfastapi 0.111.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\nfeaturetools 1.31.0 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\nflax 0.8.4 requires jax>=0.4.19, but you have jax 0.4.14 which is incompatible.\ngcsfs 2024.9.0.post1 requires fsspec==2024.9.0, but you have fsspec 2024.6.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 22.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.5 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 17.0.0 which is incompatible.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.20 which is incompatible.\njupyterlab 4.3.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkaggle-environments 1.16.9 requires scipy>=1.11.2, but you have scipy 1.9.3 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nnibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\norbax-checkpoint 0.6.4 requires jax>=0.4.26, but you have jax 0.4.14 which is incompatible.\nplotnine 0.14.3 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\npydantic 2.10.1 requires typing-extensions>=4.12.2, but you have typing-extensions 4.5.0 which is incompatible.\npydantic-core 2.27.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\npylibcudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\npytoolconfig 1.3.1 requires packaging>=23.2, but you have packaging 22.0 which is incompatible.\nrapids-dask-dependency 24.10.0a0 requires dask==2024.9.0, but you have dask 2024.11.2 which is incompatible.\nrapids-dask-dependency 24.10.0a0 requires dask-expr==1.1.14, but you have dask-expr 1.1.19 which is incompatible.\nrmm 24.10.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\ns3fs 2024.9.0 requires fsspec==2024.9.0.*, but you have fsspec 2024.6.0 which is incompatible.\nstumpy 1.13.0 requires scipy>=1.10, but you have scipy 1.9.3 which is incompatible.\ntensorflow-decision-forests 1.9.1 requires tensorflow~=2.16.1, but you have tensorflow 2.14.1 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-serving-api 2.16.1 requires tensorflow<3,>=2.16.1, but you have tensorflow 2.14.1 which is incompatible.\ntensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.14.1 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntensorstore 0.1.69 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\ntf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.14.1 which is incompatible.\nthinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.25.2 which is incompatible.\ntorch 2.4.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\ntsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.9.3 which is incompatible.\ntypeguard 4.3.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\nwoodwork 0.31.0 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\nxarray 2024.11.0 requires packaging>=23.2, but you have packaging 22.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import collections\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_federated as tff","metadata":{"id":"HGTM6tWOLo8M","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:04.141337Z","iopub.execute_input":"2024-12-18T17:20:04.141648Z","iopub.status.idle":"2024-12-18T17:20:09.833201Z","shell.execute_reply.started":"2024-12-18T17:20:04.141618Z","shell.execute_reply":"2024-12-18T17:20:09.832462Z"}},"outputs":[{"name":"stderr","text":"2024-12-18 17:20:04.431200: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-18 17:20:04.431279: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-18 17:20:04.431351: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"**NOTE**: This colab has been verified to work with the [latest released version](https://github.com/tensorflow/federated#compatibility) of the `tensorflow_federated` pip package, but the Tensorflow Federated project is still in pre-release development and may not work on `main`.","metadata":{"id":"yr3ztf28fa1F"}},{"cell_type":"markdown","source":"# Building Your Own Federated Learning Algorithm\n\nIn the [image classification](federated_learning_for_image_classification.ipynb) and\n[text generation](federated_learning_for_text_generation.ipynb) tutorials, you learned how to set up model and data pipelines for Federated Learning (FL), and performed federated training via the `tff.learning` API layer of TFF.\n\nThis is only the tip of the iceberg when it comes to FL research. This tutorial discusses how to implement federated learning algorithms *without* deferring to the `tff.learning` API. In this tutorial, you will accomplish the following:\n\n**Goals:**\n\n\n*   Understand the general structure of federated learning algorithms.\n*   Explore the *Federated Core* of TFF.\n*   Use the Federated Core to implement Federated Averaging directly.\n\nWhile this tutorial is self-contained, it may be useful to first check out the [image classification](federated_learning_for_image_classification.ipynb) and\n[text generation](federated_learning_for_text_generation.ipynb) tutorials.\n","metadata":{"id":"4Zv28F7QLo8O"}},{"cell_type":"markdown","source":"## Preparing the input data\nFirst load and preprocess the EMNIST dataset included in TFF. For more details, see the [image classification](federated_learning_for_image_classification.ipynb) tutorial.","metadata":{"id":"hQ_N9XbULo8P"}},{"cell_type":"code","source":"emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()","metadata":{"id":"-WdnFluLLo8P","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:09.834237Z","iopub.execute_input":"2024-12-18T17:20:09.835078Z","iopub.status.idle":"2024-12-18T17:20:29.460104Z","shell.execute_reply.started":"2024-12-18T17:20:09.835039Z","shell.execute_reply":"2024-12-18T17:20:29.459261Z"}},"outputs":[{"name":"stderr","text":"Downloading emnist_all.sqlite.lzma: 100%|██████████| 170507172/170507172 [00:19<00:00, 8930261.35it/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"In order to feed the dataset into our model, the data is flattened, and each example is converted into a tuple of the form `(flattened_image_vector, label)`.","metadata":{"id":"kq8893GogB8E"}},{"cell_type":"code","source":"NUM_CLIENTS = 10\nBATCH_SIZE = 20\n\ndef preprocess(dataset):\n\n  def batch_format_fn(element):\n    \"\"\"Flatten a batch of EMNIST data and return a (features, label) tuple.\"\"\"\n    return (tf.reshape(element['pixels'], [-1, 784]),\n            tf.reshape(element['label'], [-1, 1]))\n\n  return dataset.batch(BATCH_SIZE).map(batch_format_fn)","metadata":{"id":"Blrh8zJgLo8R","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:29.462344Z","iopub.execute_input":"2024-12-18T17:20:29.462794Z","iopub.status.idle":"2024-12-18T17:20:29.468041Z","shell.execute_reply.started":"2024-12-18T17:20:29.462753Z","shell.execute_reply":"2024-12-18T17:20:29.467277Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"Now, select a small number of clients, and apply the preprocessing above to their datasets.","metadata":{"id":"Piy8EzqmgNqV"}},{"cell_type":"code","source":"client_ids = sorted(emnist_train.client_ids)[:NUM_CLIENTS]\nfederated_train_data = [preprocess(emnist_train.create_tf_dataset_for_client(x))\n  for x in client_ids\n]","metadata":{"id":"-vYM_IT7Lo8W","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:29.468991Z","iopub.execute_input":"2024-12-18T17:20:29.469259Z","iopub.status.idle":"2024-12-18T17:20:37.478451Z","shell.execute_reply.started":"2024-12-18T17:20:29.469231Z","shell.execute_reply":"2024-12-18T17:20:37.477494Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Preparing the model","metadata":{"id":"gNO_Y9j_Lo8X"}},{"cell_type":"markdown","source":"This uses the same model as in the [image classification](federated_learning_for_image_classification.ipynb) tutorial. This model (implemented via `tf.keras`) has a single hidden layer, followed by a softmax layer.","metadata":{"id":"LJ0I89ixz8yV"}},{"cell_type":"code","source":"def create_keras_model():\n  initializer = tf.keras.initializers.GlorotNormal(seed=0)\n  return tf.keras.models.Sequential([\n      tf.keras.layers.Input(shape=(784,)),\n      tf.keras.layers.Dense(10, kernel_initializer=initializer),\n      tf.keras.layers.Softmax(),\n  ])","metadata":{"id":"Yfld4oFNLo8Y","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:37.479926Z","iopub.execute_input":"2024-12-18T17:20:37.480327Z","iopub.status.idle":"2024-12-18T17:20:38.096305Z","shell.execute_reply.started":"2024-12-18T17:20:37.480276Z","shell.execute_reply":"2024-12-18T17:20:38.095214Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"In order to use this model in TFF, wrap the Keras model as a [`tff.learning.models.FunctionalModel`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/models/FunctionalModel). This allows one to perform the model's [forward pass](https://www.tensorflow.org/federated/api_docs/python/tff/learning/models/FunctionalModel#predict_on_batch) within TFF, and [extract model outputs](https://www.tensorflow.org/federated/api_docs/python/tff/learning/models/FunctionalModel#finalize_metrics). For more details, also see the [image classification](federated_learning_for_image_classification.ipynb) tutorial.","metadata":{"id":"vLln0Q8G0Bky"}},{"cell_type":"code","source":"keras_model = create_keras_model()\ntff_model = tff.learning.models.functional_model_from_keras(\n    keras_model,\n    loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(),\n    input_spec=federated_train_data[0].element_spec,\n    metrics_constructor=collections.OrderedDict(\n        accuracy=tf.keras.metrics.SparseCategoricalAccuracy\n    ),\n)","metadata":{"id":"SPwbipTNLo8a","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:38.097484Z","iopub.execute_input":"2024-12-18T17:20:38.097792Z","iopub.status.idle":"2024-12-18T17:20:39.137512Z","shell.execute_reply.started":"2024-12-18T17:20:38.097766Z","shell.execute_reply":"2024-12-18T17:20:39.136788Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"While the above used `tf.keras` to create a `tff.learning.models.FunctionalModel`, TFF supports much more general models. These models have the following relevant attributes capturing the model weights:\n\n*   `trainable_variables`: An iterable of the tensors corresponding to trainable layers.\n*   `non_trainable_variables`: An iterable of the tensors corresponding to non-trainable layers.\n\nIn this tutorial, only the `trainable_variables` will be used (as the model only has those!).","metadata":{"id":"pCxa44rFiere"}},{"cell_type":"markdown","source":"# Building your own Federated Learning algorithm\n\nWhile the `tff.learning` API allows one to create many variants of Federated Averaging, there are other federated algorithms that do not fit neatly into this framework. For example, you may want to add regularization, clipping, or more complicated algorithms such as [federated GAN training](https://github.com/tensorflow/federated/tree/main/tensorflow_federated/python/research/gans). You may also be instead be interested in [federated analytics](https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html).","metadata":{"id":"fPOWP2JjsfTk"}},{"cell_type":"markdown","source":"For these more advanced algorithms, you'll have to write our own custom algorithm using TFF. In many cases, federated algorithms have 4 main components:\n\n1. A server-to-client broadcast step.\n2. A local client update step.\n3. A client-to-server upload step.\n4. A server update step.","metadata":{"id":"50N36Zz8qyY-"}},{"cell_type":"markdown","source":"In TFF, a federated algorithm is typically represented as a [`tff.templates.IterativeProcess`](https://www.tensorflow.org/federated/api_docs/python/tff/templates/IterativeProcess) (which will be referred to as just an `IterativeProcess` throughout). This is a class that contains `initialize` and `next` functions. Here, `initialize` is used to initialize the server, and `next` will perform one communication round of the federated algorithm. Let's write a skeleton of what our iterative process for FedAvg should look like.\n\nFirst, there is an initialize function that simply creates a `tff.learning.models.FunctionalModel`, and returns its trainable weights.","metadata":{"id":"jH8s0GRdQt3b"}},{"cell_type":"code","source":"def initialize_fn():\n  trainable_weights, _ =  tff_model.initial_weights\n  return trainable_weights","metadata":{"id":"ylLpRa7T5DDh","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:39.138664Z","iopub.execute_input":"2024-12-18T17:20:39.139028Z","iopub.status.idle":"2024-12-18T17:20:39.952234Z","shell.execute_reply.started":"2024-12-18T17:20:39.138972Z","shell.execute_reply":"2024-12-18T17:20:39.951244Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"This function looks good, but as you will see later, you will need to make a small modification to make it a \"TFF computation\".\n\nNext, let's write a sketch of the `next_fn`.","metadata":{"id":"nb1-XAK8fB2A"}},{"cell_type":"code","source":"def next_fn(server_weights, federated_dataset):\n  # Broadcast the server weights to the clients.\n  server_weights_at_client = broadcast(server_weights)\n\n  # Each client computes their updated weights.\n  client_weights = client_update(federated_dataset, server_weights_at_client)\n\n  # The server averages these updates.\n  mean_client_weights = mean(client_weights)\n\n  # The server updates its model.\n  server_weights = server_update(mean_client_weights)\n\n  return server_weights","metadata":{"id":"IeHN-XLZfMso","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:39.953810Z","iopub.execute_input":"2024-12-18T17:20:39.954209Z","iopub.status.idle":"2024-12-18T17:20:41.086148Z","shell.execute_reply.started":"2024-12-18T17:20:39.954162Z","shell.execute_reply":"2024-12-18T17:20:41.085244Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Let's focus on implementing these four components separately. First, let's focus on the parts that can be implemented in pure TensorFlow, namely the client and server update steps.\n","metadata":{"id":"uWXvjXPWeujU"}},{"cell_type":"markdown","source":"## TensorFlow Blocks","metadata":{"id":"3yKS4VkALo8g"}},{"cell_type":"markdown","source":"### Client update\n\nThe `tff.learning.models.FunctionalModel` can be used to do client training in essentially the same way you would train a TensorFlow model. In particular, one can use `tf.GradientTape` to compute the gradient on batches of data, then apply these gradient using a `client_optimizer`. This will only involve the trainable weights.\n","metadata":{"id":"bxpNYucgLo8g"}},{"cell_type":"code","source":"@tf.function\ndef client_update(model, dataset, initial_weights, client_optimizer):\n  \"\"\"Performs training (using the server model weights) on the client's dataset.\"\"\"\n  # Initialize the client model with the current server weights and the optimizer\n  # state.\n  client_weights = initial_weights.trainable\n  optimizer_state = client_optimizer.initialize(\n      tf.nest.map_structure(tf.TensorSpec.from_tensor, client_weights)\n  )\n\n  # Use the client_optimizer to update the local model.\n  for batch in dataset:\n    x, y = batch\n    with tf.GradientTape() as tape:\n      tape.watch(client_weights)\n      # Compute a forward pass on the batch of data\n      outputs = model.predict_on_batch(\n          model_weights=(client_weights, ()), x=x, training=True\n      )\n      loss = model.loss(output=outputs, label=y)\n\n    # Compute the corresponding gradient\n    grads = tape.gradient(loss, client_weights)\n\n    # Apply the gradient using a client optimizer.\n    optimizer_state, client_weights = client_optimizer.next(\n        optimizer_state, weights=client_weights, gradients=grads\n    )\n\n  return tff.learning.models.ModelWeights(client_weights, non_trainable=())","metadata":{"id":"c5rHPKreLo8g","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:41.089624Z","iopub.execute_input":"2024-12-18T17:20:41.089896Z","iopub.status.idle":"2024-12-18T17:20:41.098043Z","shell.execute_reply.started":"2024-12-18T17:20:41.089869Z","shell.execute_reply":"2024-12-18T17:20:41.097160Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Server Update\n\nThe server update for FedAvg is simpler than the client update. This tutorial will implement \"vanilla\" federated averaging, in which the server model weights are replaced by the average of the client model weights. Again, this only uses the trainable weights.","metadata":{"id":"pP0D9XtoLo8i"}},{"cell_type":"code","source":"@tf.function\ndef server_update(model, mean_client_weights):\n  \"\"\"Updates the server model weights as the average of the client model weights.\"\"\"\n  del model  # Unused, just take the mean_client_weights.\n  return mean_client_weights","metadata":{"id":"rYxErLvHLo8i","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:41.099177Z","iopub.execute_input":"2024-12-18T17:20:41.099778Z","iopub.status.idle":"2024-12-18T17:20:41.106878Z","shell.execute_reply.started":"2024-12-18T17:20:41.099737Z","shell.execute_reply":"2024-12-18T17:20:41.106097Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"The snippet could be simplified by simply returning the `mean_client_weights`. However, more advanced implementations of Federated Averaging use `mean_client_weights` with more sophisticated techniques, such as momentum or adaptivity.\n\n**Challenge**: Implement a version of `server_update` that updates the server weights to be the midpoint of model_weights and mean_client_weights. (Note: This kind of \"midpoint\" approach is analogous to recent work on the [Lookahead optimizer](https://arxiv.org/abs/1907.08610)!).","metadata":{"id":"ddCklfWlVr1U"}},{"cell_type":"markdown","source":"So far, this has only involved TensorFlow code. This is by design, as TFF allows you to use much of the TensorFlow code you're already familiar with. Next you will have to specify the **orchestration logic**, that is, the logic that dictates what the server broadcasts to the client, and what the client uploads to the server.\n\nThis will require the *Federated Core* of TFF.","metadata":{"id":"KuP9g6RFLo8k"}},{"cell_type":"markdown","source":"# Introduction to the Federated Core\n\nThe Federated Core (FC) is a set of lower-level interfaces that serve as the foundation for the `tff.learning` API. However, these interfaces are not limited to learning. In fact, they can be used for analytics and many other computations over distributed data.\n\nAt a high-level, the federated core is a development environment that enables compactly expressed program logic to combine TensorFlow code with distributed communication operators (such as distributed sums and broadcasts). The goal is to give researchers and practitioners explicit control over the distributed communication in their systems, without requiring system implementation details (such as specifying point-to-point network message exchanges).\n\nOne key point is that TFF is designed for privacy-preservation. Therefore, it allows explicit control over where data resides, to prevent unwanted accumulation of data at the centralized server location.","metadata":{"id":"0CgFLVPgLo8l"}},{"cell_type":"markdown","source":"## Federated data\n\nA key concept in TFF is \"federated data\", which refers to a collection of data items hosted across a group of devices in a distributed system (eg. client datasets, or the server model weights). The entire collection of values across all devices is represented as a single *federated value*.\n\nFor example, suppose there are client devices that each have a float representing the temperature of a sensor. These floats can be represented as a *federated float* by","metadata":{"id":"EYinjNqZLo8l"}},{"cell_type":"code","source":"federated_float_on_clients = tff.FederatedType(np.float32, tff.CLIENTS)","metadata":{"id":"7EJY0MHpLo8l","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:41.107778Z","iopub.execute_input":"2024-12-18T17:20:41.108277Z","iopub.status.idle":"2024-12-18T17:20:41.114860Z","shell.execute_reply.started":"2024-12-18T17:20:41.108233Z","shell.execute_reply":"2024-12-18T17:20:41.114024Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Federated types are specified by a type `T` of its member constituents (eg. `np.float32`) and a group `G` of devices. Typically, `G` is either `tff.CLIENTS` or `tff.SERVER`. Such a federated type is represented as `{T}@G`, as shown below.","metadata":{"id":"JSQAXD0FLo8n"}},{"cell_type":"code","source":"str(federated_float_on_clients)","metadata":{"id":"6mlPgubJLo8n","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:41.115750Z","iopub.execute_input":"2024-12-18T17:20:41.115960Z","iopub.status.idle":"2024-12-18T17:20:41.128675Z","shell.execute_reply.started":"2024-12-18T17:20:41.115937Z","shell.execute_reply":"2024-12-18T17:20:41.127801Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'{float32}@CLIENTS'"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"Why does TFF care so much about placements? A key goal of TFF is to enable writing code that could be deployed on a real distributed system. This means that it is vital to reason about which subsets of devices execute which code, and where different pieces of data reside.\n\nTFF focuses on three things: *data*, where the data is *placed*, and how the data is being *transformed*. The first two are encapsulated in federated types, while the last is encapsulated in *federated computations*.","metadata":{"id":"pjAQytkeLo8o"}},{"cell_type":"markdown","source":"## Federated computations","metadata":{"id":"ZLT2FmVMLo8p"}},{"cell_type":"markdown","source":"TFF is a strongly-typed functional programming environment whose basic units are *federated computations*. These are pieces of logic that accept federated values as input, and return federated values as output.\n\nFor example, suppose you wanted to average the temperatures on our client sensors. You could define the following (using our federated float):","metadata":{"id":"-XwDC1vTLo8p"}},{"cell_type":"code","source":"@tff.federated_computation(tff.FederatedType(np.float32, tff.CLIENTS))\ndef get_average_temperature(client_temperatures):\n  return tff.federated_mean(client_temperatures)","metadata":{"id":"IfwXDNR1Lo8p","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:41.129709Z","iopub.execute_input":"2024-12-18T17:20:41.129977Z","iopub.status.idle":"2024-12-18T17:20:41.136988Z","shell.execute_reply.started":"2024-12-18T17:20:41.129954Z","shell.execute_reply":"2024-12-18T17:20:41.136286Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"You might ask, how is this different from the `tf.function` decorator in TensorFlow? The key answer is that the code generated by `tff.federated_computation` is neither TensorFlow nor Python code; It is a specification of a distributed system in an internal platform-independent *glue language*.\n\nWhile this may sound complicated, you can think of TFF computations as functions with well-defined type signatures. These type signatures can be directly queried.","metadata":{"id":"iSgs6Te5Lo8r"}},{"cell_type":"code","source":"str(get_average_temperature.type_signature)","metadata":{"id":"mVq500KzG2mB","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:41.137960Z","iopub.execute_input":"2024-12-18T17:20:41.138208Z","iopub.status.idle":"2024-12-18T17:20:41.147181Z","shell.execute_reply.started":"2024-12-18T17:20:41.138184Z","shell.execute_reply":"2024-12-18T17:20:41.146361Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'({float32}@CLIENTS -> float32@SERVER)'"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"This `tff.federated_computation` accepts arguments of federated type `{float32}@CLIENTS`, and returns values of federated type `{float32}@SERVER`. Federated computations may also go from server to client, from client to client, or from server to server. Federated computations can also be composed like normal functions, as long as their type signatures match up.\n\nTo support development, TFF allows you to invoke a `tff.federated_computation` as a Python function. For example, you can call","metadata":{"id":"TveOYFfuLo8s"}},{"cell_type":"code","source":"get_average_temperature([68.5, 70.3, 69.8])","metadata":{"id":"PTowUHohG2mB","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:41.148149Z","iopub.execute_input":"2024-12-18T17:20:41.148850Z","iopub.status.idle":"2024-12-18T17:20:43.491051Z","shell.execute_reply.started":"2024-12-18T17:20:41.148824Z","shell.execute_reply":"2024-12-18T17:20:43.490033Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"69.53333"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"## Non-eager computations and TensorFlow","metadata":{"id":"ZXn-yje9RJ6H"}},{"cell_type":"markdown","source":"There are two key restrictions to be aware of. First, when the Python interpreter encounters a `tff.federated_computation` decorator, the function is traced once and serialized for future use. Due to the decentralized nature of Federated Learning, this future usage may occur elsewhere, such as a remote execution environment. Therefore, TFF computations are fundamentally *non-eager*. This behavior is somewhat analogous to that of the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) decorator in TensorFlow.\n\nSecond, a federated computation can only consist of federated operators (such as `tff.federated_mean`), they cannot contain TensorFlow operations. TensorFlow code must be confined to blocks decorated with `tff.tensorflow.computation`. Most ordinary TensorFlow code can be directly decorated, such as the following function that takes a number and adds `0.5` to it.","metadata":{"id":"nwyj8f3HLo8w"}},{"cell_type":"code","source":"@tff.tensorflow.computation(np.float32)\ndef add_half(x):\n  return tf.add(x, 0.5)","metadata":{"id":"huz3mNmMLo8w","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:43.492179Z","iopub.execute_input":"2024-12-18T17:20:43.492442Z","iopub.status.idle":"2024-12-18T17:20:43.502007Z","shell.execute_reply.started":"2024-12-18T17:20:43.492416Z","shell.execute_reply":"2024-12-18T17:20:43.501342Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"These also have type signatures, but *without placements*. For example, you can call","metadata":{"id":"5ptjWALDLo8y"}},{"cell_type":"code","source":"str(add_half.type_signature)","metadata":{"id":"34x5H2hzG2mC","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:43.502937Z","iopub.execute_input":"2024-12-18T17:20:43.503189Z","iopub.status.idle":"2024-12-18T17:20:43.511526Z","shell.execute_reply.started":"2024-12-18T17:20:43.503165Z","shell.execute_reply":"2024-12-18T17:20:43.510801Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'(float32 -> float32)'"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"This showcases an important difference between `tff.federated_computation` and `tff.tensorflow.computation`. The former has explicit placements, while the latter does not.\n\nYou can use `tff.tensorflow.computation` blocks in federated computations by specifying placements. Let's create a function that adds half, but only to federated floats at the clients. You can do this by using `tff.federated_map`, which applies a given `tff.tensorflow.computation`, while preserving the placement.","metadata":{"id":"WNjwrNMjLo8z"}},{"cell_type":"code","source":"@tff.federated_computation(tff.FederatedType(np.float32, tff.CLIENTS))\ndef add_half_on_clients(x):\n  return tff.federated_map(add_half, x)","metadata":{"id":"pG6nw3wiLo80","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:43.512409Z","iopub.execute_input":"2024-12-18T17:20:43.512654Z","iopub.status.idle":"2024-12-18T17:20:43.522130Z","shell.execute_reply.started":"2024-12-18T17:20:43.512630Z","shell.execute_reply":"2024-12-18T17:20:43.521454Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"This function is almost identical to `add_half`, except that it only accepts values with placement at `tff.CLIENTS`, and returns values with the same placement. This can be seen in its type signature:","metadata":{"id":"h4msKRKJLo81"}},{"cell_type":"code","source":"str(add_half_on_clients.type_signature)","metadata":{"id":"x3H-oeWIG2mC","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:43.523094Z","iopub.execute_input":"2024-12-18T17:20:43.523398Z","iopub.status.idle":"2024-12-18T17:20:43.531347Z","shell.execute_reply.started":"2024-12-18T17:20:43.523363Z","shell.execute_reply":"2024-12-18T17:20:43.530643Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'({float32}@CLIENTS -> {float32}@CLIENTS)'"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"In summary:\n\n*   TFF operates on federated values.\n*   Each federated value has a *federated type*, with a *type* (eg. `np.float32`) and a *placement* (eg. `tff.CLIENTS`).\n*   Federated values can be transformed using *federated computations*, which must be decorated with `tff.federated_computation` and a federated type signature.\n*   TensorFlow code must be contained in blocks with `tff.tensorflow.computation` decorators.\n*   These blocks can then be incorporated into federated computations.\n","metadata":{"id":"3JxQ0DeiLo83"}},{"cell_type":"markdown","source":"# Building your own Federated Learning algorithm, revisited\n\nNow that you've gotten a glimpse of the Federated Core, you can build our own federated learning algorithm. Remember that above, you defined an `initialize_fn` and `next_fn` for our algorithm. The `next_fn` will make use of the `client_update` and `server_update` you defined using pure TensorFlow code.\n\nHowever, in order to make our algorithm a federated computation, you will need both the `next_fn` and `initialize_fn` to each be a `tff.federated_computation`.","metadata":{"id":"PvyFWox3Lo83"}},{"cell_type":"markdown","source":"## TensorFlow Federated blocks","metadata":{"id":"CvY8fh1cLo84"}},{"cell_type":"markdown","source":"### Creating the initialization computation\n\nThe initialize function will be quite simple: You will create a model using `model_fn`. However, remember that you must separate out our TensorFlow code using `tff.tensorflow.computation`.","metadata":{"id":"g0zNTO7LLo84"}},{"cell_type":"code","source":"@tff.tensorflow.computation\ndef server_init():\n  return tff.learning.models.ModelWeights(*tff_model.initial_weights)","metadata":{"id":"jJY9xUBZLo84","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:43.532258Z","iopub.execute_input":"2024-12-18T17:20:43.532612Z","iopub.status.idle":"2024-12-18T17:20:43.543656Z","shell.execute_reply.started":"2024-12-18T17:20:43.532586Z","shell.execute_reply":"2024-12-18T17:20:43.542946Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"You can then pass this directly into a federated computation using `tff.federated_value`.","metadata":{"id":"SGlv8LLgLo85"}},{"cell_type":"code","source":"@tff.federated_computation\ndef initialize_fn():\n  return tff.federated_eval(server_init, tff.SERVER)","metadata":{"id":"m2hinzuRLo86","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:43.544445Z","iopub.execute_input":"2024-12-18T17:20:43.544719Z","iopub.status.idle":"2024-12-18T17:20:43.551319Z","shell.execute_reply.started":"2024-12-18T17:20:43.544695Z","shell.execute_reply":"2024-12-18T17:20:43.550557Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Creating the `next_fn`\n\nThe client and server update code can now be used to write the actual algorithm. First, you will turn the `client_update` into a `tff.tensorflow.computation` that accepts a client datasets and server weights, and outputs an updated client weights tensor.\n\nYou will need the corresponding types to properly decorate our function. Luckily, the type of the server weights can be extracted directly from our model.","metadata":{"id":"NFBghOgxLo88"}},{"cell_type":"code","source":"tf_dataset_type = tff.SequenceType(\n    tff.types.tensorflow_to_type(tff_model.input_spec)\n)","metadata":{"id":"ph_noHN2Lo88","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:43.552430Z","iopub.execute_input":"2024-12-18T17:20:43.552707Z","iopub.status.idle":"2024-12-18T17:20:43.560427Z","shell.execute_reply.started":"2024-12-18T17:20:43.552683Z","shell.execute_reply":"2024-12-18T17:20:43.559680Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"Let's look at the dataset type signature. Remember that you took 28 by 28 images (with integer labels) and flattened them.","metadata":{"id":"WMPgpTaW66qx"}},{"cell_type":"code","source":"str(tf_dataset_type)","metadata":{"id":"GE2sYVA9G2mE","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:43.561392Z","iopub.execute_input":"2024-12-18T17:20:43.561727Z","iopub.status.idle":"2024-12-18T17:20:43.569613Z","shell.execute_reply.started":"2024-12-18T17:20:43.561692Z","shell.execute_reply":"2024-12-18T17:20:43.568833Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'<float32[?,784],int32[?,1]>*'"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"You can also extract the model weights type by using our `server_init` function above.","metadata":{"id":"kuS8d0BHLo8-"}},{"cell_type":"code","source":"model_weights_type = server_init.type_signature.result","metadata":{"id":"4yx6CExMLo8-","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:43.570806Z","iopub.execute_input":"2024-12-18T17:20:43.571063Z","iopub.status.idle":"2024-12-18T17:20:43.577225Z","shell.execute_reply.started":"2024-12-18T17:20:43.571039Z","shell.execute_reply":"2024-12-18T17:20:43.576376Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"Examining the type signature, you'll be able to see the architecture of our model!","metadata":{"id":"mS-Eh6Xj7J15"}},{"cell_type":"code","source":"str(model_weights_type)","metadata":{"id":"_s8eFsyvG2mE","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:43.578258Z","iopub.execute_input":"2024-12-18T17:20:43.578752Z","iopub.status.idle":"2024-12-18T17:20:43.586572Z","shell.execute_reply.started":"2024-12-18T17:20:43.578727Z","shell.execute_reply":"2024-12-18T17:20:43.585901Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"'<trainable=<float32[784,10],float32[10]>,non_trainable=<>>'"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"You can now create our `tff.tensorflow.computation` for the client update.","metadata":{"id":"g1U1wTGRLo8_"}},{"cell_type":"code","source":"@tff.tensorflow.computation(tf_dataset_type, model_weights_type)\ndef client_update_fn(tf_dataset, server_weights):\n  client_optimizer = tff.learning.optimizers.build_sgdm(learning_rate=0.01)\n  return client_update(tff_model, tf_dataset, server_weights, client_optimizer)","metadata":{"id":"Q0W05pMWLo9A","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:43.587716Z","iopub.execute_input":"2024-12-18T17:20:43.588013Z","iopub.status.idle":"2024-12-18T17:20:44.537139Z","shell.execute_reply.started":"2024-12-18T17:20:43.587968Z","shell.execute_reply":"2024-12-18T17:20:44.536438Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"The `tff.tensorflow.computation` version of the server update can be defined in a similar way, using types you've already extracted.","metadata":{"id":"uP5quaAuLo9B"}},{"cell_type":"code","source":"@tff.tensorflow.computation(model_weights_type)\ndef server_update_fn(mean_client_weights):\n  return server_update(tff_model, mean_client_weights)","metadata":{"id":"F4WvQtVzLo9B","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:44.541874Z","iopub.execute_input":"2024-12-18T17:20:44.542153Z","iopub.status.idle":"2024-12-18T17:20:44.577909Z","shell.execute_reply.started":"2024-12-18T17:20:44.542126Z","shell.execute_reply":"2024-12-18T17:20:44.577281Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"Last, but not least, you need to create the `tff.federated_computation` that brings this all together. This function will accept two *federated values*, one corresponding to the server weights (with placement `tff.SERVER`), and the other corresponding to the client datasets (with placement `tff.CLIENTS`).\n\nNote that both these types were defined above! You simply need to give them the proper placement using `tff.FederatedType`.","metadata":{"id":"SImhLbu4Lo9D"}},{"cell_type":"code","source":"federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)\nfederated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)","metadata":{"id":"ekPsA8AsLo9D","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:44.578782Z","iopub.execute_input":"2024-12-18T17:20:44.579051Z","iopub.status.idle":"2024-12-18T17:20:44.583272Z","shell.execute_reply.started":"2024-12-18T17:20:44.579025Z","shell.execute_reply":"2024-12-18T17:20:44.582391Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"Remember the 4 elements of an FL algorithm?\n\n1. A server-to-client broadcast step.\n2. A local client update step.\n3. A client-to-server upload step.\n4. A server update step.\n\nNow that you've built up the above, each part can be compactly represented as a single line of TFF code. This simplicity is why you had to take extra care to specify things such as federated types!","metadata":{"id":"7FXAX7vGLo9G"}},{"cell_type":"code","source":"@tff.federated_computation(federated_server_type, federated_dataset_type)\ndef next_fn(server_weights, federated_dataset):\n  # Broadcast the server weights to the clients.\n  server_weights_at_client = tff.federated_broadcast(server_weights)\n\n  # Each client computes their updated weights.\n  client_weights = tff.federated_map(\n      client_update_fn, (federated_dataset, server_weights_at_client)\n  )\n\n  # The server averages these updates.\n  mean_client_weights = tff.federated_mean(client_weights)\n\n  # The server updates its model.\n  server_weights = tff.federated_map(server_update_fn, mean_client_weights)\n\n  return server_weights","metadata":{"id":"Epc7MwfELo9G","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:44.584332Z","iopub.execute_input":"2024-12-18T17:20:44.584813Z","iopub.status.idle":"2024-12-18T17:20:44.596796Z","shell.execute_reply.started":"2024-12-18T17:20:44.584787Z","shell.execute_reply":"2024-12-18T17:20:44.596113Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"You now have a `tff.federated_computation` for both the algorithm initialization, and for running one step of the algorithm. To finish our algorithm, you pass these into `tff.templates.IterativeProcess`.","metadata":{"id":"kWomG3TtLo9I"}},{"cell_type":"code","source":"federated_algorithm = tff.templates.IterativeProcess(\n    initialize_fn=initialize_fn,\n    next_fn=next_fn\n)","metadata":{"id":"GxdWgEddLo9I","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:44.597710Z","iopub.execute_input":"2024-12-18T17:20:44.597944Z","iopub.status.idle":"2024-12-18T17:20:44.609223Z","shell.execute_reply.started":"2024-12-18T17:20:44.597920Z","shell.execute_reply":"2024-12-18T17:20:44.608556Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"Let's look at the *type signature* of the `initialize` and `next` functions of our iterative process.","metadata":{"id":"7Z__9k-Dc1I3"}},{"cell_type":"code","source":"str(federated_algorithm.initialize.type_signature)","metadata":{"id":"EmyYgDNdG2mF","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:44.610471Z","iopub.execute_input":"2024-12-18T17:20:44.610870Z","iopub.status.idle":"2024-12-18T17:20:44.621069Z","shell.execute_reply.started":"2024-12-18T17:20:44.610833Z","shell.execute_reply":"2024-12-18T17:20:44.620383Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"'( -> <trainable=<float32[784,10],float32[10]>,non_trainable=<>>@SERVER)'"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"This reflects the fact that `federated_algorithm.initialize` is a no-arg function that returns a single-layer model (with a 784-by-10 weight matrix, and 10 bias units).","metadata":{"id":"UyyEi5Kec90_"}},{"cell_type":"code","source":"str(federated_algorithm.next.type_signature)","metadata":{"id":"ZRwHwQsCG2mG","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:44.622000Z","iopub.execute_input":"2024-12-18T17:20:44.622310Z","iopub.status.idle":"2024-12-18T17:20:44.632795Z","shell.execute_reply.started":"2024-12-18T17:20:44.622267Z","shell.execute_reply":"2024-12-18T17:20:44.631950Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'(<server_weights=<trainable=<float32[784,10],float32[10]>,non_trainable=<>>@SERVER,federated_dataset={<float32[?,784],int32[?,1]>*}@CLIENTS> -> <trainable=<float32[784,10],float32[10]>,non_trainable=<>>@SERVER)'"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"Here, one can see that `federated_algorithm.next` accepts a server model and client data, and returns an updated server model.","metadata":{"id":"efpdHodmdU_6"}},{"cell_type":"markdown","source":"## Evaluating the algorithm","metadata":{"id":"4UYZ3qeMLo9N"}},{"cell_type":"markdown","source":"Let's run a few rounds, and see how the loss changes. First, you will define an evaluation function using the *centralized* approach discussed in the second tutorial.\n\nYou will first create a centralized evaluation dataset, and then apply the same preprocessing you used for the training data.","metadata":{"id":"jwd9Gs0ULo9O"}},{"cell_type":"code","source":"central_emnist_test = emnist_test.create_tf_dataset_from_all_clients()\ncentral_emnist_test = preprocess(central_emnist_test)","metadata":{"id":"EdNgYoIwLo9P","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:44.633726Z","iopub.execute_input":"2024-12-18T17:20:44.633995Z","iopub.status.idle":"2024-12-18T17:20:44.804966Z","shell.execute_reply.started":"2024-12-18T17:20:44.633970Z","shell.execute_reply":"2024-12-18T17:20:44.804241Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"Next, you will write a function that accepts a server state, and uses Keras to evaluate on the test dataset. If you're familiar with `tf.Keras`, this will all look familiar, though note the use of `set_weights`!","metadata":{"id":"7R50NZ35dphE"}},{"cell_type":"code","source":"def evaluate(model_weights):\n  keras_model = create_keras_model()\n  keras_model.compile(\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n  )\n  model_weights.assign_weights_to(keras_model)\n  keras_model.evaluate(central_emnist_test)","metadata":{"id":"I5UEX4EWLo9Q","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:44.806204Z","iopub.execute_input":"2024-12-18T17:20:44.806577Z","iopub.status.idle":"2024-12-18T17:20:44.811672Z","shell.execute_reply.started":"2024-12-18T17:20:44.806538Z","shell.execute_reply":"2024-12-18T17:20:44.810816Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"Now, let's initialize our algorithm and evaluate on the test set.","metadata":{"id":"hygoBACkLo9S"}},{"cell_type":"code","source":"server_state = federated_algorithm.initialize()\nevaluate(server_state)","metadata":{"id":"CDarZn71G2mH","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:44.812720Z","iopub.execute_input":"2024-12-18T17:20:44.813012Z","iopub.status.idle":"2024-12-18T17:20:48.777220Z","shell.execute_reply.started":"2024-12-18T17:20:44.812984Z","shell.execute_reply":"2024-12-18T17:20:48.776561Z"}},"outputs":[{"name":"stdout","text":"2042/2042 [==============================] - 4s 2ms/step - loss: 2.8479 - sparse_categorical_accuracy: 0.1027\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"Let's train for a few rounds and see if anything changes.","metadata":{"id":"2knqix2cLo9U"}},{"cell_type":"code","source":"for _ in range(15):\n  server_state = federated_algorithm.next(server_state, federated_train_data)","metadata":{"id":"v1zBlzFILo9U","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:48.778379Z","iopub.execute_input":"2024-12-18T17:20:48.778661Z","iopub.status.idle":"2024-12-18T17:20:52.320592Z","shell.execute_reply.started":"2024-12-18T17:20:48.778633Z","shell.execute_reply":"2024-12-18T17:20:52.319768Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"evaluate(server_state)","metadata":{"id":"2QDhaI_DG2mH","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:20:52.321829Z","iopub.execute_input":"2024-12-18T17:20:52.322208Z","iopub.status.idle":"2024-12-18T17:20:55.903839Z","shell.execute_reply.started":"2024-12-18T17:20:52.322162Z","shell.execute_reply":"2024-12-18T17:20:55.903171Z"}},"outputs":[{"name":"stdout","text":"2042/2042 [==============================] - 4s 2ms/step - loss: 2.5867 - sparse_categorical_accuracy: 0.0980\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"There is a slight decrease in the loss function. While the jump is small, you've only performed 15 training rounds, and on a small subset of clients. To see better results, you may have to do hundreds if not thousands of rounds.","metadata":{"id":"XM34ammUW-T3"}},{"cell_type":"markdown","source":"## Modifying our algorithm","metadata":{"id":"o13H5dDFXRFn"}},{"cell_type":"markdown","source":"At this point, let's stop and think about what you've accomplished. You've implemented Federated Averaging directly by combining pure TensorFlow code (for the client and server updates) with federated computations from the Federated Core of TFF.\n\nTo perform more sophisticted learning, you can simply alter what you have above. In particular, by editing the pure TF code above, you can change how the client performs training, or how the server updates its model.\n\n**Challenge:** Add [gradient clipping](https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48) to the `client_update` function.\n","metadata":{"id":"Qt4jVD21XTL-"}},{"cell_type":"markdown","source":"If you wanted to make larger changes, you could also have the server store and broadcast more data. For example, the server could also store the client learning rate, and make it decay over time! Note that this will require changes to the type signatures used in the `tff.tensorflow.computation` calls above.\n\n**Harder Challenge:** Implement Federated Averaging with learning rate decay on the clients.\n\nAt this point, you may begin to realize how much flexibility there is in what you can implement in this framework. For ideas (including the answer to the harder challenge above) you can see the source-code for [`tff.learning.algorithms.build_weighted_fed_avg`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/algorithms/build_weighted_fed_avg), or check out various [research projects](https://github.com/google-research/federated) using TFF.","metadata":{"id":"p7wvwgS7bCTy"}}]}