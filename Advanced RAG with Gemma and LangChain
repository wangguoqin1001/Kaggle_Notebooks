{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":6140902,"sourceType":"datasetVersion","datasetId":3475059},{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388,"modelId":3533}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction\n\nThis notebook demonstrates how you can build an advanced RAG (Retrieval Augmented Generation) for explaining concepts from Kaggle competition solution write-ups.\n\nWe are going to use the following public dataset : https://www.kaggle.com/datasets/thedrcat/kaggle-winning-solutions-methods\n\nHere is the pipeline we are going to build :\n\n- [Gemma](https://www.kaggle.com/models/google/gemma) as LLM\n- [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) for the embeddings\n- [FAISS](https://github.com/facebookresearch/faiss) as vector database for the embeddings\n- [LangChain](https://www.langchain.com/) for orchestration\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Installation and imports\n\n## 2.1 Install packages","metadata":{}},{"cell_type":"code","source":"!pip install -q -U accelerate bitsandbytes langchain langchain-community sentence-transformers ragatouille faiss-gpu rank_bm25\n# ! pip install -q -U beautifulsoup4 # Install beautifulsoup4 if you are running the notebook not in Kaggle\n!pip install -q -U keras-nlp\n!pip install -q -U keras>3","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-25T06:54:47.951250Z","iopub.execute_input":"2024-12-25T06:54:47.951473Z","iopub.status.idle":"2024-12-25T06:56:03.280831Z","shell.execute_reply.started":"2024-12-25T06:54:47.951453Z","shell.execute_reply":"2024-12-25T06:56:03.279756Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2.2 Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport keras\nimport keras_nlp\nimport pandas as pd\n\nfrom bs4 import BeautifulSoup\nfrom typing import Optional, List, Tuple\nfrom IPython.display import display, Markdown\n\nfrom transformers import AutoTokenizer\nfrom ragatouille import RAGPretrainedModel\nfrom langchain.docstore.document import Document\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain_core.runnables import ConfigurableField\nfrom langchain_community.vectorstores import FAISS, Chroma\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import DataFrameLoader\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\" # Avoid memory fragmentation on JAX backend.","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-25T06:56:03.282617Z","iopub.execute_input":"2024-12-25T06:56:03.282893Z","iopub.status.idle":"2024-12-25T06:56:26.260420Z","shell.execute_reply.started":"2024-12-25T06:56:03.282869Z","shell.execute_reply":"2024-12-25T06:56:26.259539Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2024-12-25 06:56:05.367779: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-25 06:56:05.367890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-25 06:56:05.536994: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# 3. Prepare the data\n## 3.1 Preprocessing","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/kaggle-winning-solutions-methods/kaggle_winning_solutions_methods.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:56:26.261421Z","iopub.execute_input":"2024-12-25T06:56:26.261935Z","iopub.status.idle":"2024-12-25T06:56:27.547200Z","shell.execute_reply.started":"2024-12-25T06:56:26.261913Z","shell.execute_reply":"2024-12-25T06:56:27.546380Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                link  place  \\\n0  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n1  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n2  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n3  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n4  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n\n                              competition_name     prize   team      kind  \\\n0  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n1  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n2  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n3  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n4  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n\n                    metric  year      nm  \\\n0  PostProcessorKernelDesc  2023  406306   \n1  PostProcessorKernelDesc  2023  406306   \n2  PostProcessorKernelDesc  2023  406306   \n3  PostProcessorKernelDesc  2023  406306   \n4  PostProcessorKernelDesc  2023  406306   \n\n                                             writeup  num_tokens  \\\n0  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n1  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n2  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n3  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n4  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n\n                                             methods       cleaned_methods  \n0  ['EfficientNet-B0', 'Data Augmentation', 'Norm...  Replace augmentation  \n1  ['EfficientNet-B0', 'Data Augmentation', 'Norm...    Finger tree rotate  \n2  ['EfficientNet-B0', 'Data Augmentation', 'Norm...     Data Augmentation  \n3  ['EfficientNet-B0', 'Data Augmentation', 'Norm...    Onecycle scheduler  \n4  ['EfficientNet-B0', 'Data Augmentation', 'Norm...             Flip pose  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>link</th>\n      <th>place</th>\n      <th>competition_name</th>\n      <th>prize</th>\n      <th>team</th>\n      <th>kind</th>\n      <th>metric</th>\n      <th>year</th>\n      <th>nm</th>\n      <th>writeup</th>\n      <th>num_tokens</th>\n      <th>methods</th>\n      <th>cleaned_methods</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n      <td>2</td>\n      <td>Google - Isolated Sign Language Recognition</td>\n      <td>$100,000</td>\n      <td>1,165</td>\n      <td>Research</td>\n      <td>PostProcessorKernelDesc</td>\n      <td>2023</td>\n      <td>406306</td>\n      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n      <td>2914</td>\n      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n      <td>Replace augmentation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n      <td>2</td>\n      <td>Google - Isolated Sign Language Recognition</td>\n      <td>$100,000</td>\n      <td>1,165</td>\n      <td>Research</td>\n      <td>PostProcessorKernelDesc</td>\n      <td>2023</td>\n      <td>406306</td>\n      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n      <td>2914</td>\n      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n      <td>Finger tree rotate</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n      <td>2</td>\n      <td>Google - Isolated Sign Language Recognition</td>\n      <td>$100,000</td>\n      <td>1,165</td>\n      <td>Research</td>\n      <td>PostProcessorKernelDesc</td>\n      <td>2023</td>\n      <td>406306</td>\n      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n      <td>2914</td>\n      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n      <td>Data Augmentation</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n      <td>2</td>\n      <td>Google - Isolated Sign Language Recognition</td>\n      <td>$100,000</td>\n      <td>1,165</td>\n      <td>Research</td>\n      <td>PostProcessorKernelDesc</td>\n      <td>2023</td>\n      <td>406306</td>\n      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n      <td>2914</td>\n      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n      <td>Onecycle scheduler</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n      <td>2</td>\n      <td>Google - Isolated Sign Language Recognition</td>\n      <td>$100,000</td>\n      <td>1,165</td>\n      <td>Research</td>\n      <td>PostProcessorKernelDesc</td>\n      <td>2023</td>\n      <td>406306</td>\n      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n      <td>2914</td>\n      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n      <td>Flip pose</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"Let's look at an example of a write-up","metadata":{}},{"cell_type":"code","source":"data['writeup'][42]","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:56:27.549026Z","iopub.execute_input":"2024-12-25T06:56:27.549324Z","iopub.status.idle":"2024-12-25T06:56:27.558582Z","shell.execute_reply.started":"2024-12-25T06:56:27.549301Z","shell.execute_reply":"2024-12-25T06:56:27.557818Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'<p>Here is a quick overview of the 5th-place solution.</p>\\n<ol>\\n<li><p><strong>we applied various augmentations like flip, concatenation, etc</strong><br>\\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -&gt; 0.78)</p></li>\\n<li><p><strong>the model is only a transformer model based on the public kernels</strong><br>\\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78-&gt;0.8) in public LB.<br>\\n2.1.1. 3 layers of transformer with the embedding size 480.</p></li>\\n<li><p><strong>Preprocessing by mean and std of single sign sequence</strong><br>\\n3.1. the preprocessing does affect the final performance. <br>\\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.</p></li>\\n<li><p><strong>Feature engineering like distances between points</strong><br>\\n4.1. we selected and used around 106 points (as the public notebook by Heck).<br>\\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.</p></li>\\n<li><p><strong>some methods to prevent overfitting like awp, random mask of frames, ema, etc …</strong></p></li>\\n</ol>\\n<p>many thanks to my teammates  <a href=\"https://www.kaggle.com/qiaoshiji\" target=\"_blank\">@qiaoshiji</a> <a href=\"https://www.kaggle.com/zengzhaoyang\" target=\"_blank\">@zengzhaoyang</a></p>\\n<p>The source code for training models can be found here : <a href=\"https://github.com/zhouyuanzhe/kaggleasl5thplacesolution\" target=\"_blank\">https://github.com/zhouyuanzhe/kaggleasl5thplacesolution</a></p>'"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"The write-ups contain HTML tags and links that are not relevant to our knowledge base. So we'll use BeautifulSoup to extract all the texts and concatenate them into a single one.","metadata":{}},{"cell_type":"code","source":"%%time\n\ndef clean_html(html_content):\n    \"\"\"Function to clean up HTML tags in each writeup\"\"\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n    # Use '\\n' as a separator to preserve the structure of the various parts\n    text = soup.get_text(separator='\\n', strip=True)\n    return text\n\ndata['writeup'] = data['writeup'].apply(clean_html) # This might take a while","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:56:27.559596Z","iopub.execute_input":"2024-12-25T06:56:27.559938Z","iopub.status.idle":"2024-12-25T06:56:53.142230Z","shell.execute_reply.started":"2024-12-25T06:56:27.559916Z","shell.execute_reply":"2024-12-25T06:56:53.141428Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CPU times: user 25.5 s, sys: 78.8 ms, total: 25.6 s\nWall time: 25.6 s\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"**Here is the result :**","metadata":{}},{"cell_type":"code","source":"print(data['writeup'][42])","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:56:53.143382Z","iopub.execute_input":"2024-12-25T06:56:53.143726Z","iopub.status.idle":"2024-12-25T06:56:53.148571Z","shell.execute_reply.started":"2024-12-25T06:56:53.143697Z","shell.execute_reply":"2024-12-25T06:56:53.147743Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"**This looks good now !**\n\nTo build our knowledge base, which will serve as the context for the LLM, we will concatenate relevant information such as the name of the competition, the rank of the competitors who proposed the solution and the solution itself.\n\nNote that we can also add other columns that might also be relevant to answering the user's query.\nBut let's keep it simple for now.\n","metadata":{}},{"cell_type":"code","source":"data['LLM_context'] = (\n    \"Competition Name: \" + data['competition_name'] +\n    \",\\nPlace: \" + data['place'].astype(str) +\n    \",\\nMethods Used: \" + data['methods'] +\n    \",\\nSolution: \" + data['writeup']\n)\n\nprint(data['LLM_context'][42])","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:56:53.149726Z","iopub.execute_input":"2024-12-25T06:56:53.150021Z","iopub.status.idle":"2024-12-25T06:56:53.267935Z","shell.execute_reply.started":"2024-12-25T06:56:53.150003Z","shell.execute_reply":"2024-12-25T06:56:53.267032Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 5,\nMethods Used: ['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention'],\nSolution: Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"data = data.drop(\"writeup\", axis=1) # We remove 'writeup' column as it is already in LLM_context","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:56:53.268893Z","iopub.execute_input":"2024-12-25T06:56:53.269174Z","iopub.status.idle":"2024-12-25T06:56:53.282384Z","shell.execute_reply.started":"2024-12-25T06:56:53.269153Z","shell.execute_reply":"2024-12-25T06:56:53.281345Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 3.2 Loading data\n\nWe'll now use LangChain's [DataFrameLoader](https://python.langchain.com/docs/integrations/document_loaders/pandas_dataframe) to store the information as a LangChain [Documents](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html) list. \n\nThe **`Document`** class in **LangChain** serves as a fundamental building block for storing text and associated metadata. Let's explore its key features:\n\n1. **Purpose**: The `Document` class is designed to hold a piece of text along with relevant metadata. You can think of it as a container for textual content.\n\n2. **Attributes**:\n    - **`page_content`**: This attribute stores the actual text content of the document.\n    - **`metadata` (Optional)**: You can attach arbitrary metadata to the document. For example, this could include information about the source of the content or relationships to other documents.\n\nFor more detailed information, you can refer to the [official LangChain documentation](https://python.langchain.com/docs/modules/data_connection/document_loaders/) .\n\n","metadata":{}},{"cell_type":"code","source":"loader = DataFrameLoader(data, page_content_column=\"LLM_context\")\ndocs = loader.load()\ndocs_subset = docs[:1500] # Part of the data is used to reduce execution time.","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:56:53.283548Z","iopub.execute_input":"2024-12-25T06:56:53.284101Z","iopub.status.idle":"2024-12-25T06:56:54.169227Z","shell.execute_reply.started":"2024-12-25T06:56:53.284054Z","shell.execute_reply":"2024-12-25T06:56:54.168522Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(\"-----------PAGE CONTENT-----------\")\nprint(docs_subset[42].page_content)\nprint(\"\\n\\n-----------METADATA-----------\\n\")\nprint(docs_subset[42].metadata)","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:56:54.172511Z","iopub.execute_input":"2024-12-25T06:56:54.172763Z","iopub.status.idle":"2024-12-25T06:56:54.177445Z","shell.execute_reply.started":"2024-12-25T06:56:54.172743Z","shell.execute_reply":"2024-12-25T06:56:54.176581Z"},"trusted":true},"outputs":[{"name":"stdout","text":"-----------PAGE CONTENT-----------\nCompetition Name: Google - Isolated Sign Language Recognition,\nPlace: 5,\nMethods Used: ['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention'],\nSolution: Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution\n\n\n-----------METADATA-----------\n\n{'link': 'https://www.kaggle.com/c/asl-signs/discussion/406491', 'place': 5, 'competition_name': 'Google - Isolated Sign Language Recognition', 'prize': '$100,000', 'team': '1,165', 'kind': 'Research', 'metric': 'PostProcessorKernelDesc', 'year': 2023, 'nm': 406491, 'num_tokens': 473, 'methods': \"['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention']\", 'cleaned_methods': 'Post-processing'}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"\n# 4. Chunking","metadata":{}},{"cell_type":"markdown","source":"To create relevant answer snippets for the LLM, we break down the knowledge base documents into smaller pieces. These chunks should capture specific ideas, not be too short (cutting off the thought) or too long (making it hard to find the main point).\n\nWe use \"recursive chunking\" to achieve this. It works by repeatedly splitting the text into smaller parts using a list of separators (e.g. [\"\\n\\n\", \"\\n\", \".\", \"\"]), starting with the most important (like double line breaks) and moving down to less important ones (like sentence ends). This ensures that chunks are neither too large nor too small for the LLM to process effectively.","metadata":{}},{"cell_type":"code","source":"EMBEDDING_MODEL_NAME = \"BAAI/bge-base-en-v1.5\"\nCHUNK_SIZE = 512 # We choose a chunk size adapted to our model","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:56:54.178453Z","iopub.execute_input":"2024-12-25T06:56:54.178661Z","iopub.status.idle":"2024-12-25T06:56:54.190519Z","shell.execute_reply.started":"2024-12-25T06:56:54.178644Z","shell.execute_reply":"2024-12-25T06:56:54.189897Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"%%time\n\ndef split_documents(\n    chunk_size: int,\n    knowledge_base: List[Document],\n    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n) -> List[Document]:\n    \"\"\"\n    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n    \"\"\"\n    \n    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n        AutoTokenizer.from_pretrained(tokenizer_name),\n        chunk_size=chunk_size,\n        chunk_overlap=int(chunk_size / 10),\n        add_start_index=True,\n        strip_whitespace=True,\n    )\n\n    docs_processed = []\n    for doc in knowledge_base:\n        docs_processed += text_splitter.split_documents([doc])\n\n    # Remove duplicates\n    unique_texts = {}\n    docs_processed_unique = []\n    for doc in docs_processed:\n        if doc.page_content not in unique_texts:\n            unique_texts[doc.page_content] = True\n            docs_processed_unique.append(doc)\n\n    return docs_processed_unique\n\nchunked_docs = split_documents(\n    CHUNK_SIZE,  \n    docs_subset,\n    tokenizer_name=EMBEDDING_MODEL_NAME,\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-25T06:56:54.191518Z","iopub.execute_input":"2024-12-25T06:56:54.191784Z","iopub.status.idle":"2024-12-25T06:57:20.655391Z","shell.execute_reply.started":"2024-12-25T06:56:54.191764Z","shell.execute_reply":"2024-12-25T06:57:20.654468Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe3a280509bb4366b0c1b77f0447d898"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cab00f57dd7844a383a22b9eb57b2354"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1361bbd4564455b8481dc9336d496e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea76d23b02174e6cbae2a294108dab75"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (956 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 25.9 s, sys: 20.2 ms, total: 25.9 s\nWall time: 26.5 s\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"**If the dataset is too large, chunking all the documents can take a long time. To speed things up, consider working with a representative subset of the data.**\n","metadata":{}},{"cell_type":"markdown","source":"# 5. Embeddings and retriever\n## 5.1 Embeddings\n\nNow that the documents are correctly sized, we're ready to start building a database that includes their embeddings.\n\nTo create embeddings for document segments, we'll be using LangChain's [HuggingFaceEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub) in conjunction with the [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) embeddings model. A broader selection of text embedding models can be found on the Hugging Face Hub, where the most effective models are highlighted in the [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).","metadata":{}},{"cell_type":"code","source":"%%time\n\nembedding_model = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    multi_process=True,\n    model_kwargs={\"device\": \"cuda\"},\n    encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-25T06:57:20.656358Z","iopub.execute_input":"2024-12-25T06:57:20.656576Z","iopub.status.idle":"2024-12-25T06:57:24.690338Z","shell.execute_reply.started":"2024-12-25T06:57:20.656558Z","shell.execute_reply":"2024-12-25T06:57:24.689390Z"},"trusted":true},"outputs":[{"name":"stderr","text":"<timed exec>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c30a33087427485fa13e7a9c48224309"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9390f3f018914c40a58032264b5a6159"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e87cc8d5b32431fa97a9a25f5d6bc05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"013da960cf654f04ac81839756867e64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d7a8293ea74eb4bd72c4d3e05f00c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e690e97be0714f7da18ccea566d1138f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94806fbccd604f3dbf0c1c96871909b8"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 1.1 s, sys: 1.09 s, total: 2.19 s\nWall time: 4.03 s\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 5.2 Fusion retrieval or hybrid search\n\nThis concept, though not entirely new, involves integrating the strengths of two distinct search methods: traditional keyword-based search, which employs sparse retrieval algorithms such as [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) or the search industry standard [BM25](https://en.wikipedia.org/wiki/Okapi_BM25), and contemporary semantic or vector search.\n\nThe challenge lies in effectively merging the results obtained from these different similarity scoring methods. This issue is typically addressed using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) (RRF) algorithm, which re-ranks the retrieved results to produce the final output.\n\nIn LangChain this is implemented in the [Ensemble Retriever class](https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble), combining a list of retrievers you define, for example a Faiss vector index and a BM25 based retriever and using RRF for reranking.\n\n\nAs vector database, we'll use [FAISS](https://github.com/facebookresearch/faiss), a library developed by Facebook AI. FAISS specializes in the efficient similarity search and clustering of dense vectors, which suits our needs perfectly. Currently, FAISS is among the top libraries for conducting Nearest Neighbor (NN) search in large datasets.\n\n","metadata":{}},{"cell_type":"code","source":"num_docs = 5 # Default number of documents to retrieve\n\nbm25_retriever = BM25Retriever.from_documents(\n    chunked_docs\n    ).configurable_fields(\n    k=ConfigurableField(\n        id=\"search_kwargs_bm25\",\n        name=\"k\",\n        description=\"The search kwargs to use\",\n    )\n)\n\nfaiss_vectorstore = FAISS.from_documents(\n    chunked_docs, embedding_model, distance_strategy=DistanceStrategy.COSINE\n)\n\nfaiss_retriever = faiss_vectorstore.as_retriever(\n    search_kwargs={\"k\": num_docs}\n    ).configurable_fields(\n    search_kwargs=ConfigurableField(\n        id=\"search_kwargs_faiss\",\n        name=\"Search Kwargs\",\n        description=\"The search kwargs to use\",\n    )\n)\n\n# initialize the ensemble retriever\nvector_database = EnsembleRetriever(\n    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5] # You can adjust the weight of each retriever in the EnsembleRetriever\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:57:24.691312Z","iopub.execute_input":"2024-12-25T06:57:24.691577Z","iopub.status.idle":"2024-12-25T06:57:40.540962Z","shell.execute_reply.started":"2024-12-25T06:57:24.691555Z","shell.execute_reply":"2024-12-25T06:57:40.540071Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"I pick the row 42 as base to generate questions for the model.","metadata":{}},{"cell_type":"code","source":"print(data.iloc[42, :])","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:57:40.542057Z","iopub.execute_input":"2024-12-25T06:57:40.542345Z","iopub.status.idle":"2024-12-25T06:57:40.547717Z","shell.execute_reply.started":"2024-12-25T06:57:40.542324Z","shell.execute_reply":"2024-12-25T06:57:40.546850Z"},"trusted":true},"outputs":[{"name":"stdout","text":"link                https://www.kaggle.com/c/asl-signs/discussion/...\nplace                                                               5\ncompetition_name          Google - Isolated Sign Language Recognition\nprize                                                        $100,000\nteam                                                            1,165\nkind                                                         Research\nmetric                                        PostProcessorKernelDesc\nyear                                                             2023\nnm                                                             406491\nnum_tokens                                                        473\nmethods             ['Augmentation', 'Transformer model', 'Preproc...\ncleaned_methods                                       Post-processing\nLLM_context         Competition Name: Google - Isolated Sign Langu...\nName: 42, dtype: object\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(data['LLM_context'][42])","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:57:40.548719Z","iopub.execute_input":"2024-12-25T06:57:40.548953Z","iopub.status.idle":"2024-12-25T06:57:40.559322Z","shell.execute_reply.started":"2024-12-25T06:57:40.548933Z","shell.execute_reply":"2024-12-25T06:57:40.558488Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 5,\nMethods Used: ['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention'],\nSolution: Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Below are several questions that can be derived from the above solution description (generated by a LLM):\n\n- \"What specific augmentations were applied to improve the cross-validation score, and how did each contribute to the increase?\"\n- \"Why was a transformer model chosen for this solution, and how did public kernels influence its development?\"\n- \"Can you detail the impact of increasing the model's parameters on its performance on the public leaderboard?\"\n- \"Describe the architecture of the 3-layer transformer model, specifically focusing on the choice of embedding size.\"\n- \"How does preprocessing with mean and standard deviation of single sign sequences enhance model performance?\"\n- \"What process did you use to determine that using mean and std of single sign sequences yields better cross-validation scores?\"\n- \"In terms of feature engineering, why were distances between points chosen as a feature, and how were they calculated?\"\n- \"How did the selection of 106 points influence the model's ability to understand and process the data?\"\n- \"What methods were implemented to prevent overfitting, and can you explain how each method contributed to model robustness?\"\n- \"Reflecting on your teamwork, how did your teammates contribute to the development and success of the solution?\"\n\nYou can use them as inspiration or rephrase them before asking Gemma the question. I'll choose one to test the model.","metadata":{}},{"cell_type":"markdown","source":"Let's make a simple query on our database !","metadata":{}},{"cell_type":"code","source":"user_query = \"\"\"\nI want to understand the 5th-place solution in the 'Google - Isolated Sign Language Recognition' competition. \nWhat overfitting prevention techniques were used, and how did they ensure model robustness?\n\"\"\"\nconfig = {\"configurable\": {\"search_kwargs_faiss\": {\"k\": 5}, \"search_kwargs_bm25\": 5}}\nretrieved_docs = vector_database.invoke(user_query, config=config)\nprint(\"----------------------Top document content----------------------\")\nprint(retrieved_docs[0].page_content)\nprint(\"----------------------Top document metadata----------------------\")\nprint(retrieved_docs[0].metadata)","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:57:40.560501Z","iopub.execute_input":"2024-12-25T06:57:40.561136Z","iopub.status.idle":"2024-12-25T06:57:47.963826Z","shell.execute_reply.started":"2024-12-25T06:57:40.561068Z","shell.execute_reply":"2024-12-25T06:57:47.962856Z"},"trusted":true},"outputs":[{"name":"stdout","text":"----------------------Top document content----------------------\nCompetition Name: Google - Isolated Sign Language Recognition,\nPlace: 5,\nMethods Used: ['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention'],\nSolution: Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution\n----------------------Top document metadata----------------------\n{'link': 'https://www.kaggle.com/c/asl-signs/discussion/406491', 'place': 5, 'competition_name': 'Google - Isolated Sign Language Recognition', 'prize': '$100,000', 'team': '1,165', 'kind': 'Research', 'metric': 'PostProcessorKernelDesc', 'year': 2023, 'nm': 406491, 'num_tokens': 473, 'methods': \"['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention']\", 'cleaned_methods': 'Transformer model', 'start_index': 0}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# 6. Reranking \n\nA practical strategy for RAG involves fetching a larger number of documents initially than the final count you aim for, followed by employing a stronger retrieval model to rerank these results. This process narrows down the selection to only the best top_k documents.\n\nTo implement this, we will use [Colbertv2](https://arxiv.org/abs/2112.01488), which is conveniently accessible through the [RAGatouille library](https://github.com/bclavie/RAGatouille).\n","metadata":{}},{"cell_type":"code","source":"reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-25T06:57:47.965137Z","iopub.execute_input":"2024-12-25T06:57:47.965467Z","iopub.status.idle":"2024-12-25T06:57:51.306042Z","shell.execute_reply.started":"2024-12-25T06:57:47.965436Z","shell.execute_reply":"2024-12-25T06:57:51.305371Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07fc19c0815a4f568f0da6aa80243bf9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0843faee93734f6383c728946658073a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3152c05c6ea64aa9a16487e36f9f36d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc22cca829074e2a9d7d6f0d49322f45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"016a5aa61daf4d1e824cfa45a2428562"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34c3c76c29ab43978b4fa7f0da53cfd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71f6e7a2404148fd9a01ecfa132aab36"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"page_contents = [doc.page_content for doc in retrieved_docs]  # keep only the text\nrelevant_docs = reranker.rerank(user_query, page_contents, k=5)\nrelevant_docs = [doc[\"content\"] for doc in relevant_docs]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-25T06:57:51.306978Z","iopub.execute_input":"2024-12-25T06:57:51.307231Z","iopub.status.idle":"2024-12-25T06:57:52.281391Z","shell.execute_reply.started":"2024-12-25T06:57:51.307211Z","shell.execute_reply":"2024-12-25T06:57:52.280396Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  6.34it/s]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(relevant_docs[0])","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:57:52.282595Z","iopub.execute_input":"2024-12-25T06:57:52.282861Z","iopub.status.idle":"2024-12-25T06:57:52.287428Z","shell.execute_reply.started":"2024-12-25T06:57:52.282839Z","shell.execute_reply":"2024-12-25T06:57:52.286565Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 5,\nMethods Used: ['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention'],\nSolution: Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# 7. Model building","metadata":{}},{"cell_type":"code","source":"%%time\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-25T06:57:52.288442Z","iopub.execute_input":"2024-12-25T06:57:52.288685Z","iopub.status.idle":"2024-12-25T06:58:49.903111Z","shell.execute_reply.started":"2024-12-25T06:57:52.288665Z","shell.execute_reply":"2024-12-25T06:58:49.902271Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Attaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 10.6 s, sys: 15.3 s, total: 25.9 s\nWall time: 57.6 s\n","output_type":"stream"},{"name":"stderr","text":"normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## 7.1 Testing Gemma model directly","metadata":{}},{"cell_type":"code","source":"%%time\ndisplay(Markdown(gemma_lm.generate(\"Hi, what can you tell me about Kaggle competitions?\", max_length=256)))","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:58:49.904171Z","iopub.execute_input":"2024-12-25T06:58:49.904425Z","iopub.status.idle":"2024-12-25T06:59:23.540438Z","shell.execute_reply.started":"2024-12-25T06:58:49.904403Z","shell.execute_reply":"2024-12-25T06:59:23.539595Z"},"trusted":true},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1735109953.312099      34 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1735109953.370958      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1735109953.676943      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Hi, what can you tell me about Kaggle competitions?\n\n**What are Kaggle competitions?**\n\nKaggle competitions are a platform where data scientists and machine learning engineers can participate in a wide range of data science and machine learning challenges. These competitions offer a unique opportunity to learn from experts, solve real-world problems, and potentially win prizes.\n\n**Key features of Kaggle competitions:**\n\n* **Real-world datasets:** Competitions typically use real-world datasets that are relevant to various industries and domains.\n* **Multiple data modalities:** Competitions allow participants to submit solutions for various data modalities, including images, text, and time series.\n* **Various challenge levels:** Competitions offer different challenge levels to cater to different skill sets and experience levels.\n* **Community engagement:** Kaggle provides a vibrant community where participants can interact, share knowledge, and collaborate on solutions.\n* **Prizes and recognition:** Winners of Kaggle competitions receive significant prizes and recognition, including cash, prizes, and public acclaim.\n\n**Benefits of participating in Kaggle competitions:**\n\n* **Learn from industry experts:** Solve real-world problems and gain insights from data science and machine learning experts.\n* **Boost your resume:** Winning a Kaggle competition can significantly enhance your"},"metadata":{}},{"name":"stdout","text":"CPU times: user 35.2 s, sys: 400 ms, total: 35.6 s\nWall time: 33.6 s\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## 7.2 Prompt\n\nThe template for the RAG prompt we will use involves inputting it in the format preferred by the LLM's chat interface. This format includes providing our context along with the user's question.","metadata":{}},{"cell_type":"code","source":"prompt_template = \"\"\"\nBased on your extensive knowledge and the following detailed context, \nplease provide a comprehensive answer to explain concepts from Kaggle competition solution write-ups:\n\nCONTEXT:\n{context}\n\nQUESTION:\n{question}\n\nANSWER:\n\"\"\"\n\nRAG_PROMPT_TEMPLATE = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:59:23.541740Z","iopub.execute_input":"2024-12-25T06:59:23.542524Z","iopub.status.idle":"2024-12-25T06:59:23.547231Z","shell.execute_reply.started":"2024-12-25T06:59:23.542490Z","shell.execute_reply":"2024-12-25T06:59:23.546241Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# 8. Creating the RAG pipeline","metadata":{}},{"cell_type":"code","source":"def answer_with_rag(\n    question: str,\n    llm,\n    knowledge_index: FAISS,\n    reranker: Optional[RAGPretrainedModel] = None,\n    num_retrieved_docs: int = 10,\n    num_docs_final: int = 5,\n) -> Tuple[str, List[Document]]:\n    # Gather documents with retriever\n    print(\"=> Retrieving documents...\")\n    config = {\"configurable\": {\"search_kwargs_faiss\": {\"k\": num_retrieved_docs}, \"search_kwargs_bm25\": num_retrieved_docs}}\n    relevant_docs = knowledge_index.invoke(question, config=config)\n    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n    \n    # Optionally rerank results\n    if reranker:\n        print(\"=> Reranking documents...\")\n        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n        \n    relevant_docs = relevant_docs[:num_docs_final] # Keeping only num_docs_final documents\n\n    # Build the final prompt\n    context = relevant_docs[0] # We select only the top relevant document\n    \n    final_prompt = RAG_PROMPT_TEMPLATE.format(\n        context = context,  \n        question=question\n    )\n\n    # Redact an answer\n    print(\"=> Generating answer...\")\n    answer = llm.generate(final_prompt, max_length=1024)\n\n    return answer, relevant_docs","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:59:23.548321Z","iopub.execute_input":"2024-12-25T06:59:23.548594Z","iopub.status.idle":"2024-12-25T06:59:23.559348Z","shell.execute_reply.started":"2024-12-25T06:59:23.548558Z","shell.execute_reply":"2024-12-25T06:59:23.558631Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"%%time\nquestion = \"\"\"I want to understand the 5th-place solution in the 'Google - Isolated Sign Language Recognition' competition. \nWhat overfitting prevention techniques were used, and how did they ensure model robustness?\n\"\"\"\nanswer, relevant_docs = answer_with_rag(question, gemma_lm, vector_database, reranker)","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:59:23.560459Z","iopub.execute_input":"2024-12-25T06:59:23.561025Z","iopub.status.idle":"2024-12-25T07:00:12.376022Z","shell.execute_reply.started":"2024-12-25T06:59:23.560996Z","shell.execute_reply":"2024-12-25T07:00:12.375162Z"},"trusted":true},"outputs":[{"name":"stdout","text":"=> Retrieving documents...\n=> Reranking documents...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Generating answer...\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1735109997.093674      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1735109998.085068      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 45.2 s, sys: 1.3 s, total: 46.5 s\nWall time: 48.8 s\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"def get_gemma_answer(generated_answer: str) -> str:\n    \"\"\"Function to get Gemma answer\"\"\"\n    split = generated_answer.split(\"ANSWER:\")\n    return split[1] if len(split) > 1 else \"No answer has been generatedCliquez pour utiliser cette solution\"\n\ndisplay(Markdown(\"### Gemma Answer\"))\ndisplay(Markdown(get_gemma_answer(answer)))\ndisplay(Markdown(\"### Source docs\"))\nfor i, doc in enumerate(relevant_docs):\n    display(Markdown(f\"**Document {i}------------------------------------------------------------**\"))\n    display(Markdown(doc))","metadata":{"execution":{"iopub.status.busy":"2024-12-25T07:00:12.377188Z","iopub.execute_input":"2024-12-25T07:00:12.377519Z","iopub.status.idle":"2024-12-25T07:00:12.401105Z","shell.execute_reply.started":"2024-12-25T07:00:12.377485Z","shell.execute_reply":"2024-12-25T07:00:12.400339Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Gemma Answer"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n**Overfitting prevention techniques used in the 5th-place solution:**\n\n* **Random masking of frames:** This technique randomly selects a subset of frames from the training data and trains the model on this subset. This helps to prevent the model from overfitting to the specific training data and improves itsgeneralizability.\n* **Early stopping:** This technique stops training the model when it reaches a certain number of epochs or when the validation loss starts to increase. This helps to prevent the model from overfitting to the training data and improves itsgeneralizability.\n* **Data augmentation:** This technique is used to increase the size of the training dataset and to introduce diversity into the training data. This helps to prevent the model from overfitting to the training data and improves itsgeneralizability.\n* **Mean and standard deviation of the single sign sequence:** This technique is used to pre-process the training data and to improve the performance of the model.\n\n**How these techniques ensured model robustness:**\n\n* **Random masking of frames:** This technique helped to prevent the model from overfitting to the specific training data by exposing it to a wide range of images.\n* **Early stopping:** This technique helped to prevent the model from overfitting to the training data by stopping training when it reached a certain number of epochs.\n* **Data augmentation:** This technique helped to increase the size of the training dataset and to introduce diversity into the training data. This helped to prevent the model from overfitting to the training data and improved itsgeneralizability.\n* **Mean and standard deviation of the single sign sequence:** This technique helped to improve the performance of the model by reducing overfitting and by introducing diversity into the training data."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Source docs"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 0------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 5,\nMethods Used: ['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention'],\nSolution: Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 1------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 8,\nMethods Used: ['Transformer models', 'FFN encoder', 'Cosine schedule', 'Dropout', 'Label smoothing', 'Sequence cutout augmentation', 'Mirror left augmentation', 'Random rotate augmentation', 'Linear interpolation', 'Min-max normalization', 'Mean/std normalization', 'Time shift delta features', 'Angle features', 'Point to point distances', 'Tflite conversion', 'Speed up with model.half().float()', 'Normalizing points across the whole sequence', 'Mixup (tried but did not work)', 'CNNs with mixup (tried but did not work)'],\nSolution: Here is a quick overview of the 8th place solution.\n3 transformers models, 2 layers each (384 hidden, 512 hidden ffn), with an ffn encoder (512->384), trained from scratch. LR 8e-4 with cosine schedule trained for ~300 epochs, dropout 0.1, batch size 1024, label smoothing 0.1. Using hands, lips and pose (above waist only). On one transformer all pose and a subset of lips were used for diversity.\nAugmentations\nmost important was sequence cutout. On each sample, and each body part (left hand, right hand, lips, pose) with a 0.4 proba convert to nan 5 random slices of 0.15 x SequenceLength. It was hard to overfit with this in.\nmirror left\nrandom rotate.\nPreprocessing\nLinear interpolation of longer sequences to max length of 96.\nNormalise each body part, using min max - I found this better than mean/std. In one model I used mean/std for diversity.\nCreate time shift delta features on a subset of points, using time shifts of\n[1, 2, 3, 4, 6, 8, 12, 16]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 2------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 6,\nMethods Used: ['MLP', 'Encoder', 'Transformer', 'Convolutional Neural Network (CNN)', 'Data Augmentation', 'Cross Entropy Loss', 'Weight Decay', 'Mean Teacher', 'Knowledge Distillation', 'Ensemble Learning', 'Stratified K-fold', 'Baseline Model', 'Deberta', 'Max Pooling', 'Normalization', 'Interpolation', 'Manifold Mixup', 'Face CutMix', 'Outlier Sample Mining (OUSM)', 'Model Soup', 'Data Relabeling', 'Data Truncation', 'Mish Activation Function'],\nSolution: Thanks to both, the organizers of this competition who offered a fun yet challenging problem as well as all of the other competitors - well done to everyone who worked hard for small incremental increases.\nAlthough I am the one posting the topic, this is the result of a great team effort, so big shoutout to\n@christofhenkel\n.\nBrief Summary\nOur solution is a 2 model ensemble of a MLP-encoder-frame-transformer model. We pushed our transformer models close to the limit and implemented a lot of tricks to climb up to 6th place.\nI have 1403 hours of experiment monitoring time in April (that’s 48h per day :)).\nUpdate :\nCode is available here :\nhttps://github.com/TheoViel/kaggle_islr\nDetailed Summary\nPreprocessing & Model\nPreprocessing\nRemove frames without fingers\nStride the sequence (use 1 every n frames) such that the sequence size is\n<= max_len\n. We used\nmax_len=25\nand\n80\nin the final ensemble"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 3------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 26,\nMethods Used: ['Mixup', 'Mirroring', 'LLaMa-inspired architecture', 'RMSNorm normalization', 'Lion optimizer', 'Cosine decay learning rate', 'Batch size 128', 'Dropout 0.1', 'Exponential moving average of weights'],\nSolution: Github with all the code used\nSummary\nThe most important part of the solution is the data utilization. Major improvements were from keypoints choice and mixup. External data does not help because it is from a very different distribution. Given data amount does not benefit larger models so ensembles of small models is the way to utilize given constraints to the fullest.\nMost augmentations are not helpful, because they prevent model from learning the true data distribution. So only used mirroring and mixup (0.5).\nInputs to the model\nAll models are trained to support sequences of up to 512 frames.\nPreprocessing\nOnly 2d coordinates are used as 3rd dimension leads to unstable training.\nTo normalize inputs all keypoints are shifted so that head is located at the origin.\nScaling did not provide any benefit so not used.\nAll nans are replaced with 0 after normalization.\nChosen keypoints\nAll (21) hand keypoints\n26 face keypoints\n17 pose keypoints\nArchitecture\nLLaMa-inspired architecture. Most notable improvement comes from much better normalization RMSNorm.\nFor all models head dimensions are set to 64\nSingle model (Private/Public LB: 0.8543689/0.7702471)\n6 heads 5 layers 9.2M parameters\nEnsemble of 3 models (Private/Public LB: 0.8584568/0.7725324)\n2 heads 6 layers 1.7M parameters per model\nLarger models could be fit into file size limit, but it would time out during submission.\nAugmentations"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 4------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 11,\nMethods Used: ['Ensemble', 'Strong augmentation', 'Manual model conversion from pytorch to tensorflow', 'CLIP transformer architecture', 'Decrease parameter size', 'Motion features', 'Longer epoch'],\nSolution: Thank you to the organizer and Kaggle for hosting this interesting challenge.\nEspecially I enjoyed this strict inference time restriction. It keeps model size reasonable and requires us for some practical technique.\nTL;DR\nEnsemble 5 transformer models\nStrong augmentation\nManual model conversion from pytroch to tensorflow\nCode is available here ->\nhttps://github.com/bamps53/kaggle-asl-11th-place-solution\nOverview\nI started from\n@hengck23\n‘s\ngreat discussion\nand\nnotebook\n. Thanks for sharing a lot of useful tricks as always!\nThe changes I made are following;\nChange model architecture to CLIP transformer in HuggingFace\nDecrease parameter size to maximize latency within the range of same accuracy\nSome strong augmentations\nHorizontal flip(p=0.5)\nRandom 3d rotation(p=1, -45~45)\nRandom scale(p=1, 0.5~1.5)\nRandom shift(p=1, 0.7~1.3)\nRandom mask frames(p=1, mask_ratio=0.5)\nRandom resize (p=1, 0.5~1.5)\nAdd motion features\ncurrent - prev\nnext - current\nVelocity\nLonger epoch, 250 for 5 fold and 300 for all data\nFor the details, please refer to the code.(planning to upload)\nModel conversion"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"**Let's ask a another question**","metadata":{}},{"cell_type":"code","source":"%%time\nquestion = \"\"\"What can you tell me about the 'RSNA Screening Mammography Breast Cancer Detection' competition ?\n\"\"\"\nanswer, relevant_docs = answer_with_rag(question, gemma_lm, vector_database, reranker)\n\ndisplay(Markdown(\"### Gemma Answer\"))\ndisplay(Markdown(get_gemma_answer(answer)))\ndisplay(Markdown(\"### Source docs\"))\nfor i, doc in enumerate(relevant_docs):\n    display(Markdown(f\"**Document {i}------------------------------------------------------------**\"))\n    display(Markdown(doc))","metadata":{"execution":{"iopub.status.busy":"2024-12-25T07:00:12.402169Z","iopub.execute_input":"2024-12-25T07:00:12.402486Z","iopub.status.idle":"2024-12-25T07:00:26.717015Z","shell.execute_reply.started":"2024-12-25T07:00:12.402458Z","shell.execute_reply":"2024-12-25T07:00:26.716137Z"},"trusted":true},"outputs":[{"name":"stdout","text":"=> Retrieving documents...\n=> Reranking documents...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  5.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Generating answer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Gemma Answer"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\nSure, here's a summary of the RSNA Screening Mammography Breast Cancer Detection competition:\n\n- The competition is a Kaggle competition focused on breast cancer detection using medical images.\n- The competition received 10,000 chest X-ray images of patients with breast cancer.\n- The goal of the competition is to develop a machine learning model that can accurately detect breast cancer in the X-ray images.\n- The competition used a variety of machine learning methods, including YOLOX, classification models, and ensemble methods.\n- The winning model achieved a validation accuracy of 92.2%, which placed the team in third place in the competition."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Source docs"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 0------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: RSNA Screening Mammography Breast Cancer Detection,\nPlace: 3,\nMethods Used: ['YOLOX', 'Classification models', 'Average weighting fusion', 'External data', 'Data augmentation', 'CNN', 'EfficientNet', 'Convnext', 'Multi-view model', 'LSTM', 'Ensemble'],\nSolution: Introduction\nThank you to all the participants for your hard work in the competition.We are honored to have achieved a good result, coming in third place in this competition. We also want to express our deepest gratitude to the organizers for putting together such a fantastic event.Thank you very much.\nFinally, I want to thank my excellent teammates\n@haqishen\n,\n@boliu0\n,\n@kevin1742064161\n. On behalf of my teammates, I would like to introduce part of our solution, and another part is presented by\n@boliu0\nin another thread\n.\n1. Overview of the pipeline\nExtract ROI with a fixed aspect ratio(1.6:1) using YOLOX\nFeed the ROI into different classification models\nAverage weighting fusion of the results from the classification models\n2. External Data\nWe use 4 external data in total. Not all models used all the external data. Some models only used CBIS-DDSM + CMMD, while the remaining models used all four external data. Although these external data appear to be different from the competition data, they can improve the CV and significantly enhance the stability of the training.\n1)\nCBIS-DDSM\nThe classification labels of CBIS-DDSM are: MALIGNANT, BENIGN WITHOUT CALLBACK, BENIGN. We consider MALIGNANT as positive and the others as negative, resulting in 1,350 positive and 1,753 negative samples.\n2)\nCMMD"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 1------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: RSNA Screening Mammography Breast Cancer Detection,\nPlace: 19,\nMethods Used: ['Label Smoothing', 'Auxiliary Classes', 'Weighted BCELoss', 'Mosaic', 'Mixup', 'ROI Cropping'],\nSolution: 19th Place Solution\nThanks to the team at RSNA and Kaggle for putting this competition together, and thanks to my teammates\n@ragnar123\nand\n@harshitsheoran\n.\nThe final submission consisted of 2 CNN models : eca_nfnet_l0 and tf_efficientnet_b3_ns.\nModel\nCV\nBest Public\nBest Private\nUsed in Ensemble\neca_nfnet_l0 @ 1536 (Harshit)\n.488\n.6\n.47\n0\neca_nfnet_l0 @ 1536 (Ivan)\n.4688\n.61\n.46\n1\ntf_efficientnet_b3_ns @ 1536 (Ivan)\n.491\n.63\n.48\n1\ntf_efficientnet_b3_ns @ 1920 by 1536 (Martin)\n.464\n.6\n.51\n0\nWhen combined, these two models had a best Public LB of .65 and a best Private LB of .5. Unfortunately there was no correlation between public and private or CV and private. This led to us picking the wrong submission. In the end we had 12 submissions that would have gotten us into gold but had no way of telling if they were the correct ones.\nWhat Worked?\nLabel Smoothing\nAuxiliary Classes (Bi-raids 2, Benign, Invasive, Biopsy)\nWeighted BCELoss\nMosaic (with class max as target)\nMixup (with class max as target)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 2------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: RSNA Screening Mammography Breast Cancer Detection,\nPlace: 18,\nMethods Used: ['Windowing', 'Min-max scaling', 'YOLOv5', 'Affine transformation', 'GeM pooling', 'imgaug augmentations', 'BCE Loss', 'Focal Loss', 'Exponential moving average', 'AdamW optimizer', 'OneCycleLR scheduler', 'Flip test TTA', 'LP pooling', 'Ensemble averaging', 'Percentile based thresholding'],\nSolution: Thanks to Kaggle, the hosts and competitors for this meaningful competition.\nIn the following, I want to provide a brief summary of my solution.\nOverview\nSimilar to many public codes, my pipeline is as follows.\nDetect a breast area for each image and crop that area\nPredict cancer image-wise using various backbones\nAggregate image-wise predictions and apply thresholding to get final prediction for each target\nPreprocessing\nMy preprocessing depends on many public codes. I am grateful to the authors of those codes.\nSigmoid/linear windowing is applied based on\nVOILUTFunction\n,\nWindowCenter\nand\nWindowWidth\nin dicom data. After windowing, images are processed with min-max scaling and treated as 8-bit images.\nBreast detector\nI annotated breast bounding boxes for about 1000 images. In addition to those labels, I also used labels provided by\n@remekkinas\n(about 500 images) in\nthis code\nto train a single YOLOv5n6 with the input size of 1024. mAP_0.5:0.95 of a validation split is 0.952.\nGiven the detections, affine transformation is applied to obtain fixed size cropped images.\nAt that time, expanding the bboxes so that the aspect ratio and the size of the bbox relative to the original images did not change too much improved somewhat of local cv."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 3------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: RSNA Screening Mammography Breast Cancer Detection,\nPlace: 9,\nMethods Used: ['Sampling strategy', 'Positive class balancing', 'Augmentation', 'Model selection', 'Postprocessing', 'DICOM to PNG conversion', 'Inference with ConvNext models', 'Ensemble averaging', 'Voting strategy', 'Thresholding', 'Probf1 metric', 'ROC_AUC metric', 'Precision recall metric', 'MCC metric', 'Local validation', 'TTA (Horizontal Flip)', 'Model probabilities ensembling', 'Better than median function', 'Voting and averaging', 'RAdam optimizer', 'Lookahead optimizer', 'OneCycle scheduler', 'Weight decay', 'Dataset seed', 'Batch size', 'Mixed precision training', 'Gradient clipping', 'BCEWithLogitsLoss', 'Sequential sampler', 'ConvNext_v1 small model', 'Average pooling', 'GeM pooling', 'Training on 1536x768 images', 'Balancer class', 'Weights & Biases experiment tracking'],\nSolution: First of all congratulations to all participants. Congratulations to dream teams from gold zone. I’m impressed by your consistency in winning Kaggle competition. Waiting to learn from your solution.\nThank you my team mate Andrij\n@aikhmelnytskyy\nWe had great collaboration 👍👍👍 - I feel that from first minute we played in one team having one goal - find better solution.\nGold in competition was dream for me. Last year we (with\n@christofhenkel\n) were #1 in sliver (#12 solution in Image Matching Challange 2021). This year I decided to work hard to experience gold zone and finally become competition master. Even there is no official LB finalized …. we are #9 and in gold! :) and I am …… extremely happy! 😁😁😍😜"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 4------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: RSNA Screening Mammography Breast Cancer Detection,\nPlace: 2,\nMethods Used: ['Pretraining with external dataset', 'Fine-tuning', 'Data augmentation', 'ConvnextV1 small model', 'Manual annotation of bounding box', 'Faster R-CNN for cropping', 'ShiftScaleRotate augmentation', 'RandomFlip augmentation', 'RandAugment augmentation', 'RandomErasing augmentation', 'EQL loss', 'High resolution', 'Large batch size', 'Auxiliary loss', 'More training epochs', 'Dual view model', 'Multi laterality dual view model', 'Optimizer: AdamW', 'lr: 0.00015', 'Scheduler: CosineAnnealingLR', 'Epochs: 24', 'Batch size: 192', 'EMA', 'Diagonal flip TTA'],\nSolution: 2nd place solution\nI would like to express my gratitude to Kaggle for hosting this meaningful competition, and to my teammates, particularly\n@kapenon\n, who persevered alongside me throughout the entire competition.\nI would like to extend my gratitude to\n@theoviel\nfor providing the fast DALI inference notebook, which greatly aided in the completion of this competition. Additionally, I would like to thank\n@pourchot\nfor generously sharing the external data, which contained valuable positive case data that contributed to the success of our final solution.\nFortunately, our team was able to get 2nd place, and I am excited to share our approach.\nSummary of our approach\nStages\nPretrain a single view model in 1280x1280 resolution with external dataset (Thanks to\n@pourchot\n,\nDataset\n)\nFine-tune the single view model in 1536x1536 resolution without external dataset\nUse the fine-tuned single view model to further fine-tune a dual view model and a four view model\nModel"},"metadata":{}},{"name":"stdout","text":"CPU times: user 7 s, sys: 15 ms, total: 7.02 s\nWall time: 14.3 s\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# 9. To go further\n\nHere are a few ideas to improve this Notebook:\n\n- Adjust chunking: change chunk sizes, split on different separators…\n- Switch embedding models\n- Try semantic chunking for different insights.\n- Adjust the EnsembleRetriever (e.g. use a different index than FAISS) or just use one retriever. You can see the list of indexes supported by LangChain [here](https://python.langchain.com/docs/integrations/vectorstores)\n- Evaluate the RAG pipeline with [Ragas](https://github.com/explodinggradients/ragas) or [TruLens](https://github.com/truera/trulens) tools.\n- Fine-tune the Gemma model on your dataset for better performance.","metadata":{}},{"cell_type":"markdown","source":"# 10. Resources\n\n- [Advanced RAG Techniques: an Illustrated Overview](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6) by [IVAN ILIN](https://medium.com/@ivanilin_iki)\n- [Advanced RAG on HuggingFace documentation using langchain](https://huggingface.co/learn/cookbook/advanced_rag) by [Aymeric Roucher](https://huggingface.co/m-ric)\n- [Simple RAG for GitHub issues using Hugging Face Zephyr and LangChain](https://huggingface.co/learn/cookbook/rag_zephyr_langchain) by [Maria Khalusova](https://github.com/MKhalusova)","metadata":{}}]}