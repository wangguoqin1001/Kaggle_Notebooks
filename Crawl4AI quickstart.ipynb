{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0cba38e5","cell_type":"markdown","source":"# Crawl4AI üï∑Ô∏èü§ñ\n<a href=\"https://trendshift.io/repositories/11716\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/11716\" alt=\"unclecode%2Fcrawl4ai | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n[![GitHub Stars](https://img.shields.io/github/stars/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/stargazers)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/Crawl4AI)\n[![GitHub Forks](https://img.shields.io/github/forks/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/network/members)\n[![GitHub Issues](https://img.shields.io/github/issues/unclecode/crawl4ai)](https://github.com/unclecode/crawl4ai/issues)\n[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/unclecode/crawl4ai)](https://github.com/unclecode/crawl4ai/pulls)\n[![License](https://img.shields.io/github/license/unclecode/crawl4ai)](https://github.com/unclecode/crawl4ai/blob/main/LICENSE)\n\nCrawl4AI simplifies asynchronous web crawling and data extraction, making it accessible for large language models (LLMs) and AI applications. üÜìüåê\n\n- GitHub Repository: [https://github.com/unclecode/crawl4ai](https://github.com/unclecode/crawl4ai)\n- Twitter: [@unclecode](https://twitter.com/unclecode)\n- Website: [https://crawl4ai.com](https://crawl4ai.com)\n\n## üåü Meet the Crawl4AI Assistant: Your Copilot for Crawling\nUse the [Crawl4AI GPT Assistant](https://tinyurl.com/crawl4ai-gpt) as your AI-powered copilot! With this assistant, you can:\n- üßë‚Äçüíª Generate code for complex crawling and extraction tasks\n- üí° Get tailored support and examples\n- üìò Learn Crawl4AI faster with step-by-step guidance","metadata":{"id":"0cba38e5"}},{"id":"41de6458","cell_type":"markdown","source":"### **Quickstart with Crawl4AI**","metadata":{"id":"41de6458"}},{"id":"1380e951","cell_type":"markdown","source":"#### 1. **Installation**\nInstall Crawl4AI and necessary dependencies:","metadata":{"id":"1380e951"}},{"id":"05fecfad","cell_type":"code","source":"# %%capture\n!pip install crawl4ai\n!pip install nest_asyncio\n!playwright install-deps\n!playwright install","metadata":{"id":"05fecfad","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:25:00.314989Z","iopub.execute_input":"2024-12-18T11:25:00.315537Z","iopub.status.idle":"2024-12-18T11:26:35.568426Z","shell.execute_reply.started":"2024-12-18T11:25:00.315485Z","shell.execute_reply":"2024-12-18T11:26:35.567183Z"}},"outputs":[{"name":"stdout","text":"Collecting crawl4ai\n  Downloading Crawl4AI-0.4.23-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: aiosqlite~=0.20 in /opt/conda/lib/python3.10/site-packages (from crawl4ai) (0.20.0)\nRequirement already satisfied: lxml~=5.3 in /opt/conda/lib/python3.10/site-packages (from crawl4ai) (5.3.0)\nCollecting litellm>=1.53.1 (from crawl4ai)\n  Downloading litellm-1.55.3-py3-none-any.whl.metadata (34 kB)\nRequirement already satisfied: numpy<3,>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from crawl4ai) (1.26.4)\nCollecting pillow~=10.4 (from crawl4ai)\n  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\nCollecting playwright>=1.49.0 (from crawl4ai)\n  Downloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: python-dotenv~=1.0 in /opt/conda/lib/python3.10/site-packages (from crawl4ai) (1.0.1)\nRequirement already satisfied: requests~=2.26 in /opt/conda/lib/python3.10/site-packages (from crawl4ai) (2.32.3)\nRequirement already satisfied: beautifulsoup4~=4.12 in /opt/conda/lib/python3.10/site-packages (from crawl4ai) (4.12.3)\nCollecting tf-playwright-stealth>=1.1.0 (from crawl4ai)\n  Downloading tf_playwright_stealth-1.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: xxhash~=3.4 in /opt/conda/lib/python3.10/site-packages (from crawl4ai) (3.4.1)\nCollecting rank-bm25~=0.2 (from crawl4ai)\n  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting aiofiles>=24.1.0 (from crawl4ai)\n  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: colorama~=0.4 in /opt/conda/lib/python3.10/site-packages (from crawl4ai) (0.4.6)\nRequirement already satisfied: snowballstemmer~=2.2 in /opt/conda/lib/python3.10/site-packages (from crawl4ai) (2.2.0)\nRequirement already satisfied: pydantic>=2.10 in /opt/conda/lib/python3.10/site-packages (from crawl4ai) (2.10.2)\nRequirement already satisfied: typing_extensions>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiosqlite~=0.20->crawl4ai) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4~=4.12->crawl4ai) (2.5)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from litellm>=1.53.1->crawl4ai) (3.9.5)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from litellm>=1.53.1->crawl4ai) (8.1.7)\nRequirement already satisfied: httpx<0.28.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from litellm>=1.53.1->crawl4ai) (0.27.0)\nRequirement already satisfied: importlib-metadata>=6.8.0 in /opt/conda/lib/python3.10/site-packages (from litellm>=1.53.1->crawl4ai) (7.0.0)\nRequirement already satisfied: jinja2<4.0.0,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from litellm>=1.53.1->crawl4ai) (3.1.4)\nRequirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from litellm>=1.53.1->crawl4ai) (4.22.0)\nCollecting openai>=1.55.3 (from litellm>=1.53.1->crawl4ai)\n  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\nCollecting tiktoken>=0.7.0 (from litellm>=1.53.1->crawl4ai)\n  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (from litellm>=1.53.1->crawl4ai) (0.20.3)\nCollecting greenlet==3.1.1 (from playwright>=1.49.0->crawl4ai)\n  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\nCollecting pyee==12.0.0 (from playwright>=1.49.0->crawl4ai)\n  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.10->crawl4ai) (2.27.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests~=2.26->crawl4ai) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests~=2.26->crawl4ai) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.26->crawl4ai) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.26->crawl4ai) (2024.6.2)\nCollecting fake-http-header<0.4.0,>=0.3.5 (from tf-playwright-stealth>=1.1.0->crawl4ai)\n  Downloading fake_http_header-0.3.5-py3-none-any.whl.metadata (3.5 kB)\nCollecting pytest-mockito<0.0.5,>=0.0.4 (from tf-playwright-stealth>=1.1.0->crawl4ai)\n  Downloading pytest-mockito-0.0.4.tar.gz (3.0 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (0.14.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.19.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (2.1.5)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.18.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (1.9.0)\nCollecting jiter<1,>=0.4.0 (from openai>=1.55.3->litellm>=1.53.1->crawl4ai)\n  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (4.66.4)\nRequirement already satisfied: pytest>=3 in /opt/conda/lib/python3.10/site-packages (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (8.3.3)\nCollecting mockito>=1.0.6 (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai)\n  Downloading mockito-1.5.3-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken>=0.7.0->litellm>=1.53.1->crawl4ai) (2024.5.15)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.3.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (4.0.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.26.2)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2024.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (6.0.2)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (2.0.0)\nRequirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.10/site-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (1.5.0)\nRequirement already satisfied: tomli>=1 in /opt/conda/lib/python3.10/site-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (2.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.1.2)\nDownloading Crawl4AI-0.4.23-py3-none-any.whl (140 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m140.3/140.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\nDownloading litellm-1.55.3-py3-none-any.whl (6.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl (44.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyee-12.0.0-py3-none-any.whl (14 kB)\nDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\nDownloading tf_playwright_stealth-1.1.0-py3-none-any.whl (33 kB)\nDownloading fake_http_header-0.3.5-py3-none-any.whl (14 kB)\nDownloading openai-1.58.1-py3-none-any.whl (454 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mockito-1.5.3-py3-none-any.whl (30 kB)\nBuilding wheels for collected packages: pytest-mockito\n  Building wheel for pytest-mockito (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytest-mockito: filename=pytest_mockito-0.0.4-py3-none-any.whl size=3697 sha256=f8d91fb434d64f0faea3d147e89f8cf01609ef0eedaea541db7f30298e6c8ac2\n  Stored in directory: /root/.cache/pip/wheels/df/72/3d/ad383ec25e3ebb14d12326426166bee101969601f5d35d5462\nSuccessfully built pytest-mockito\nInstalling collected packages: rank-bm25, pyee, pillow, mockito, jiter, greenlet, fake-http-header, aiofiles, tiktoken, playwright, pytest-mockito, openai, tf-playwright-stealth, litellm, crawl4ai\n  Attempting uninstall: pillow\n    Found existing installation: pillow 10.3.0\n    Uninstalling pillow-10.3.0:\n      Successfully uninstalled pillow-10.3.0\n  Attempting uninstall: greenlet\n    Found existing installation: greenlet 3.0.3\n    Uninstalling greenlet-3.0.3:\n      Successfully uninstalled greenlet-3.0.3\n  Attempting uninstall: aiofiles\n    Found existing installation: aiofiles 22.1.0\n    Uninstalling aiofiles-22.1.0:\n      Successfully uninstalled aiofiles-22.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.3 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.10.2 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.3 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nydata-profiling 4.12.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 24.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiofiles-24.1.0 crawl4ai-0.4.23 fake-http-header-0.3.5 greenlet-3.1.1 jiter-0.8.2 litellm-1.55.3 mockito-1.5.3 openai-1.58.1 pillow-10.4.0 playwright-1.49.1 pyee-12.0.0 pytest-mockito-0.0.4 rank-bm25-0.2.2 tf-playwright-stealth-1.1.0 tiktoken-0.8.0\nRequirement already satisfied: nest_asyncio in /opt/conda/lib/python3.10/site-packages (1.6.0)\nInstalling dependencies...\nGet:1 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1227 B]\nGet:2 https://packages.cloud.google.com/apt cloud-sdk InRelease [1618 B]       \nGet:3 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]      \nHit:4 http://archive.ubuntu.com/ubuntu focal InRelease                         \nGet:5 https://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [30.1 kB]\nGet:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]        \nGet:7 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3464 kB]\nGet:8 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1594 kB]\nGet:9 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [4206 kB]\nGet:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\nGet:11 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [4418 kB]\nGet:12 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1281 kB]\nGet:13 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [34.6 kB]\nGet:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4632 kB]\nGet:15 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [4146 kB]\nGet:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1573 kB]\nFetched 25.8 MB in 4s (6016 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nNote, selecting 'libfontconfig1' instead of 'libfontconfig'\nfonts-liberation is already the newest version (1:1.07.4-11).\nfonts-liberation set to manually installed.\nlibatk1.0-0 is already the newest version (2.35.1-1ubuntu2).\nlibatk1.0-0 set to manually installed.\nlibatspi2.0-0 is already the newest version (2.36.0-2).\nlibatspi2.0-0 set to manually installed.\nlibcairo-gobject2 is already the newest version (1.16.0-4ubuntu1).\nlibcairo-gobject2 set to manually installed.\nlibcairo2 is already the newest version (1.16.0-4ubuntu1).\nlibcairo2 set to manually installed.\nlibepoxy0 is already the newest version (1.5.4-1).\nlibepoxy0 set to manually installed.\nlibevent-2.1-7 is already the newest version (2.1.11-stable-1).\nlibevent-2.1-7 set to manually installed.\nlibfontconfig1 is already the newest version (2.13.1-2ubuntu3).\nlibopus0 is already the newest version (1.3.1-0ubuntu1).\nlibopus0 set to manually installed.\nlibpango-1.0-0 is already the newest version (1.44.7-2ubuntu4).\nlibpango-1.0-0 set to manually installed.\nlibpangocairo-1.0-0 is already the newest version (1.44.7-2ubuntu4).\nlibpangocairo-1.0-0 set to manually installed.\nlibpangoft2-1.0-0 is already the newest version (1.44.7-2ubuntu4).\nlibpangoft2-1.0-0 set to manually installed.\nlibpng16-16 is already the newest version (1.6.37-2).\nlibpng16-16 set to manually installed.\nlibxcb-shm0 is already the newest version (1.14-2).\nlibxcb-shm0 set to manually installed.\nlibxcb1 is already the newest version (1.14-2).\nlibxcb1 set to manually installed.\nlibxcomposite1 is already the newest version (1:0.4.5-1).\nlibxcomposite1 set to manually installed.\nlibxcursor1 is already the newest version (1:1.2.0-2).\nlibxcursor1 set to manually installed.\nlibxdamage1 is already the newest version (1:1.1.5-2).\nlibxdamage1 set to manually installed.\nlibxext6 is already the newest version (2:1.3.4-0ubuntu1).\nlibxfixes3 is already the newest version (1:5.0.3-2).\nlibxfixes3 set to manually installed.\nlibxi6 is already the newest version (2:1.7.10-0ubuntu1).\nlibxi6 set to manually installed.\nlibxkbcommon0 is already the newest version (0.10.0-1).\nlibxkbcommon0 set to manually installed.\nlibxrandr2 is already the newest version (2:1.5.2-0ubuntu1).\nlibxrandr2 set to manually installed.\nlibxrender1 is already the newest version (1:0.9.10-1).\nlibxshmfence1 is already the newest version (1.3-1).\nlibxshmfence1 set to manually installed.\nlibxt6 is already the newest version (1:1.1.5-1).\nlibxt6 set to manually installed.\nlibxtst6 is already the newest version (2:1.2.3-1).\nlibxtst6 set to manually installed.\nlibflite1 is already the newest version (2.1-release-3).\nlibflite1 set to manually installed.\nlibx264-155 is already the newest version (2:0.155.2917+git0a84d98-2).\nlibx264-155 set to manually installed.\nlibasound2 is already the newest version (1.2.2-2.1ubuntu2.5).\nlibasound2 set to manually installed.\nlibatk-bridge2.0-0 is already the newest version (2.34.2-0ubuntu2~20.04.1).\nlibatk-bridge2.0-0 set to manually installed.\nlibatomic1 is already the newest version (10.5.0-1ubuntu1~20.04).\nlibatomic1 set to manually installed.\nlibdbus-1-3 is already the newest version (1.12.16-2ubuntu2.3).\nlibdbus-1-3 set to manually installed.\nlibdrm2 is already the newest version (2.4.107-8ubuntu1~20.04.2).\nlibdrm2 set to manually installed.\nlibfreetype6 is already the newest version (2.10.1-2ubuntu0.3).\nlibfreetype6 set to manually installed.\nlibgdk-pixbuf2.0-0 is already the newest version (2.40.0+dfsg-3ubuntu0.5).\nlibgdk-pixbuf2.0-0 set to manually installed.\nlibgl1 is already the newest version (1.3.2-1~ubuntu0.20.04.2).\nlibgl1 set to manually installed.\nlibglib2.0-0 is already the newest version (2.64.6-1~ubuntu20.04.8).\nlibharfbuzz-icu0 is already the newest version (2.6.4-1ubuntu4.2).\nlibharfbuzz-icu0 set to manually installed.\nlibharfbuzz0b is already the newest version (2.6.4-1ubuntu4.2).\nlibharfbuzz0b set to manually installed.\nlibicu66 is already the newest version (66.1-2ubuntu2.1).\nlibicu66 set to manually installed.\nlibjpeg-turbo8 is already the newest version (2.0.3-0ubuntu1.20.04.3).\nlibjpeg-turbo8 set to manually installed.\nlibnghttp2-14 is already the newest version (1.40.0-1ubuntu0.3).\nlibnghttp2-14 set to manually installed.\nlibnspr4 is already the newest version (2:4.35-0ubuntu0.20.04.1).\nlibnspr4 set to manually installed.\nlibnss3 is already the newest version (2:3.98-0ubuntu0.20.04.2).\nlibnss3 set to manually installed.\nlibopenjp2-7 is already the newest version (2.3.1-1ubuntu4.20.04.3).\nlibopenjp2-7 set to manually installed.\nlibvpx6 is already the newest version (1.8.2-1ubuntu0.3).\nlibvpx6 set to manually installed.\nlibwayland-client0 is already the newest version (1.18.0-1ubuntu0.1).\nlibwayland-client0 set to manually installed.\nlibwayland-egl1 is already the newest version (1.18.0-1ubuntu0.1).\nlibwayland-egl1 set to manually installed.\nlibwebp6 is already the newest version (0.6.1-2ubuntu0.20.04.3).\nlibwebp6 set to manually installed.\nlibx11-6 is already the newest version (2:1.6.9-2ubuntu1.6).\nlibx11-6 set to manually installed.\nlibx11-xcb1 is already the newest version (2:1.6.9-2ubuntu1.6).\nlibx11-xcb1 set to manually installed.\nlibxml2 is already the newest version (2.9.10+dfsg-5ubuntu0.20.04.7).\nlibxml2 set to manually installed.\nffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\nxvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.18).\nThe following additional packages will be installed:\n  dictionaries-common fonts-ubuntu hunspell-en-us libaspell15 libegl-mesa0\n  libgtk-3-common libhunspell-1.7-0 libsecret-common libtext-iconv-perl\nSuggested packages:\n  ispell | aspell | hunspell wordlist hunspell openoffice.org-hunspell\n  | openoffice.org-core aspell cups-common libenchant-2-voikko\n  libenchant-voikko gvfs\nRecommended packages:\n  fonts-ipafont-mincho fonts-tlwg-loma aspell-en | aspell-dictionary\n  | aspell6a-dictionary enchant-2 enchant libgtk-3-bin gnome-shell\n  | notification-daemon\nThe following NEW packages will be installed:\n  dictionaries-common fonts-ipafont-gothic fonts-noto-color-emoji\n  fonts-tlwg-loma-otf fonts-ubuntu fonts-wqy-zenhei hunspell-en-us libaspell15\n  libdbus-glib-1-2 libegl-mesa0 libegl1 libenchant-2-2 libenchant1c2a\n  libevdev2 libgbm1 libgles2 libgudev-1.0-0 libhunspell-1.7-0 libhyphen0\n  libnotify4 libopengl0 libsecret-1-0 libsecret-common libtext-iconv-perl\n  libwayland-server0 libwebpdemux2 libwoff1 libxslt1.1 ttf-ubuntu-font-family\n  ttf-unifont xfonts-cyrillic xfonts-scalable\nThe following packages will be upgraded:\n  libcups2 libgtk-3-0 libgtk-3-common\n3 upgraded, 32 newly installed, 0 to remove and 57 not upgraded.\nNeed to get 31.1 MB of archives.\nAfter this operation, 71.4 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 fonts-ipafont-gothic all 00303-18ubuntu1 [3526 kB]\nGet:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 fonts-wqy-zenhei all 0.9.45-7ubuntu1 [7468 kB]\nGet:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libtext-iconv-perl amd64 1.7-7 [13.8 kB]\nGet:4 http://archive.ubuntu.com/ubuntu focal/main amd64 dictionaries-common all 1.28.1 [178 kB]\nGet:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 fonts-noto-color-emoji all 0~20200916-1~ubuntu20.04.1 [9940 kB]\nGet:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 fonts-tlwg-loma-otf all 1:0.7.1-3 [98.8 kB]\nGet:7 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-ubuntu all 0.83-4ubuntu1 [1458 kB]\nGet:8 http://archive.ubuntu.com/ubuntu focal/main amd64 hunspell-en-us all 1:2018.04.16-1 [170 kB]\nGet:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libaspell15 amd64 0.60.8-1ubuntu0.1 [328 kB]\nGet:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libcups2 amd64 2.3.1-9ubuntu1.9 [234 kB]\nGet:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libdbus-glib-1-2 amd64 0.110-5fakssync1 [59.1 kB]\nGet:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libwayland-server0 amd64 1.18.0-1ubuntu0.1 [31.3 kB]\nGet:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgbm1 amd64 21.2.6-0ubuntu0.1~20.04.2 [29.2 kB]\nGet:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libegl-mesa0 amd64 21.2.6-0ubuntu0.1~20.04.2 [96.3 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal/main amd64 libhunspell-1.7-0 amd64 1.7.0-2build2 [147 kB]\nGet:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libenchant-2-2 amd64 2.2.8-1ubuntu0.20.04.1 [45.6 kB]\nGet:17 http://archive.ubuntu.com/ubuntu focal/universe amd64 libenchant1c2a amd64 1.6.0-11.3build1 [64.7 kB]\nGet:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgtk-3-common all 3.24.20-0ubuntu1.2 [234 kB]\nGet:19 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgtk-3-0 amd64 3.24.20-0ubuntu1.2 [2620 kB]\nGet:20 http://archive.ubuntu.com/ubuntu focal/main amd64 libgudev-1.0-0 amd64 1:233-1 [14.0 kB]\nGet:21 http://archive.ubuntu.com/ubuntu focal/main amd64 libhyphen0 amd64 2.8.8-7 [27.0 kB]\nGet:22 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libnotify4 amd64 0.7.9-1ubuntu3.20.04.2 [19.5 kB]\nGet:23 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsecret-common all 0.20.4-0ubuntu1 [3940 B]\nGet:24 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsecret-1-0 amd64 0.20.4-0ubuntu1 [110 kB]\nGet:25 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libwebpdemux2 amd64 0.6.1-2ubuntu0.20.04.3 [9560 B]\nGet:26 http://archive.ubuntu.com/ubuntu focal/main amd64 libwoff1 amd64 1.0.2-1build2 [42.0 kB]\nGet:27 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libxslt1.1 amd64 1.1.34-4ubuntu0.20.04.1 [151 kB]\nGet:28 http://archive.ubuntu.com/ubuntu focal/universe amd64 ttf-ubuntu-font-family all 1:0.83-4ubuntu1 [10.2 kB]\nGet:29 http://archive.ubuntu.com/ubuntu focal/main amd64 ttf-unifont all 1:12.0.01-2 [3137 kB]\nGet:30 http://archive.ubuntu.com/ubuntu focal/universe amd64 xfonts-cyrillic all 1:1.0.4 [387 kB]\nGet:31 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-scalable all 1:1.0.3-1.1 [304 kB]\nGet:32 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libegl1 amd64 1.3.2-1~ubuntu0.20.04.2 [31.9 kB]\nGet:33 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libevdev2 amd64 1.9.0+dfsg-1ubuntu0.2 [31.6 kB]\nGet:34 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgles2 amd64 1.3.2-1~ubuntu0.20.04.2 [15.6 kB]\nGet:35 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libopengl0 amd64 1.3.2-1~ubuntu0.20.04.2 [29.2 kB]\nFetched 31.1 MB in 4s (7508 kB/s)      \nExtracting templates from packages: 100%\nPreconfiguring packages ...\nSelecting previously unselected package fonts-ipafont-gothic.\n(Reading database ... 115958 files and directories currently installed.)\nPreparing to unpack .../00-fonts-ipafont-gothic_00303-18ubuntu1_all.deb ...\nUnpacking fonts-ipafont-gothic (00303-18ubuntu1) ...\nSelecting previously unselected package fonts-wqy-zenhei.\nPreparing to unpack .../01-fonts-wqy-zenhei_0.9.45-7ubuntu1_all.deb ...\nUnpacking fonts-wqy-zenhei (0.9.45-7ubuntu1) ...\nSelecting previously unselected package libtext-iconv-perl.\nPreparing to unpack .../02-libtext-iconv-perl_1.7-7_amd64.deb ...\nUnpacking libtext-iconv-perl (1.7-7) ...\nSelecting previously unselected package dictionaries-common.\nPreparing to unpack .../03-dictionaries-common_1.28.1_all.deb ...\nAdding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\nUnpacking dictionaries-common (1.28.1) ...\nSelecting previously unselected package fonts-noto-color-emoji.\nPreparing to unpack .../04-fonts-noto-color-emoji_0~20200916-1~ubuntu20.04.1_all.deb ...\nUnpacking fonts-noto-color-emoji (0~20200916-1~ubuntu20.04.1) ...\nSelecting previously unselected package fonts-tlwg-loma-otf.\nPreparing to unpack .../05-fonts-tlwg-loma-otf_1%3a0.7.1-3_all.deb ...\nUnpacking fonts-tlwg-loma-otf (1:0.7.1-3) ...\nSelecting previously unselected package fonts-ubuntu.\nPreparing to unpack .../06-fonts-ubuntu_0.83-4ubuntu1_all.deb ...\nUnpacking fonts-ubuntu (0.83-4ubuntu1) ...\nSelecting previously unselected package hunspell-en-us.\nPreparing to unpack .../07-hunspell-en-us_1%3a2018.04.16-1_all.deb ...\nUnpacking hunspell-en-us (1:2018.04.16-1) ...\nSelecting previously unselected package libaspell15:amd64.\nPreparing to unpack .../08-libaspell15_0.60.8-1ubuntu0.1_amd64.deb ...\nUnpacking libaspell15:amd64 (0.60.8-1ubuntu0.1) ...\nPreparing to unpack .../09-libcups2_2.3.1-9ubuntu1.9_amd64.deb ...\nUnpacking libcups2:amd64 (2.3.1-9ubuntu1.9) over (2.3.1-9ubuntu1.6) ...\nSelecting previously unselected package libdbus-glib-1-2:amd64.\nPreparing to unpack .../10-libdbus-glib-1-2_0.110-5fakssync1_amd64.deb ...\nUnpacking libdbus-glib-1-2:amd64 (0.110-5fakssync1) ...\nSelecting previously unselected package libwayland-server0:amd64.\nPreparing to unpack .../11-libwayland-server0_1.18.0-1ubuntu0.1_amd64.deb ...\nUnpacking libwayland-server0:amd64 (1.18.0-1ubuntu0.1) ...\nSelecting previously unselected package libgbm1:amd64.\nPreparing to unpack .../12-libgbm1_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libgbm1:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSelecting previously unselected package libegl-mesa0:amd64.\nPreparing to unpack .../13-libegl-mesa0_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libegl-mesa0:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSelecting previously unselected package libhunspell-1.7-0:amd64.\nPreparing to unpack .../14-libhunspell-1.7-0_1.7.0-2build2_amd64.deb ...\nUnpacking libhunspell-1.7-0:amd64 (1.7.0-2build2) ...\nSelecting previously unselected package libenchant-2-2:amd64.\nPreparing to unpack .../15-libenchant-2-2_2.2.8-1ubuntu0.20.04.1_amd64.deb ...\nUnpacking libenchant-2-2:amd64 (2.2.8-1ubuntu0.20.04.1) ...\nSelecting previously unselected package libenchant1c2a:amd64.\nPreparing to unpack .../16-libenchant1c2a_1.6.0-11.3build1_amd64.deb ...\nUnpacking libenchant1c2a:amd64 (1.6.0-11.3build1) ...\nPreparing to unpack .../17-libgtk-3-common_3.24.20-0ubuntu1.2_all.deb ...\nUnpacking libgtk-3-common (3.24.20-0ubuntu1.2) over (3.24.20-0ubuntu1.1) ...\nPreparing to unpack .../18-libgtk-3-0_3.24.20-0ubuntu1.2_amd64.deb ...\nUnpacking libgtk-3-0:amd64 (3.24.20-0ubuntu1.2) over (3.24.20-0ubuntu1.1) ...\nSelecting previously unselected package libgudev-1.0-0:amd64.\nPreparing to unpack .../19-libgudev-1.0-0_1%3a233-1_amd64.deb ...\nUnpacking libgudev-1.0-0:amd64 (1:233-1) ...\nSelecting previously unselected package libhyphen0:amd64.\nPreparing to unpack .../20-libhyphen0_2.8.8-7_amd64.deb ...\nUnpacking libhyphen0:amd64 (2.8.8-7) ...\nSelecting previously unselected package libnotify4:amd64.\nPreparing to unpack .../21-libnotify4_0.7.9-1ubuntu3.20.04.2_amd64.deb ...\nUnpacking libnotify4:amd64 (0.7.9-1ubuntu3.20.04.2) ...\nSelecting previously unselected package libsecret-common.\nPreparing to unpack .../22-libsecret-common_0.20.4-0ubuntu1_all.deb ...\nUnpacking libsecret-common (0.20.4-0ubuntu1) ...\nSelecting previously unselected package libsecret-1-0:amd64.\nPreparing to unpack .../23-libsecret-1-0_0.20.4-0ubuntu1_amd64.deb ...\nUnpacking libsecret-1-0:amd64 (0.20.4-0ubuntu1) ...\nSelecting previously unselected package libwebpdemux2:amd64.\nPreparing to unpack .../24-libwebpdemux2_0.6.1-2ubuntu0.20.04.3_amd64.deb ...\nUnpacking libwebpdemux2:amd64 (0.6.1-2ubuntu0.20.04.3) ...\nSelecting previously unselected package libwoff1:amd64.\nPreparing to unpack .../25-libwoff1_1.0.2-1build2_amd64.deb ...\nUnpacking libwoff1:amd64 (1.0.2-1build2) ...\nSelecting previously unselected package libxslt1.1:amd64.\nPreparing to unpack .../26-libxslt1.1_1.1.34-4ubuntu0.20.04.1_amd64.deb ...\nUnpacking libxslt1.1:amd64 (1.1.34-4ubuntu0.20.04.1) ...\nSelecting previously unselected package ttf-ubuntu-font-family.\nPreparing to unpack .../27-ttf-ubuntu-font-family_1%3a0.83-4ubuntu1_all.deb ...\nUnpacking ttf-ubuntu-font-family (1:0.83-4ubuntu1) ...\nSelecting previously unselected package ttf-unifont.\nPreparing to unpack .../28-ttf-unifont_1%3a12.0.01-2_all.deb ...\nUnpacking ttf-unifont (1:12.0.01-2) ...\nSelecting previously unselected package xfonts-cyrillic.\nPreparing to unpack .../29-xfonts-cyrillic_1%3a1.0.4_all.deb ...\nUnpacking xfonts-cyrillic (1:1.0.4) ...\nSelecting previously unselected package xfonts-scalable.\nPreparing to unpack .../30-xfonts-scalable_1%3a1.0.3-1.1_all.deb ...\nUnpacking xfonts-scalable (1:1.0.3-1.1) ...\nSelecting previously unselected package libegl1:amd64.\nPreparing to unpack .../31-libegl1_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libegl1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSelecting previously unselected package libevdev2:amd64.\nPreparing to unpack .../32-libevdev2_1.9.0+dfsg-1ubuntu0.2_amd64.deb ...\nUnpacking libevdev2:amd64 (1.9.0+dfsg-1ubuntu0.2) ...\nSelecting previously unselected package libgles2:amd64.\nPreparing to unpack .../33-libgles2_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libgles2:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSelecting previously unselected package libopengl0:amd64.\nPreparing to unpack .../34-libopengl0_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\nUnpacking libopengl0:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libtext-iconv-perl (1.7-7) ...\nSetting up libwayland-server0:amd64 (1.18.0-1ubuntu0.1) ...\nSetting up libwoff1:amd64 (1.0.2-1build2) ...\nSetting up libhyphen0:amd64 (2.8.8-7) ...\nSetting up libgbm1:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up dictionaries-common (1.28.1) ...\nInstall emacsen-common for emacs\nemacsen-common: Handling install of emacsen flavor emacs\nInstall dictionaries-common for emacs\ninstall/dictionaries-common: Byte-compiling for emacsen flavour emacs\nSetting up fonts-noto-color-emoji (0~20200916-1~ubuntu20.04.1) ...\nSetting up libaspell15:amd64 (0.60.8-1ubuntu0.1) ...\nSetting up ttf-unifont (1:12.0.01-2) ...\nSetting up fonts-ubuntu (0.83-4ubuntu1) ...\nSetting up libwebpdemux2:amd64 (0.6.1-2ubuntu0.20.04.3) ...\nSetting up fonts-wqy-zenhei (0.9.45-7ubuntu1) ...\nSetting up libopengl0:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up libegl-mesa0:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up libgles2:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up fonts-tlwg-loma-otf (1:0.7.1-3) ...\nSetting up libdbus-glib-1-2:amd64 (0.110-5fakssync1) ...\nSetting up libnotify4:amd64 (0.7.9-1ubuntu3.20.04.2) ...\nSetting up libcups2:amd64 (2.3.1-9ubuntu1.9) ...\nSetting up fonts-ipafont-gothic (00303-18ubuntu1) ...\nupdate-alternatives: using /usr/share/fonts/opentype/ipafont-gothic/ipag.ttf to provide /usr/share/fonts/truetype/fonts-japanese-gothic.ttf (fonts-japanese-gothic.ttf) in auto mode\nSetting up xfonts-cyrillic (1:1.0.4) ...\nSetting up libxslt1.1:amd64 (1.1.34-4ubuntu0.20.04.1) ...\nSetting up libegl1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\nSetting up hunspell-en-us (1:2018.04.16-1) ...\nSetting up libhunspell-1.7-0:amd64 (1.7.0-2build2) ...\nSetting up ttf-ubuntu-font-family (1:0.83-4ubuntu1) ...\nSetting up libevdev2:amd64 (1.9.0+dfsg-1ubuntu0.2) ...\nSetting up libgudev-1.0-0:amd64 (1:233-1) ...\nSetting up libsecret-common (0.20.4-0ubuntu1) ...\nSetting up xfonts-scalable (1:1.0.3-1.1) ...\nSetting up libgtk-3-common (3.24.20-0ubuntu1.2) ...\nSetting up libenchant-2-2:amd64 (2.2.8-1ubuntu0.20.04.1) ...\nSetting up libenchant1c2a:amd64 (1.6.0-11.3build1) ...\nSetting up libsecret-1-0:amd64 (0.20.4-0ubuntu1) ...\nProcessing triggers for libc-bin (2.31-0ubuntu9.16) ...\nProcessing triggers for man-db (2.9.1-1) ...\nProcessing triggers for fontconfig (2.13.1-2ubuntu3) ...\nProcessing triggers for libglib2.0-0:amd64 (2.64.6-1~ubuntu20.04.8) ...\nSetting up libgtk-3-0:amd64 (3.24.20-0ubuntu1.2) ...\nProcessing triggers for dictionaries-common (1.28.1) ...\nProcessing triggers for libc-bin (2.31-0ubuntu9.16) ...\nDownloading Chromium 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-linux.zip\u001b[22m\n\u001b[1G161.3 MiB [                    ] 0% 0.0s\u001b[0K\u001b[1G161.3 MiB [                    ] 0% 5.0s\u001b[0K\u001b[1G161.3 MiB [                    ] 1% 2.9s\u001b[0K\u001b[1G161.3 MiB [                    ] 1% 2.5s\u001b[0K\u001b[1G161.3 MiB [=                   ] 2% 2.3s\u001b[0K\u001b[1G161.3 MiB [=                   ] 3% 2.1s\u001b[0K\u001b[1G161.3 MiB [=                   ] 4% 2.0s\u001b[0K\u001b[1G161.3 MiB [=                   ] 5% 1.9s\u001b[0K\u001b[1G161.3 MiB [=                   ] 6% 1.8s\u001b[0K\u001b[1G161.3 MiB [=                   ] 6% 1.9s\u001b[0K\u001b[1G161.3 MiB [==                  ] 7% 1.9s\u001b[0K\u001b[1G161.3 MiB [==                  ] 9% 1.7s\u001b[0K\u001b[1G161.3 MiB [==                  ] 10% 1.7s\u001b[0K\u001b[1G161.3 MiB [==                  ] 11% 1.7s\u001b[0K\u001b[1G161.3 MiB [==                  ] 12% 1.6s\u001b[0K\u001b[1G161.3 MiB [===                 ] 13% 1.5s\u001b[0K\u001b[1G161.3 MiB [===                 ] 15% 1.4s\u001b[0K\u001b[1G161.3 MiB [===                 ] 16% 1.4s\u001b[0K\u001b[1G161.3 MiB [====                ] 17% 1.3s\u001b[0K\u001b[1G161.3 MiB [====                ] 19% 1.3s\u001b[0K\u001b[1G161.3 MiB [====                ] 21% 1.2s\u001b[0K\u001b[1G161.3 MiB [=====               ] 22% 1.2s\u001b[0K\u001b[1G161.3 MiB [=====               ] 23% 1.1s\u001b[0K\u001b[1G161.3 MiB [=====               ] 25% 1.1s\u001b[0K\u001b[1G161.3 MiB [=====               ] 27% 1.0s\u001b[0K\u001b[1G161.3 MiB [======              ] 29% 1.0s\u001b[0K\u001b[1G161.3 MiB [======              ] 30% 1.0s\u001b[0K\u001b[1G161.3 MiB [======              ] 32% 0.9s\u001b[0K\u001b[1G161.3 MiB [=======             ] 33% 0.9s\u001b[0K\u001b[1G161.3 MiB [=======             ] 35% 0.9s\u001b[0K\u001b[1G161.3 MiB [=======             ] 37% 0.8s\u001b[0K\u001b[1G161.3 MiB [========            ] 39% 0.8s\u001b[0K\u001b[1G161.3 MiB [========            ] 41% 0.8s\u001b[0K\u001b[1G161.3 MiB [========            ] 42% 0.7s\u001b[0K\u001b[1G161.3 MiB [=========           ] 43% 0.7s\u001b[0K\u001b[1G161.3 MiB [=========           ] 45% 0.7s\u001b[0K\u001b[1G161.3 MiB [=========           ] 46% 0.7s\u001b[0K\u001b[1G161.3 MiB [==========          ] 48% 0.7s\u001b[0K\u001b[1G161.3 MiB [==========          ] 49% 0.6s\u001b[0K\u001b[1G161.3 MiB [==========          ] 51% 0.6s\u001b[0K\u001b[1G161.3 MiB [===========         ] 52% 0.6s\u001b[0K\u001b[1G161.3 MiB [===========         ] 54% 0.6s\u001b[0K\u001b[1G161.3 MiB [===========         ] 55% 0.5s\u001b[0K\u001b[1G161.3 MiB [===========         ] 57% 0.5s\u001b[0K\u001b[1G161.3 MiB [============        ] 58% 0.5s\u001b[0K\u001b[1G161.3 MiB [============        ] 60% 0.5s\u001b[0K\u001b[1G161.3 MiB [============        ] 61% 0.5s\u001b[0K\u001b[1G161.3 MiB [=============       ] 63% 0.4s\u001b[0K\u001b[1G161.3 MiB [=============       ] 64% 0.4s\u001b[0K\u001b[1G161.3 MiB [=============       ] 66% 0.4s\u001b[0K\u001b[1G161.3 MiB [==============      ] 68% 0.4s\u001b[0K\u001b[1G161.3 MiB [==============      ] 70% 0.4s\u001b[0K\u001b[1G161.3 MiB [==============      ] 71% 0.3s\u001b[0K\u001b[1G161.3 MiB [===============     ] 73% 0.3s\u001b[0K\u001b[1G161.3 MiB [===============     ] 74% 0.3s\u001b[0K\u001b[1G161.3 MiB [===============     ] 76% 0.3s\u001b[0K\u001b[1G161.3 MiB [================    ] 77% 0.3s\u001b[0K\u001b[1G161.3 MiB [================    ] 79% 0.2s\u001b[0K\u001b[1G161.3 MiB [================    ] 80% 0.2s\u001b[0K\u001b[1G161.3 MiB [=================   ] 82% 0.2s\u001b[0K\u001b[1G161.3 MiB [=================   ] 84% 0.2s\u001b[0K\u001b[1G161.3 MiB [=================   ] 85% 0.2s\u001b[0K\u001b[1G161.3 MiB [=================   ] 87% 0.1s\u001b[0K\u001b[1G161.3 MiB [==================  ] 88% 0.1s\u001b[0K\u001b[1G161.3 MiB [==================  ] 90% 0.1s\u001b[0K\u001b[1G161.3 MiB [==================  ] 91% 0.1s\u001b[0K\u001b[1G161.3 MiB [=================== ] 93% 0.1s\u001b[0K\u001b[1G161.3 MiB [=================== ] 95% 0.1s\u001b[0K\u001b[1G161.3 MiB [=================== ] 97% 0.0s\u001b[0K\u001b[1G161.3 MiB [====================] 99% 0.0s\u001b[0K\u001b[1G161.3 MiB [====================] 100% 0.0s\u001b[0K\nChromium 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium-1148\nDownloading Chromium Headless Shell 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-headless-shell-linux.zip\u001b[22m\n\u001b[1G100.9 MiB [                    ] 0% 0.0s\u001b[0K\u001b[1G100.9 MiB [                    ] 0% 4.5s\u001b[0K\u001b[1G100.9 MiB [                    ] 0% 12.7s\u001b[0K\u001b[1G100.9 MiB [                    ] 2% 6.1s\u001b[0K\u001b[1G100.9 MiB [=                   ] 3% 4.0s\u001b[0K\u001b[1G100.9 MiB [=                   ] 5% 3.0s\u001b[0K\u001b[1G100.9 MiB [=                   ] 7% 2.4s\u001b[0K\u001b[1G100.9 MiB [==                  ] 8% 2.2s\u001b[0K\u001b[1G100.9 MiB [==                  ] 9% 3.1s\u001b[0K\u001b[1G100.9 MiB [==                  ] 10% 2.9s\u001b[0K\u001b[1G100.9 MiB [==                  ] 11% 2.8s\u001b[0K\u001b[1G100.9 MiB [==                  ] 12% 2.7s\u001b[0K\u001b[1G100.9 MiB [===                 ] 13% 2.6s\u001b[0K\u001b[1G100.9 MiB [===                 ] 14% 2.5s\u001b[0K\u001b[1G100.9 MiB [===                 ] 16% 2.3s\u001b[0K\u001b[1G100.9 MiB [====                ] 17% 2.1s\u001b[0K\u001b[1G100.9 MiB [====                ] 19% 1.9s\u001b[0K\u001b[1G100.9 MiB [====                ] 21% 1.7s\u001b[0K\u001b[1G100.9 MiB [=====               ] 23% 1.6s\u001b[0K\u001b[1G100.9 MiB [=====               ] 24% 1.6s\u001b[0K\u001b[1G100.9 MiB [=====               ] 25% 1.5s\u001b[0K\u001b[1G100.9 MiB [=====               ] 27% 1.5s\u001b[0K\u001b[1G100.9 MiB [======              ] 28% 1.4s\u001b[0K\u001b[1G100.9 MiB [======              ] 29% 1.4s\u001b[0K\u001b[1G100.9 MiB [======              ] 31% 1.3s\u001b[0K\u001b[1G100.9 MiB [=======             ] 33% 1.2s\u001b[0K\u001b[1G100.9 MiB [=======             ] 34% 1.2s\u001b[0K\u001b[1G100.9 MiB [=======             ] 36% 1.1s\u001b[0K\u001b[1G100.9 MiB [========            ] 38% 1.0s\u001b[0K\u001b[1G100.9 MiB [========            ] 40% 1.0s\u001b[0K\u001b[1G100.9 MiB [========            ] 41% 1.0s\u001b[0K\u001b[1G100.9 MiB [=========           ] 42% 1.0s\u001b[0K\u001b[1G100.9 MiB [=========           ] 44% 0.9s\u001b[0K\u001b[1G100.9 MiB [=========           ] 45% 0.9s\u001b[0K\u001b[1G100.9 MiB [=========           ] 46% 0.9s\u001b[0K\u001b[1G100.9 MiB [==========          ] 48% 0.8s\u001b[0K\u001b[1G100.9 MiB [==========          ] 50% 0.8s\u001b[0K\u001b[1G100.9 MiB [==========          ] 52% 0.7s\u001b[0K\u001b[1G100.9 MiB [===========         ] 53% 0.7s\u001b[0K\u001b[1G100.9 MiB [===========         ] 55% 0.7s\u001b[0K\u001b[1G100.9 MiB [===========         ] 56% 0.6s\u001b[0K\u001b[1G100.9 MiB [============        ] 58% 0.6s\u001b[0K\u001b[1G100.9 MiB [============        ] 60% 0.6s\u001b[0K\u001b[1G100.9 MiB [============        ] 61% 0.6s\u001b[0K\u001b[1G100.9 MiB [=============       ] 62% 0.5s\u001b[0K\u001b[1G100.9 MiB [=============       ] 63% 0.5s\u001b[0K\u001b[1G100.9 MiB [=============       ] 64% 0.5s\u001b[0K\u001b[1G100.9 MiB [=============       ] 66% 0.5s\u001b[0K\u001b[1G100.9 MiB [==============      ] 67% 0.5s\u001b[0K\u001b[1G100.9 MiB [==============      ] 69% 0.5s\u001b[0K\u001b[1G100.9 MiB [==============      ] 70% 0.4s\u001b[0K\u001b[1G100.9 MiB [==============      ] 71% 0.4s\u001b[0K\u001b[1G100.9 MiB [===============     ] 73% 0.4s\u001b[0K\u001b[1G100.9 MiB [===============     ] 74% 0.4s\u001b[0K\u001b[1G100.9 MiB [===============     ] 76% 0.3s\u001b[0K\u001b[1G100.9 MiB [===============     ] 77% 0.3s\u001b[0K\u001b[1G100.9 MiB [================    ] 78% 0.3s\u001b[0K\u001b[1G100.9 MiB [================    ] 79% 0.3s\u001b[0K\u001b[1G100.9 MiB [================    ] 80% 0.3s\u001b[0K\u001b[1G100.9 MiB [=================   ] 82% 0.2s\u001b[0K\u001b[1G100.9 MiB [=================   ] 84% 0.2s\u001b[0K\u001b[1G100.9 MiB [=================   ] 86% 0.2s\u001b[0K\u001b[1G100.9 MiB [==================  ] 87% 0.2s\u001b[0K\u001b[1G100.9 MiB [==================  ] 88% 0.2s\u001b[0K\u001b[1G100.9 MiB [==================  ] 89% 0.1s\u001b[0K\u001b[1G100.9 MiB [==================  ] 90% 0.1s\u001b[0K\u001b[1G100.9 MiB [==================  ] 92% 0.1s\u001b[0K\u001b[1G100.9 MiB [=================== ] 93% 0.1s\u001b[0K\u001b[1G100.9 MiB [=================== ] 94% 0.1s\u001b[0K\u001b[1G100.9 MiB [=================== ] 96% 0.0s\u001b[0K\u001b[1G100.9 MiB [====================] 97% 0.0s\u001b[0K\u001b[1G100.9 MiB [====================] 98% 0.0s\u001b[0K\u001b[1G100.9 MiB [====================] 99% 0.0s\u001b[0K\u001b[1G100.9 MiB [====================] 100% 0.0s\u001b[0K\nChromium Headless Shell 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1148\nDownloading Firefox 132.0 (playwright build v1466)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1466/firefox-ubuntu-20.04.zip\u001b[22m\n\u001b[1G87.6 MiB [                    ] 0% 0.0s\u001b[0K\u001b[1G87.6 MiB [                    ] 0% 3.4s\u001b[0K\u001b[1G87.6 MiB [                    ] 1% 1.8s\u001b[0K\u001b[1G87.6 MiB [=                   ] 2% 1.6s\u001b[0K\u001b[1G87.6 MiB [=                   ] 4% 1.5s\u001b[0K\u001b[1G87.6 MiB [=                   ] 5% 1.3s\u001b[0K\u001b[1G87.6 MiB [=                   ] 7% 1.2s\u001b[0K\u001b[1G87.6 MiB [==                  ] 9% 1.1s\u001b[0K\u001b[1G87.6 MiB [==                  ] 10% 1.1s\u001b[0K\u001b[1G87.6 MiB [==                  ] 12% 1.1s\u001b[0K\u001b[1G87.6 MiB [===                 ] 13% 1.1s\u001b[0K\u001b[1G87.6 MiB [===                 ] 15% 1.0s\u001b[0K\u001b[1G87.6 MiB [====                ] 17% 0.9s\u001b[0K\u001b[1G87.6 MiB [====                ] 19% 0.9s\u001b[0K\u001b[1G87.6 MiB [====                ] 20% 0.9s\u001b[0K\u001b[1G87.6 MiB [=====               ] 22% 0.9s\u001b[0K\u001b[1G87.6 MiB [=====               ] 24% 0.8s\u001b[0K\u001b[1G87.6 MiB [=====               ] 26% 0.8s\u001b[0K\u001b[1G87.6 MiB [======              ] 28% 0.7s\u001b[0K\u001b[1G87.6 MiB [======              ] 30% 0.7s\u001b[0K\u001b[1G87.6 MiB [=======             ] 33% 0.7s\u001b[0K\u001b[1G87.6 MiB [=======             ] 35% 0.6s\u001b[0K\u001b[1G87.6 MiB [========            ] 37% 0.6s\u001b[0K\u001b[1G87.6 MiB [========            ] 40% 0.6s\u001b[0K\u001b[1G87.6 MiB [=========           ] 43% 0.5s\u001b[0K\u001b[1G87.6 MiB [=========           ] 45% 0.5s\u001b[0K\u001b[1G87.6 MiB [==========          ] 48% 0.5s\u001b[0K\u001b[1G87.6 MiB [==========          ] 50% 0.4s\u001b[0K\u001b[1G87.6 MiB [===========         ] 53% 0.4s\u001b[0K\u001b[1G87.6 MiB [===========         ] 54% 0.4s\u001b[0K\u001b[1G87.6 MiB [===========         ] 56% 0.4s\u001b[0K\u001b[1G87.6 MiB [============        ] 59% 0.3s\u001b[0K\u001b[1G87.6 MiB [============        ] 62% 0.3s\u001b[0K\u001b[1G87.6 MiB [=============       ] 65% 0.3s\u001b[0K\u001b[1G87.6 MiB [==============      ] 68% 0.3s\u001b[0K\u001b[1G87.6 MiB [==============      ] 71% 0.2s\u001b[0K\u001b[1G87.6 MiB [===============     ] 74% 0.2s\u001b[0K\u001b[1G87.6 MiB [================    ] 78% 0.2s\u001b[0K\u001b[1G87.6 MiB [================    ] 80% 0.1s\u001b[0K\u001b[1G87.6 MiB [=================   ] 83% 0.1s\u001b[0K\u001b[1G87.6 MiB [=================   ] 86% 0.1s\u001b[0K\u001b[1G87.6 MiB [==================  ] 89% 0.1s\u001b[0K\u001b[1G87.6 MiB [==================  ] 92% 0.1s\u001b[0K\u001b[1G87.6 MiB [=================== ] 94% 0.0s\u001b[0K\u001b[1G87.6 MiB [====================] 98% 0.0s\u001b[0K\u001b[1G87.6 MiB [====================] 100% 0.0s\u001b[0K\nFirefox 132.0 (playwright build v1466) downloaded to /root/.cache/ms-playwright/firefox-1466\nYou are using a frozen webkit browser which does not receive updates anymore on ubuntu20.04-x64. Please update to the latest version of your operating system to test up-to-date browsers.\nDownloading Webkit playwright build v2092\u001b[2m from https://playwright.azureedge.net/builds/webkit/2092/webkit-ubuntu-20.04.zip\u001b[22m\n\u001b[1G142.7 MiB [                    ] 0% 0.0s\u001b[0K\u001b[1G142.7 MiB [                    ] 0% 7.7s\u001b[0K\u001b[1G142.7 MiB [                    ] 1% 3.0s\u001b[0K\u001b[1G142.7 MiB [                    ] 1% 2.5s\u001b[0K\u001b[1G142.7 MiB [=                   ] 2% 2.3s\u001b[0K\u001b[1G142.7 MiB [=                   ] 3% 2.0s\u001b[0K\u001b[1G142.7 MiB [=                   ] 5% 1.8s\u001b[0K\u001b[1G142.7 MiB [=                   ] 6% 1.7s\u001b[0K\u001b[1G142.7 MiB [=                   ] 7% 1.6s\u001b[0K\u001b[1G142.7 MiB [==                  ] 7% 1.7s\u001b[0K\u001b[1G142.7 MiB [==                  ] 9% 1.6s\u001b[0K\u001b[1G142.7 MiB [==                  ] 10% 1.5s\u001b[0K\u001b[1G142.7 MiB [==                  ] 11% 1.5s\u001b[0K\u001b[1G142.7 MiB [===                 ] 12% 1.4s\u001b[0K\u001b[1G142.7 MiB [===                 ] 13% 1.4s\u001b[0K\u001b[1G142.7 MiB [===                 ] 15% 1.3s\u001b[0K\u001b[1G142.7 MiB [===                 ] 16% 1.3s\u001b[0K\u001b[1G142.7 MiB [====                ] 17% 1.3s\u001b[0K\u001b[1G142.7 MiB [====                ] 19% 1.2s\u001b[0K\u001b[1G142.7 MiB [====                ] 20% 1.2s\u001b[0K\u001b[1G142.7 MiB [====                ] 22% 1.1s\u001b[0K\u001b[1G142.7 MiB [=====               ] 23% 1.1s\u001b[0K\u001b[1G142.7 MiB [=====               ] 24% 1.1s\u001b[0K\u001b[1G142.7 MiB [=====               ] 26% 1.0s\u001b[0K\u001b[1G142.7 MiB [=====               ] 27% 1.0s\u001b[0K\u001b[1G142.7 MiB [======              ] 28% 1.0s\u001b[0K\u001b[1G142.7 MiB [======              ] 30% 1.0s\u001b[0K\u001b[1G142.7 MiB [======              ] 31% 0.9s\u001b[0K\u001b[1G142.7 MiB [=======             ] 33% 0.9s\u001b[0K\u001b[1G142.7 MiB [=======             ] 34% 0.9s\u001b[0K\u001b[1G142.7 MiB [=======             ] 36% 0.8s\u001b[0K\u001b[1G142.7 MiB [========            ] 38% 0.8s\u001b[0K\u001b[1G142.7 MiB [========            ] 40% 0.7s\u001b[0K\u001b[1G142.7 MiB [========            ] 42% 0.7s\u001b[0K\u001b[1G142.7 MiB [=========           ] 44% 0.7s\u001b[0K\u001b[1G142.7 MiB [=========           ] 45% 0.7s\u001b[0K\u001b[1G142.7 MiB [==========          ] 47% 0.6s\u001b[0K\u001b[1G142.7 MiB [==========          ] 49% 0.6s\u001b[0K\u001b[1G142.7 MiB [==========          ] 51% 0.6s\u001b[0K\u001b[1G142.7 MiB [===========         ] 52% 0.6s\u001b[0K\u001b[1G142.7 MiB [===========         ] 54% 0.5s\u001b[0K\u001b[1G142.7 MiB [===========         ] 56% 0.5s\u001b[0K\u001b[1G142.7 MiB [============        ] 57% 0.5s\u001b[0K\u001b[1G142.7 MiB [============        ] 59% 0.5s\u001b[0K\u001b[1G142.7 MiB [============        ] 61% 0.4s\u001b[0K\u001b[1G142.7 MiB [=============       ] 62% 0.4s\u001b[0K\u001b[1G142.7 MiB [=============       ] 64% 0.4s\u001b[0K\u001b[1G142.7 MiB [=============       ] 66% 0.4s\u001b[0K\u001b[1G142.7 MiB [==============      ] 68% 0.4s\u001b[0K\u001b[1G142.7 MiB [==============      ] 70% 0.3s\u001b[0K\u001b[1G142.7 MiB [==============      ] 72% 0.3s\u001b[0K\u001b[1G142.7 MiB [===============     ] 74% 0.3s\u001b[0K\u001b[1G142.7 MiB [===============     ] 75% 0.3s\u001b[0K\u001b[1G142.7 MiB [================    ] 77% 0.2s\u001b[0K\u001b[1G142.7 MiB [================    ] 79% 0.2s\u001b[0K\u001b[1G142.7 MiB [================    ] 81% 0.2s\u001b[0K\u001b[1G142.7 MiB [=================   ] 83% 0.2s\u001b[0K\u001b[1G142.7 MiB [=================   ] 85% 0.2s\u001b[0K\u001b[1G142.7 MiB [=================   ] 86% 0.1s\u001b[0K\u001b[1G142.7 MiB [==================  ] 88% 0.1s\u001b[0K\u001b[1G142.7 MiB [==================  ] 90% 0.1s\u001b[0K\u001b[1G142.7 MiB [==================  ] 92% 0.1s\u001b[0K\u001b[1G142.7 MiB [=================== ] 93% 0.1s\u001b[0K\u001b[1G142.7 MiB [=================== ] 95% 0.1s\u001b[0K\u001b[1G142.7 MiB [=================== ] 96% 0.0s\u001b[0K\u001b[1G142.7 MiB [====================] 98% 0.0s\u001b[0K\u001b[1G142.7 MiB [====================] 100% 0.0s\u001b[0K\nWebkit playwright build v2092 downloaded to /root/.cache/ms-playwright/webkit_ubuntu20.04_x64_special-2092\nDownloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n\u001b[1G2.3 MiB [                    ] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [==                  ] 12% 0.1s\u001b[0K\u001b[1G2.3 MiB [=========           ] 44% 0.0s\u001b[0K\u001b[1G2.3 MiB [==================  ] 87% 0.0s\u001b[0K\u001b[1G2.3 MiB [====================] 100% 0.0s\u001b[0K\nFFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n","output_type":"stream"}],"execution_count":1},{"id":"2c2a74c8","cell_type":"code","source":"import asyncio\nimport nest_asyncio\nnest_asyncio.apply()","metadata":{"id":"2c2a74c8","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:26:35.570689Z","iopub.execute_input":"2024-12-18T11:26:35.571062Z","iopub.status.idle":"2024-12-18T11:26:35.582965Z","shell.execute_reply.started":"2024-12-18T11:26:35.571028Z","shell.execute_reply":"2024-12-18T11:26:35.581595Z"}},"outputs":[],"execution_count":2},{"id":"f3c558d7","cell_type":"markdown","source":"#### 2. **Basic Setup and Simple Crawl**","metadata":{"id":"f3c558d7"}},{"id":"003376f3","cell_type":"code","source":"!playwright install chrome\n\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def simple_crawl():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            bypass_cache=True # By default this is False, meaning the cache will be used\n        )\n        print(result.markdown[:500])  # Print the first 500 characters\n\nasyncio.run(simple_crawl())","metadata":{"id":"003376f3","outputId":"1e110cf5-9446-4e4c-9254-31ce56727ba3","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:27:52.533769Z","iopub.execute_input":"2024-12-18T11:27:52.534201Z","iopub.status.idle":"2024-12-18T11:28:21.982288Z","shell.execute_reply.started":"2024-12-18T11:27:52.534164Z","shell.execute_reply":"2024-12-18T11:28:21.980890Z"}},"outputs":[{"name":"stdout","text":"++ arch\n+ [[ x86_64 == \\a\\a\\r\\c\\h\\6\\4 ]]\n+ '[' -z '' ']'\n+ [[ ! -f /etc/os-release ]]\n++ bash -c 'source /etc/os-release && echo $ID'\n+ ID=ubuntu\n+ [[ ubuntu != \\u\\b\\u\\n\\t\\u ]]\n+ dpkg --get-selections\n+ grep -q '^google-chrome[[:space:]]*install$'\n+ apt-get update\nHit:1 https://packages.cloud.google.com/apt gcsfuse-focal InRelease\nHit:2 https://packages.cloud.google.com/apt cloud-sdk InRelease                \nHit:3 http://archive.ubuntu.com/ubuntu focal InRelease                         \nHit:4 http://security.ubuntu.com/ubuntu focal-security InRelease           \nHit:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease\nHit:6 http://archive.ubuntu.com/ubuntu focal-backports InRelease\nReading package lists... Done\n+ command -v curl\n+ cd /tmp\n+ curl -O https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\ncurl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  107M  100  107M    0     0   317M      0 --:--:-- --:--:-- --:--:--  317M\n+ apt-get install -y ./google-chrome-stable_current_amd64.deb\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nNote, selecting 'google-chrome-stable' instead of './google-chrome-stable_current_amd64.deb'\nThe following NEW packages will be installed:\n  google-chrome-stable\n0 upgraded, 1 newly installed, 0 to remove and 57 not upgraded.\nNeed to get 0 B/112 MB of archives.\nAfter this operation, 366 MB of additional disk space will be used.\nGet:1 /tmp/google-chrome-stable_current_amd64.deb google-chrome-stable amd64 131.0.6778.139-1 [112 MB]\nSelecting previously unselected package google-chrome-stable.\n(Reading database ... 116488 files and directories currently installed.)\nPreparing to unpack .../google-chrome-stable_current_amd64.deb ...\nUnpacking google-chrome-stable (131.0.6778.139-1) ...\nSetting up google-chrome-stable (131.0.6778.139-1) ...\nupdate-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\nupdate-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\nupdate-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\nProcessing triggers for man-db (2.9.1-1) ...\nProcessing triggers for mime-support (3.64ubuntu1) ...\n+ rm -rf ./google-chrome-stable_current_amd64.deb\n+ cd -\n/opt/conda/lib/python3.10/site-packages/playwright/driver/package/bin\n+ google-chrome --version\nGoogle Chrome 131.0.6778.139 \n[INIT].... ‚Üí Crawl4AI 0.4.23\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_24/1654752836.py:8: DeprecationWarning: Cache control boolean flags are deprecated and will be removed in version 0.5.0. Use 'cache_mode' parameter instead.\n  result = await crawler.arun(\n","output_type":"stream"},{"name":"stdout","text":"[FETCH]... ‚Üì https://www.nbcnews.com/business... | Status: True | Time: 3.97s\n[SCRAPE].. ‚óÜ Processed https://www.nbcnews.com/business... | Time: 794ms\n[COMPLETE] ‚óè https://www.nbcnews.com/business... | Status: True | Total: 4.78s\nIE 11 is not supported. For an optimal experience visit our site on another browser.\n\nSkip to Content\n\n[NBC News Logo](https://www.nbcnews.com)\n\nSponsored By\n\n  * [U.S. News](https://www.nbcnews.com/us-news)\n  * [Politics](https://www.nbcnews.com/politics)\n  * Local\n  * [New York](https://www.nbcnews.com/new-york)\n  * [Los Angeles](https://www.nbcnews.com/los-angeles)\n  * [Chicago](https://www.nbcnews.com/chicago)\n  * [Dallas-Fort Worth](https://www.nbcnews.com/dallas-fort-worth)\n  * [Philadelph\n","output_type":"stream"}],"execution_count":4},{"id":"da9b4d50","cell_type":"markdown","source":"#### 3. **Dynamic Content Handling**","metadata":{"id":"da9b4d50"}},{"id":"5bb8c1e4","cell_type":"code","source":"async def crawl_dynamic_content():\n    # You can use wait_for to wait for a condition to be met before returning the result\n    # wait_for = \"\"\"() => {\n    #     return Array.from(document.querySelectorAll('article.tease-card')).length > 10;\n    # }\"\"\"\n\n    # wait_for can be also just a css selector\n    # wait_for = \"article.tease-card:nth-child(10)\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        js_code = [\n            \"const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();\"\n        ]\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            # wait_for=wait_for,\n            bypass_cache=True,\n        )\n        print(result.markdown[:500])  # Print first 500 characters\n\nasyncio.run(crawl_dynamic_content())","metadata":{"id":"5bb8c1e4","outputId":"95d19c12-2259-4d25-dad5-562ab027324b","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:28:30.787296Z","iopub.execute_input":"2024-12-18T11:28:30.787787Z","iopub.status.idle":"2024-12-18T11:28:37.707414Z","shell.execute_reply.started":"2024-12-18T11:28:30.787746Z","shell.execute_reply":"2024-12-18T11:28:37.702505Z"}},"outputs":[{"name":"stdout","text":"[INIT].... ‚Üí Crawl4AI 0.4.23\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_24/4258180376.py:14: DeprecationWarning: Cache control boolean flags are deprecated and will be removed in version 0.5.0. Use 'cache_mode' parameter instead.\n  result = await crawler.arun(\n","output_type":"stream"},{"name":"stdout","text":"[FETCH]... ‚Üì https://www.nbcnews.com/business... | Status: True | Time: 4.70s\n[SCRAPE].. ‚óÜ Processed https://www.nbcnews.com/business... | Time: 1046ms\n[COMPLETE] ‚óè https://www.nbcnews.com/business... | Status: True | Total: 5.76s\nIE 11 is not supported. For an optimal experience visit our site on another browser.\n\nSkip to Content\n\n[NBC News Logo](https://www.nbcnews.com)\n\nSponsored By\n\n  * [U.S. News](https://www.nbcnews.com/us-news)\n  * [Politics](https://www.nbcnews.com/politics)\n  * Local\n  * [New York](https://www.nbcnews.com/new-york)\n  * [Los Angeles](https://www.nbcnews.com/los-angeles)\n  * [Chicago](https://www.nbcnews.com/chicago)\n  * [Dallas-Fort Worth](https://www.nbcnews.com/dallas-fort-worth)\n  * [Philadelph\n","output_type":"stream"}],"execution_count":5},{"id":"86febd8d","cell_type":"markdown","source":"#### 4. **Content Cleaning and Fit Markdown**","metadata":{"id":"86febd8d"}},{"id":"8e8ab01f","cell_type":"code","source":"async def clean_content():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://janineintheworld.com/places-to-visit-in-central-mexico\",\n            excluded_tags=['nav', 'footer', 'aside'],\n            remove_overlay_elements=True,\n            word_count_threshold=10,\n            bypass_cache=True\n        )\n        full_markdown_length = len(result.markdown)\n        fit_markdown_length = len(result.fit_markdown)\n        print(f\"Full Markdown Length: {full_markdown_length}\")\n        print(f\"Fit Markdown Length: {fit_markdown_length}\")\n        print(result.fit_markdown[:1000])\n\n\nasyncio.run(clean_content())","metadata":{"id":"8e8ab01f","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:29:33.565808Z","iopub.execute_input":"2024-12-18T11:29:33.566249Z","iopub.status.idle":"2024-12-18T11:29:43.054241Z","shell.execute_reply.started":"2024-12-18T11:29:33.566202Z","shell.execute_reply":"2024-12-18T11:29:43.052578Z"}},"outputs":[{"name":"stdout","text":"[INIT].... ‚Üí Crawl4AI 0.4.23\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_24/4046680307.py:3: DeprecationWarning: Cache control boolean flags are deprecated and will be removed in version 0.5.0. Use 'cache_mode' parameter instead.\n  result = await crawler.arun(\n","output_type":"stream"},{"name":"stdout","text":"[FETCH]... ‚Üì https://janineintheworld.com/places-to-visit-in-ce... | Status: True | Time: 7.94s\n[SCRAPE].. ‚óÜ Processed https://janineintheworld.com/places-to-visit-in-ce... | Time: 379ms\n[COMPLETE] ‚óè https://janineintheworld.com/places-to-visit-in-ce... | Status: True | Total: 8.34s\nFull Markdown Length: 86192\nFit Markdown Length: 0\n\n","output_type":"stream"}],"execution_count":6},{"id":"55715146","cell_type":"markdown","source":"#### 5. **Link Analysis and Smart Filtering**","metadata":{"id":"55715146"}},{"id":"2ae47c69","cell_type":"code","source":"async def link_analysis():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            bypass_cache=True,\n            exclude_external_links=True,\n            exclude_social_media_links=True,\n            # exclude_domains=[\"facebook.com\", \"twitter.com\"]\n        )\n        print(f\"Found {len(result.links['internal'])} internal links\")\n        print(f\"Found {len(result.links['external'])} external links\")\n\n        for link in result.links['internal'][:5]:\n            print(f\"Href: {link['href']}\\nText: {link['text']}\\n\")\n\n\nasyncio.run(link_analysis())","metadata":{"id":"2ae47c69","outputId":"30bcf929-55a9-4f76-835d-042cd97b7520","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:30:52.501072Z","iopub.execute_input":"2024-12-18T11:30:52.501502Z","iopub.status.idle":"2024-12-18T11:30:58.844151Z","shell.execute_reply.started":"2024-12-18T11:30:52.501464Z","shell.execute_reply":"2024-12-18T11:30:58.842860Z"}},"outputs":[{"name":"stdout","text":"[INIT].... ‚Üí Crawl4AI 0.4.23\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_24/2816730214.py:3: DeprecationWarning: Cache control boolean flags are deprecated and will be removed in version 0.5.0. Use 'cache_mode' parameter instead.\n  result = await crawler.arun(\n","output_type":"stream"},{"name":"stdout","text":"[FETCH]... ‚Üì https://www.nbcnews.com/business... | Status: True | Time: 4.48s\n[SCRAPE].. ‚óÜ Processed https://www.nbcnews.com/business... | Time: 807ms\n[COMPLETE] ‚óè https://www.nbcnews.com/business... | Status: True | Total: 5.31s\nFound 121 internal links\nFound 57 external links\nHref: https://www.nbcnews.com\nText: NBC News Logo\n\nHref: https://www.nbcnews.com/us-news\nText: U.S. News\n\nHref: https://www.nbcnews.com/politics\nText: Politics\n\nHref: https://www.nbcnews.com/new-york\nText: New York\n\nHref: https://www.nbcnews.com/los-angeles\nText: Los Angeles\n\n","output_type":"stream"}],"execution_count":7},{"id":"80cceef3","cell_type":"markdown","source":"#### 6. **Media Handling**","metadata":{"id":"80cceef3"}},{"id":"1fed7f99","cell_type":"code","source":"async def media_handling():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            bypass_cache=True,\n            exclude_external_images=False,\n            screenshot=True\n        )\n        for img in result.media['images'][:5]:\n            print(f\"Image URL: {img['src']}, Alt: {img['alt']}, Score: {img['score']}\")\n\nasyncio.run(media_handling())","metadata":{"id":"1fed7f99","outputId":"c2316755-925e-4bad-d2ed-deffd0cef4f2","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:32:17.075690Z","iopub.execute_input":"2024-12-18T11:32:17.076136Z","iopub.status.idle":"2024-12-18T11:32:27.201637Z","shell.execute_reply.started":"2024-12-18T11:32:17.076098Z","shell.execute_reply":"2024-12-18T11:32:27.200616Z"}},"outputs":[{"name":"stdout","text":"[INIT].... ‚Üí Crawl4AI 0.4.23\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_24/1847414041.py:3: DeprecationWarning: Cache control boolean flags are deprecated and will be removed in version 0.5.0. Use 'cache_mode' parameter instead.\n  result = await crawler.arun(\n","output_type":"stream"},{"name":"stdout","text":"[EXPORT].. ‚Ñπ Exporting PDF and taking screenshot took 2.36s\n[FETCH]... ‚Üì https://www.nbcnews.com/business... | Status: True | Time: 8.27s\n[SCRAPE].. ‚óÜ Processed https://www.nbcnews.com/business... | Time: 908ms\n[COMPLETE] ‚óè https://www.nbcnews.com/business... | Status: True | Total: 9.21s\nImage URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-762x508,f_auto,q_auto:best/rockcms/2024-12/241217-jerome-powell-se-308p-1c8f65.jpg, Alt: Jerome Powell speaks At NYT DealBook Summit, Score: 5\nImage URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-762x508,f_auto,q_auto:best/rockcms/2024-12/241217-joe-biden-ac-557p-f3d491.jpg, Alt: Image: Democratic National Convention (DNC) 2024 - Day One, Score: 5\nImage URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-80x80,f_auto,q_auto:best/rockcms/2024-12/241217-grubhub-se-247p-45d19e.jpg, Alt: A Grubhub delivery worker on his bike, Score: 5\nImage URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-80x80,f_auto,q_auto:best/rockcms/2024-09/240909-los-angeles-starbucks-2022-ac-531p-f7fc9e.jpg, Alt: starbucks sign pedestrian dusk, Score: 5\nImage URL: https://media-cldnry.s-nbcnews.com/image/upload/t_focal-80x80,f_auto,q_auto:best/rockcms/2024-12/241217-honda-nissan-japan-ew-200p-e8cc14.jpg, Alt: Makoto Uchida, president and CEO of Nissan, shakes hands with Toshihiro Mibe, director, president and representative executive officer of Honda, Score: 5\n","output_type":"stream"}],"execution_count":8},{"id":"9290499a","cell_type":"markdown","source":"#### 7. **Using Hooks for Custom Workflow**","metadata":{"id":"9290499a"}},{"id":"9d069c2b","cell_type":"markdown","source":"Hooks in Crawl4AI allow you to run custom logic at specific stages of the crawling process. This can be invaluable for scenarios like setting custom headers, logging activities, or processing content before it is returned. Below is an example of a basic workflow using a hook, followed by a complete list of available hooks and explanations on their usage.","metadata":{"id":"9d069c2b"}},{"id":"bc4d2fc8","cell_type":"code","source":"async def custom_hook_workflow():\n    async with AsyncWebCrawler() as crawler:\n        # Set a 'before_goto' hook to run custom code just before navigation\n        crawler.crawler_strategy.set_hook(\"before_goto\", lambda page, context: print(\"[Hook] Preparing to navigate...\"))\n\n        # Perform the crawl operation\n        result = await crawler.arun(\n            url=\"https://crawl4ai.com\",\n            bypass_cache=True\n        )\n        print(result.markdown[:500])  # Display the first 500 characters\n\nasyncio.run(custom_hook_workflow())","metadata":{"id":"bc4d2fc8","outputId":"48cecb26-0300-4d81-ea21-fbdeee0e255c","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:38:58.699702Z","iopub.execute_input":"2024-12-18T11:38:58.700100Z","iopub.status.idle":"2024-12-18T11:39:02.712463Z","shell.execute_reply.started":"2024-12-18T11:38:58.700065Z","shell.execute_reply":"2024-12-18T11:39:02.709691Z"}},"outputs":[{"name":"stdout","text":"[INIT].... ‚Üí Crawl4AI 0.4.23\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_24/2672093240.py:7: DeprecationWarning: Cache control boolean flags are deprecated and will be removed in version 0.5.0. Use 'cache_mode' parameter instead.\n  result = await crawler.arun(\n","output_type":"stream"},{"name":"stdout","text":"[Hook] Preparing to navigate...\n[FETCH]... ‚Üì https://crawl4ai.com... | Status: True | Time: 3.00s\n[SCRAPE].. ‚óÜ Processed https://crawl4ai.com... | Time: 82ms\n[COMPLETE] ‚óè https://crawl4ai.com... | Status: True | Total: 3.11s\n[Crawl4AI Documentation](https://docs.crawl4ai.com/)\n\n  * [ Home ](.)\n  * [ Installation ](basic/installation/)\n  * [ Docker Deployment ](basic/docker-deploymeny/)\n  * [ Quick Start ](basic/quickstart/)\n  * [ Search ](#)\n\n\n\n  * Home\n  * [Installation](basic/installation/)\n  * [Docker Deployment](basic/docker-deploymeny/)\n  * [Quick Start](basic/quickstart/)\n  * Changelog & Blog\n    * [Blog Home](blog/)\n    * [Latest (0.4.2)](blog/releases/0.4.2/)\n    * [Changelog](https://github.com/unclecode/cr\n","output_type":"stream"}],"execution_count":10},{"id":"3ff45e21","cell_type":"markdown","source":"List of available hooks and examples for each stage of the crawling process:\n\n- **on_browser_created**\n    ```python\n    async def on_browser_created_hook(browser):\n        print(\"[Hook] Browser created\")\n    ```\n\n- **before_goto**\n    ```python\n    async def before_goto_hook(page):\n        await page.set_extra_http_headers({\"X-Test-Header\": \"test\"})\n    ```\n\n- **after_goto**\n    ```python\n    async def after_goto_hook(page):\n        print(f\"[Hook] Navigated to {page.url}\")\n    ```\n\n- **on_execution_started**\n    ```python\n    async def on_execution_started_hook(page):\n        print(\"[Hook] JavaScript execution started\")\n    ```\n\n- **before_return_html**\n    ```python\n    async def before_return_html_hook(page, html):\n        print(f\"[Hook] HTML length: {len(html)}\")\n    ```","metadata":{"id":"3ff45e21"}},{"id":"2d56ebb1","cell_type":"markdown","source":"#### 8. **Session-Based Crawling**\n\nWhen to Use Session-Based Crawling:\nSession-based crawling is especially beneficial when navigating through multi-page content where each page load needs to maintain the same session context. For instance, in cases where a ‚ÄúNext Page‚Äù button must be clicked to load subsequent data, the new data often replaces the previous content. Here, session-based crawling keeps the browser state intact across each interaction, allowing for sequential actions within the same session.\n\nExample: Multi-Page Navigation Using JavaScript\nIn this example, we‚Äôll navigate through multiple pages by clicking a \"Next Page\" button. After each page load, we extract the new content and repeat the process.","metadata":{"id":"2d56ebb1"}},{"id":"e7bfebae","cell_type":"code","source":"async def multi_page_session_crawl():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"page_navigation_session\"\n        url = \"https://example.com/paged-content\"\n\n        for page_number in range(1, 4):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.next-page-button').click();\" if page_number > 1 else None,\n                css_selector=\".content-section\",\n                bypass_cache=True\n            )\n            print(f\"Page {page_number} Content:\")\n            print(result.markdown[:500])  # Print first 500 characters\n\n# asyncio.run(multi_page_session_crawl())","metadata":{"id":"e7bfebae","trusted":true},"outputs":[],"execution_count":null},{"id":"ad32a778","cell_type":"markdown","source":"#### 9. **Using Extraction Strategies**\n\n**LLM Extraction**\n\nThis example demonstrates how to use language model-based extraction to retrieve structured data from a pricing page on OpenAI‚Äôs site.","metadata":{"id":"ad32a778"}},{"id":"3011a7c5","cell_type":"code","source":"from crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\nimport os, json\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(\n        ..., description=\"Fee for output token for the OpenAI model.\"\n    )\n\nasync def extract_structured_data_using_llm(provider: str, api_token: str = None, extra_headers: dict = None):\n    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n\n    # Skip if API token is missing (for providers that require it)\n    if api_token is None and provider != \"ollama\":\n        print(f\"API token is required for {provider}. Skipping this example.\")\n        return\n\n    extra_args = {\"extra_headers\": extra_headers} if extra_headers else {}\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://openai.com/api/pricing/\",\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=provider,\n                api_token=api_token,\n                schema=OpenAIModelFee.schema(),\n                extraction_type=\"schema\",\n                instruction=\"\"\"Extract all model names along with fees for input and output tokens.\"\n                \"{model_name: 'GPT-4', input_fee: 'US$10.00 / 1M tokens', output_fee: 'US$30.00 / 1M tokens'}.\"\"\",\n                **extra_args\n            ),\n            bypass_cache=True,\n        )\n        print(json.loads(result.extracted_content)[:5])\n\n# Usage:\nawait extract_structured_data_using_llm(\"openai/gpt-4o-mini\", os.getenv(\"OPENAI_API_KEY\"))","metadata":{"id":"3011a7c5","outputId":"691304a1-e850-4460-ca94-e786b180acc0","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:40:16.973904Z","iopub.execute_input":"2024-12-18T11:40:16.974311Z","iopub.status.idle":"2024-12-18T11:40:17.007340Z","shell.execute_reply.started":"2024-12-18T11:40:16.974276Z","shell.execute_reply":"2024-12-18T11:40:17.005893Z"}},"outputs":[{"name":"stdout","text":"\n--- Extracting Structured Data with openai/gpt-4o-mini ---\nAPI token is required for openai/gpt-4o-mini. Skipping this example.\n","output_type":"stream"}],"execution_count":12},{"id":"6532db9d","cell_type":"markdown","source":"**Cosine Similarity Strategy**\n\nThis strategy uses semantic clustering to extract relevant content based on contextual similarity, which is helpful when extracting related sections from a single topic.","metadata":{"id":"6532db9d"}},{"id":"ec079108","cell_type":"code","source":"from crawl4ai.extraction_strategy import CosineStrategy\n\nasync def cosine_similarity_extraction():\n    async with AsyncWebCrawler() as crawler:\n        strategy = CosineStrategy(\n            word_count_threshold=10,\n            max_dist=0.2, # Maximum distance between two words\n            linkage_method=\"ward\", # Linkage method for hierarchical clustering (ward, complete, average, single)\n            top_k=3, # Number of top keywords to extract\n            sim_threshold=0.3, # Similarity threshold for clustering\n            semantic_filter=\"McDonald's economic impact, American consumer trends\", # Keywords to filter the content semantically using embeddings\n            verbose=True\n        )\n\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business/consumer/how-mcdonalds-e-coli-crisis-inflation-politics-reflect-american-story-rcna177156\",\n            extraction_strategy=strategy\n        )\n        print(json.loads(result.extracted_content)[:5])\n\nasyncio.run(cosine_similarity_extraction())\n","metadata":{"id":"ec079108","outputId":"730f4ba8-8f1f-4a5d-8849-2de4881e6236","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T11:40:31.602338Z","iopub.execute_input":"2024-12-18T11:40:31.602834Z","iopub.status.idle":"2024-12-18T11:41:42.238863Z","shell.execute_reply.started":"2024-12-18T11:40:31.602791Z","shell.execute_reply":"2024-12-18T11:41:42.237825Z"}},"outputs":[{"name":"stdout","text":"[INIT].... ‚Üí Crawl4AI 0.4.23\n[LOG] Loading Extraction Model for cpu device.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12c262a544744c3bb3e6369f80ba1078"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbd3888bb15041918b148c97b72382f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6f9e310b80483c8ccaa84091c6f326"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a489982cf64a4afc84331b5ed544237b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44a27d4a0eca4956816cdb2d41bdac15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d995db6e70f4417bab0631a961c3ed0"}},"metadata":{}},{"name":"stdout","text":"[LOG] Loading Multilabel Classifier for cpu device.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"236ce87cbe4149c2b3849f3173667a2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2ad7350bdb74dbab1cf255334e11890"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a79553df1c04e8faa16a4422df2bc69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27d5b6861473486b8f700889e4e7aed3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc2649ee67146638d234b31ddea9d3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.88k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82a94434951947a3a1ce9b730fd98d9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27af546d70a04d52a8d7e76ec6d88fbd"}},"metadata":{}},{"name":"stdout","text":"[LOG] Model loaded sentence-transformers/all-MiniLM-L6-v2, models/reuters, took 36.728569984436035 seconds\n[COMPLETE] ‚óè Database backup created at: /root/.crawl4ai/crawl4ai.db.backup_20241218_114109\n[INIT].... ‚Üí Starting database migration...\n[COMPLETE] ‚óè Migration completed. 0 records processed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fa23d0427fe4aacba7309fa00c52162"}},"metadata":{}},{"name":"stdout","text":"[FETCH]... ‚Üì https://www.nbcnews.com/business/consumer/how-mcdo... | Status: True | Time: 6.93s\n[SCRAPE].. ‚óÜ Processed https://www.nbcnews.com/business/consumer/how-mcdo... | Time: 367ms\n[LOG] üöÄ Assign tags using cpu\n[LOG] üöÄ Categorization done in 0.93 seconds\n[EXTRACT]. ‚ñ† Completed for https://www.nbcnews.com/business/consumer/how-mcdo... | Time: 24.310929086999977s\n[COMPLETE] ‚óè https://www.nbcnews.com/business/consumer/how-mcdo... | Status: True | Total: 32.74s\n[{'index': 1, 'tags': ['food_&_dining', 'news_&_social_concern'], 'content': '‚ÄúThe thing that McDonald‚Äôs had struggled with, and why I think we‚Äôre seeing kind of an inflection point, is a value proposition,‚Äù Senatore said. ‚ÄúMcDonald‚Äôs menu price increases had run ahead of a lot of its restaurant peers. ‚Ä¶ Consumers are savvy enough to know that.‚Äù'}, {'index': 2, 'tags': ['diaries_&_daily_life', 'food_&_dining'], 'content': 'Landing on the right formula isn‚Äôt easy, Oches said, as it requires adjusting to shifts in consumer behavior that are still in flux: ‚ÄúSomeone might be saying, ‚ÄòNo more McDonald‚Äôs beverage treats today, I‚Äôll eat at home ‚Äî but maybe I‚Äôll still get one tomorrow.‚Äô But even a few visits less, over millions of people, has a big impact.‚Äù'}, {'index': 3, 'tags': ['business_&_entrepreneurs', 'food_&_dining', 'news_&_social_concern'], 'content': 'McDonald‚Äôs has faced a customer revolt over pricey Big Macs, an unsolicited cameo in election-season crossfire, and now an E. coli outbreak ‚Äî just as the company had been luring customers back with more affordable burgers. Despite a difficult quarter, McDonald‚Äôs looks resilient in the face of various pressures, analysts say ‚Äî something the company shares with U.S. consumers overall. ‚ÄúMcDonald‚Äôs has also done a good job of embedding the brand in popular culture to enhance its relevance and meaning around fun and family. But it also needed to modify the product line to meet the expectations of a consumer who is on a tight budget,‚Äù he said. For many consumers, the fast-food giant‚Äôs menus serve as an informal gauge of the economy overall, said Sara Senatore, a Bank of America analyst covering restaurants. ‚ÄúThe spotlight is always on McDonald‚Äôs because it‚Äôs so big‚Äù and something of a ‚Äúbellwether,‚Äù she said. ‚ÄúConsumers continue to be even more discriminating with every dollar that they spend,‚Äù he said at the time. Going forward, McDonald‚Äôs would be ‚Äúlaser-focused‚Äù on affordability. '}, {'index': 4, 'tags': ['business_&_entrepreneurs', 'news_&_social_concern'], 'content': 'The fast-food giant [reported Tuesday](https://www.cnbc.com/2024/10/29/mcdonalds-mcd-earnings-q3-2024.html) that it had reversed its recent U.S. sales drop, posting a 0.3% uptick in the third quarter. Foot traffic was still down slightly, but the company said its summer of discounts was paying off. ‚ÄúWe will stay laser-focused on providing an unparalleled experience with simple, everyday value and affordability that our consumers can count on as they continue to be mindful about their spending,‚Äù CEO Chris Kempczinski [said in a statement](https://www.prnewswire.com/news-releases/mcdonalds-reports-third-quarter-2024-results-302289216.html?Fds-Load-Behavior=force-external) alongside the earnings report. The financial results come toward the end of a humbling year for the nearly $213 billion restaurant chain, whose shares remained steady on the heels of its latest earnings. Kempczinski [sought to reassure investors](https://www.cnbc.com/2024/10/29/mcdonalds-e-coli-outbreak-ceo-comments.html) that [the E. coli outbreak](https://www.nbcnews.com/health/health-news/illnesses-linked-mcdonalds-e-coli-outbreak-rise-75-cdc-says-rcna177260), linked to Quarter Pounder burgers, was under control after the health crisis temporarily dented the company‚Äôs stock and caused U.S. foot traffic to drop nearly 10% in the days afterward, according to estimates by Gordon Haskett financial researchers. Relative to other major brands, like Starbucks, that have [struggled to reconnect with customers](https://www.cnbc.com/2024/10/22/starbucks-shares-slide-after-preliminary-results-show-sales-fell-again.html) who are watching their wallets, McDonald‚Äôs has done an ‚Äúexcellent job‚Äù of bringing diners back, said Ravi Dhar, the director of Yale University‚Äôs Center for Customer Insights. Like many major brands, McDonald‚Äôs raked in big profits as the economy reopened from the pandemic. In October 2022, [executives were boasting](https://www.cnbc.com/2022/10/27/mcdonalds-mcd-earnings-q3-2022.html) that they‚Äôd been raising prices without crimping traffic, even as competitors began to warn that some customers were closing their wallets after inflation peaked above 9% that summer. Still, the U.S. had repeatedly dodged a much-forecast recession, and [Americans kept spending on nonessentials](https://www.nbcnews.com/business/economy/year-peak-inflation-travel-leisure-mostly-cost-less-rcna92760) like travel and dining out ‚Äî despite regularly relaying to pollsters their dismal views of an otherwise solid economy. But by early this year, [photos of eye-watering menu prices](https://x.com/sam_learner/status/1681367351143301129) at some McDonald‚Äôs locations ‚Äî including an $18 Big Mac combo at a Connecticut rest stop from July 2023 ‚Äî went viral, bringing diners‚Äô long-simmering frustrations to a boiling point that the company couldn‚Äôt ignore. On an earnings call in April, Kempczinski acknowledged that foot traffic had fallen.'}, {'index': 5, 'tags': ['food_&_dining', 'news_&_social_concern'], 'content': '![mcdonalds drive-thru economy fast food](https://media-cldnry.s-nbcnews.com/image/upload/t_fit-760w,f_auto,q_auto:best/rockcms/2024-10/241024-los-angeles-mcdonalds-drive-thru-ac-1059p-cfc311.jpg)McDonald‚Äôs has had some success leaning into discounts this year. Eric Thayer / Bloomberg via Getty Images file'}]\n","output_type":"stream"}],"execution_count":13},{"id":"ff423629","cell_type":"markdown","source":"#### 10. **Conclusion and Next Steps**\n\nYou‚Äôve explored core features of Crawl4AI, including dynamic content handling, link analysis, and advanced extraction strategies. Visit our documentation for further details on using Crawl4AI‚Äôs extensive features.\n\n- GitHub Repository: [https://github.com/unclecode/crawl4ai](https://github.com/unclecode/crawl4ai)\n- Twitter: [@unclecode](https://twitter.com/unclecode)\n- Website: [https://crawl4ai.com](https://crawl4ai.com)\n\nHappy Crawling with Crawl4AI! üï∑Ô∏èü§ñ\n","metadata":{"id":"ff423629"}},{"id":"d34c1d35","cell_type":"markdown","source":"","metadata":{"id":"d34c1d35"}}]}